Index: server.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import socket\nimport pickle\nfrom _thread import *\n\nfrom player import Player\n\nserver = '192.168.1.146'\nport = 5555\n\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\ntry:\n    s.bind((server, port))\nexcept socket.error as e:\n    print(e)\n\ns.listen(2)\nprint('waiting for connection, Server Started')\n\n\nplayers = [Player(0, 0, 50, 50, (0, 255, 0)),\n           Player(100, 100, 50, 50, (255, 0, 0))]\n\n\ndef client(conn, player):  # threaded function\n    conn.send(pickle.dumps(players[player]))\n    reply = ''\n\n    while True:\n        try:\n            data = pickle.loads(conn.recv(2048))\n            players[player] = data\n\n            if not data:\n                print('disconnected')\n                break\n            else:\n                if player == 1:\n                    reply = players[0]\n                else:\n                    reply = players[1]\n                print('Received:', data)\n                print('Sending:', reply)\n            conn.sendall(pickle.dumps(reply))\n        except:\n            break\n\n    print('Connection lost')\n    conn.close()\n\n\ncurrent_player = 0\nwhile True:\n    conn, addr = s.accept()\n    print('Connected to:', addr)\n\n    start_new_thread(client, (conn, current_player))\n    current_player += 1\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- server.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ server.py	(date 1602098372169)
@@ -4,7 +4,8 @@
 
 from player import Player
 
-server = '192.168.1.146'
+server = '172.16.1.165'
+# 172.16.1.165
 port = 5555
 
 s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
Index: network.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import socket\nimport pickle\n\n\nclass Network:\n    def __init__(self):\n        self.client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.server = \"192.168.1.146\"\n        self.port = 5555\n        self.addr = (self.server, self.port)\n        self.p = self.connect()\n\n    def get_p(self):\n        return self.p\n\n    def connect(self,):\n        try:\n            self.client.connect(self.addr)\n            return pickle.loads(self.client.recv(2048))\n        except:\n            pass\n\n    def send(self, data):\n        try:\n            self.client.send(pickle.dumps(data))\n            return pickle.loads(self.client.recv(2048))\n        except socket.error as e:\n            print(e)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- network.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ network.py	(date 1602098551732)
@@ -5,7 +5,7 @@
 class Network:
     def __init__(self):
         self.client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
-        self.server = "192.168.1.146"
+        self.server = "172.16.1.165"
         self.port = 5555
         self.addr = (self.server, self.port)
         self.p = self.connect()
Index: client.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pygame\n\nfrom network import Network\nfrom player import Player\n\nWIDTH = 500\nHEIGHT = 500\nFPS = 100\nwin = pygame.display.set_mode((WIDTH, HEIGHT))\npygame.display.set_caption('Client')\n\n\ndef redrawWindow(win, player, player2):\n    win.fill((255, 255, 255))\n    player.draw(win)\n    player2.draw(win)\n    pygame.display.update()\n\n\ndef main():\n    run = True\n    n = Network()\n    p = n.get_p()\n\n    clock = pygame.time.Clock()\n\n    while run:\n        clock.tick(FPS)\n        p2 = n.send(p)\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                run = False\n                pygame.quit()\n\n        p.move()\n        redrawWindow(win, p, p2)\n\n\nmain()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- client.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ client.py	(date 1602098531457)
@@ -5,7 +5,7 @@
 
 WIDTH = 500
 HEIGHT = 500
-FPS = 100
+FPS = 160
 win = pygame.display.set_mode((WIDTH, HEIGHT))
 pygame.display.set_caption('Client')
 
Index: env/lib/python3.8/site-packages/pip-20.2.1.dist-info/top_level.txt
===================================================================
--- env/lib/python3.8/site-packages/pip-20.2.1.dist-info/top_level.txt	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pip-20.2.1.dist-info/top_level.txt	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
@@ -1,1 +0,0 @@
-pip
Index: env/lib/python3.8/site-packages/setuptools-49.2.1.dist-info/entry_points.txt
===================================================================
--- env/lib/python3.8/site-packages/setuptools-49.2.1.dist-info/entry_points.txt	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools-49.2.1.dist-info/entry_points.txt	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
@@ -1,68 +0,0 @@
-[console_scripts]
-easy_install = setuptools.command.easy_install:main
-easy_install-3.8 = setuptools.command.easy_install:main
-
-[distutils.commands]
-alias = setuptools.command.alias:alias
-bdist_egg = setuptools.command.bdist_egg:bdist_egg
-bdist_rpm = setuptools.command.bdist_rpm:bdist_rpm
-bdist_wininst = setuptools.command.bdist_wininst:bdist_wininst
-build_clib = setuptools.command.build_clib:build_clib
-build_ext = setuptools.command.build_ext:build_ext
-build_py = setuptools.command.build_py:build_py
-develop = setuptools.command.develop:develop
-dist_info = setuptools.command.dist_info:dist_info
-easy_install = setuptools.command.easy_install:easy_install
-egg_info = setuptools.command.egg_info:egg_info
-install = setuptools.command.install:install
-install_egg_info = setuptools.command.install_egg_info:install_egg_info
-install_lib = setuptools.command.install_lib:install_lib
-install_scripts = setuptools.command.install_scripts:install_scripts
-rotate = setuptools.command.rotate:rotate
-saveopts = setuptools.command.saveopts:saveopts
-sdist = setuptools.command.sdist:sdist
-setopt = setuptools.command.setopt:setopt
-test = setuptools.command.test:test
-upload_docs = setuptools.command.upload_docs:upload_docs
-
-[distutils.setup_keywords]
-convert_2to3_doctests = setuptools.dist:assert_string_list
-dependency_links = setuptools.dist:assert_string_list
-eager_resources = setuptools.dist:assert_string_list
-entry_points = setuptools.dist:check_entry_points
-exclude_package_data = setuptools.dist:check_package_data
-extras_require = setuptools.dist:check_extras
-include_package_data = setuptools.dist:assert_bool
-install_requires = setuptools.dist:check_requirements
-namespace_packages = setuptools.dist:check_nsp
-package_data = setuptools.dist:check_package_data
-packages = setuptools.dist:check_packages
-python_requires = setuptools.dist:check_specifier
-setup_requires = setuptools.dist:check_requirements
-test_loader = setuptools.dist:check_importable
-test_runner = setuptools.dist:check_importable
-test_suite = setuptools.dist:check_test_suite
-tests_require = setuptools.dist:check_requirements
-use_2to3 = setuptools.dist:assert_bool
-use_2to3_exclude_fixers = setuptools.dist:assert_string_list
-use_2to3_fixers = setuptools.dist:assert_string_list
-zip_safe = setuptools.dist:assert_bool
-
-[egg_info.writers]
-PKG-INFO = setuptools.command.egg_info:write_pkg_info
-dependency_links.txt = setuptools.command.egg_info:overwrite_arg
-depends.txt = setuptools.command.egg_info:warn_depends_obsolete
-eager_resources.txt = setuptools.command.egg_info:overwrite_arg
-entry_points.txt = setuptools.command.egg_info:write_entries
-namespace_packages.txt = setuptools.command.egg_info:overwrite_arg
-requires.txt = setuptools.command.egg_info:write_requirements
-top_level.txt = setuptools.command.egg_info:write_toplevel_names
-
-[setuptools.finalize_distribution_options]
-2to3_doctests = setuptools.dist:Distribution._finalize_2to3_doctests
-keywords = setuptools.dist:Distribution._finalize_setup_keywords
-parent_finalize = setuptools.dist:_Distribution.finalize_options
-
-[setuptools.installation]
-eggsecutable = setuptools.command.easy_install:bootstrap
-
Index: env/lib/python3.8/site-packages/setuptools/distutils_patch.py
===================================================================
--- env/lib/python3.8/site-packages/setuptools/distutils_patch.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/distutils_patch.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
@@ -1,61 +0,0 @@
-"""
-Ensure that the local copy of distutils is preferred over stdlib.
-
-See https://github.com/pypa/setuptools/issues/417#issuecomment-392298401
-for more motivation.
-"""
-
-import sys
-import re
-import os
-import importlib
-import warnings
-
-
-is_pypy = '__pypy__' in sys.builtin_module_names
-
-
-def warn_distutils_present():
-    if 'distutils' not in sys.modules:
-        return
-    if is_pypy and sys.version_info < (3, 7):
-        # PyPy for 3.6 unconditionally imports distutils, so bypass the warning
-        # https://foss.heptapod.net/pypy/pypy/-/blob/be829135bc0d758997b3566062999ee8b23872b4/lib-python/3/site.py#L250
-        return
-    warnings.warn(
-        "Distutils was imported before Setuptools. This usage is discouraged "
-        "and may exhibit undesirable behaviors or errors. Please use "
-        "Setuptools' objects directly or at least import Setuptools first.")
-
-
-def clear_distutils():
-    if 'distutils' not in sys.modules:
-        return
-    warnings.warn("Setuptools is replacing distutils.")
-    mods = [name for name in sys.modules if re.match(r'distutils\b', name)]
-    for name in mods:
-        del sys.modules[name]
-
-
-def enabled():
-    """
-    Allow selection of distutils by environment variable.
-    """
-    which = os.environ.get('SETUPTOOLS_USE_DISTUTILS', 'stdlib')
-    return which == 'local'
-
-
-def ensure_local_distutils():
-    clear_distutils()
-    distutils = importlib.import_module('setuptools._distutils')
-    distutils.__name__ = 'distutils'
-    sys.modules['distutils'] = distutils
-
-    # sanity check that submodules load as expected
-    core = importlib.import_module('distutils.core')
-    assert '_distutils' in core.__file__, core.__file__
-
-
-warn_distutils_present()
-if enabled():
-    ensure_local_distutils()
Index: env/lib/python3.8/site-packages/setuptools-49.2.1.dist-info/dependency_links.txt
===================================================================
--- env/lib/python3.8/site-packages/setuptools-49.2.1.dist-info/dependency_links.txt	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools-49.2.1.dist-info/dependency_links.txt	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
@@ -1,2 +0,0 @@
-https://files.pythonhosted.org/packages/source/c/certifi/certifi-2016.9.26.tar.gz#md5=baa81e951a29958563689d868ef1064d
-https://files.pythonhosted.org/packages/source/w/wincertstore/wincertstore-0.2.zip#md5=ae728f2f007185648d0c7a8679b361e2
Index: env/lib/python3.8/site-packages/pip-20.2.1.dist-info/LICENSE.txt
===================================================================
--- env/lib/python3.8/site-packages/pip-20.2.1.dist-info/LICENSE.txt	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pip-20.2.1.dist-info/LICENSE.txt	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
@@ -1,20 +0,0 @@
-Copyright (c) 2008-2019 The pip developers (see AUTHORS.txt file)
-
-Permission is hereby granted, free of charge, to any person obtaining
-a copy of this software and associated documentation files (the
-"Software"), to deal in the Software without restriction, including
-without limitation the rights to use, copy, modify, merge, publish,
-distribute, sublicense, and/or sell copies of the Software, and to
-permit persons to whom the Software is furnished to do so, subject to
-the following conditions:
-
-The above copyright notice and this permission notice shall be
-included in all copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
-EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
-MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
-NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
-LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
-OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
-WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
Index: env/lib/python3.8/site-packages/setuptools/_vendor/six.py
===================================================================
--- env/lib/python3.8/site-packages/setuptools/_vendor/six.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/_vendor/six.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
@@ -1,868 +0,0 @@
-"""Utilities for writing code that runs on Python 2 and 3"""
-
-# Copyright (c) 2010-2015 Benjamin Peterson
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in all
-# copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-# SOFTWARE.
-
-from __future__ import absolute_import
-
-import functools
-import itertools
-import operator
-import sys
-import types
-
-__author__ = "Benjamin Peterson <benjamin@python.org>"
-__version__ = "1.10.0"
-
-
-# Useful for very coarse version differentiation.
-PY2 = sys.version_info[0] == 2
-PY3 = sys.version_info[0] == 3
-PY34 = sys.version_info[0:2] >= (3, 4)
-
-if PY3:
-    string_types = str,
-    integer_types = int,
-    class_types = type,
-    text_type = str
-    binary_type = bytes
-
-    MAXSIZE = sys.maxsize
-else:
-    string_types = basestring,
-    integer_types = (int, long)
-    class_types = (type, types.ClassType)
-    text_type = unicode
-    binary_type = str
-
-    if sys.platform.startswith("java"):
-        # Jython always uses 32 bits.
-        MAXSIZE = int((1 << 31) - 1)
-    else:
-        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).
-        class X(object):
-
-            def __len__(self):
-                return 1 << 31
-        try:
-            len(X())
-        except OverflowError:
-            # 32-bit
-            MAXSIZE = int((1 << 31) - 1)
-        else:
-            # 64-bit
-            MAXSIZE = int((1 << 63) - 1)
-        del X
-
-
-def _add_doc(func, doc):
-    """Add documentation to a function."""
-    func.__doc__ = doc
-
-
-def _import_module(name):
-    """Import module, returning the module after the last dot."""
-    __import__(name)
-    return sys.modules[name]
-
-
-class _LazyDescr(object):
-
-    def __init__(self, name):
-        self.name = name
-
-    def __get__(self, obj, tp):
-        result = self._resolve()
-        setattr(obj, self.name, result)  # Invokes __set__.
-        try:
-            # This is a bit ugly, but it avoids running this again by
-            # removing this descriptor.
-            delattr(obj.__class__, self.name)
-        except AttributeError:
-            pass
-        return result
-
-
-class MovedModule(_LazyDescr):
-
-    def __init__(self, name, old, new=None):
-        super(MovedModule, self).__init__(name)
-        if PY3:
-            if new is None:
-                new = name
-            self.mod = new
-        else:
-            self.mod = old
-
-    def _resolve(self):
-        return _import_module(self.mod)
-
-    def __getattr__(self, attr):
-        _module = self._resolve()
-        value = getattr(_module, attr)
-        setattr(self, attr, value)
-        return value
-
-
-class _LazyModule(types.ModuleType):
-
-    def __init__(self, name):
-        super(_LazyModule, self).__init__(name)
-        self.__doc__ = self.__class__.__doc__
-
-    def __dir__(self):
-        attrs = ["__doc__", "__name__"]
-        attrs += [attr.name for attr in self._moved_attributes]
-        return attrs
-
-    # Subclasses should override this
-    _moved_attributes = []
-
-
-class MovedAttribute(_LazyDescr):
-
-    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):
-        super(MovedAttribute, self).__init__(name)
-        if PY3:
-            if new_mod is None:
-                new_mod = name
-            self.mod = new_mod
-            if new_attr is None:
-                if old_attr is None:
-                    new_attr = name
-                else:
-                    new_attr = old_attr
-            self.attr = new_attr
-        else:
-            self.mod = old_mod
-            if old_attr is None:
-                old_attr = name
-            self.attr = old_attr
-
-    def _resolve(self):
-        module = _import_module(self.mod)
-        return getattr(module, self.attr)
-
-
-class _SixMetaPathImporter(object):
-
-    """
-    A meta path importer to import six.moves and its submodules.
-
-    This class implements a PEP302 finder and loader. It should be compatible
-    with Python 2.5 and all existing versions of Python3
-    """
-
-    def __init__(self, six_module_name):
-        self.name = six_module_name
-        self.known_modules = {}
-
-    def _add_module(self, mod, *fullnames):
-        for fullname in fullnames:
-            self.known_modules[self.name + "." + fullname] = mod
-
-    def _get_module(self, fullname):
-        return self.known_modules[self.name + "." + fullname]
-
-    def find_module(self, fullname, path=None):
-        if fullname in self.known_modules:
-            return self
-        return None
-
-    def __get_module(self, fullname):
-        try:
-            return self.known_modules[fullname]
-        except KeyError:
-            raise ImportError("This loader does not know module " + fullname)
-
-    def load_module(self, fullname):
-        try:
-            # in case of a reload
-            return sys.modules[fullname]
-        except KeyError:
-            pass
-        mod = self.__get_module(fullname)
-        if isinstance(mod, MovedModule):
-            mod = mod._resolve()
-        else:
-            mod.__loader__ = self
-        sys.modules[fullname] = mod
-        return mod
-
-    def is_package(self, fullname):
-        """
-        Return true, if the named module is a package.
-
-        We need this method to get correct spec objects with
-        Python 3.4 (see PEP451)
-        """
-        return hasattr(self.__get_module(fullname), "__path__")
-
-    def get_code(self, fullname):
-        """Return None
-
-        Required, if is_package is implemented"""
-        self.__get_module(fullname)  # eventually raises ImportError
-        return None
-    get_source = get_code  # same as get_code
-
-_importer = _SixMetaPathImporter(__name__)
-
-
-class _MovedItems(_LazyModule):
-
-    """Lazy loading of moved objects"""
-    __path__ = []  # mark as package
-
-
-_moved_attributes = [
-    MovedAttribute("cStringIO", "cStringIO", "io", "StringIO"),
-    MovedAttribute("filter", "itertools", "builtins", "ifilter", "filter"),
-    MovedAttribute("filterfalse", "itertools", "itertools", "ifilterfalse", "filterfalse"),
-    MovedAttribute("input", "__builtin__", "builtins", "raw_input", "input"),
-    MovedAttribute("intern", "__builtin__", "sys"),
-    MovedAttribute("map", "itertools", "builtins", "imap", "map"),
-    MovedAttribute("getcwd", "os", "os", "getcwdu", "getcwd"),
-    MovedAttribute("getcwdb", "os", "os", "getcwd", "getcwdb"),
-    MovedAttribute("range", "__builtin__", "builtins", "xrange", "range"),
-    MovedAttribute("reload_module", "__builtin__", "importlib" if PY34 else "imp", "reload"),
-    MovedAttribute("reduce", "__builtin__", "functools"),
-    MovedAttribute("shlex_quote", "pipes", "shlex", "quote"),
-    MovedAttribute("StringIO", "StringIO", "io"),
-    MovedAttribute("UserDict", "UserDict", "collections"),
-    MovedAttribute("UserList", "UserList", "collections"),
-    MovedAttribute("UserString", "UserString", "collections"),
-    MovedAttribute("xrange", "__builtin__", "builtins", "xrange", "range"),
-    MovedAttribute("zip", "itertools", "builtins", "izip", "zip"),
-    MovedAttribute("zip_longest", "itertools", "itertools", "izip_longest", "zip_longest"),
-    MovedModule("builtins", "__builtin__"),
-    MovedModule("configparser", "ConfigParser"),
-    MovedModule("copyreg", "copy_reg"),
-    MovedModule("dbm_gnu", "gdbm", "dbm.gnu"),
-    MovedModule("_dummy_thread", "dummy_thread", "_dummy_thread"),
-    MovedModule("http_cookiejar", "cookielib", "http.cookiejar"),
-    MovedModule("http_cookies", "Cookie", "http.cookies"),
-    MovedModule("html_entities", "htmlentitydefs", "html.entities"),
-    MovedModule("html_parser", "HTMLParser", "html.parser"),
-    MovedModule("http_client", "httplib", "http.client"),
-    MovedModule("email_mime_multipart", "email.MIMEMultipart", "email.mime.multipart"),
-    MovedModule("email_mime_nonmultipart", "email.MIMENonMultipart", "email.mime.nonmultipart"),
-    MovedModule("email_mime_text", "email.MIMEText", "email.mime.text"),
-    MovedModule("email_mime_base", "email.MIMEBase", "email.mime.base"),
-    MovedModule("BaseHTTPServer", "BaseHTTPServer", "http.server"),
-    MovedModule("CGIHTTPServer", "CGIHTTPServer", "http.server"),
-    MovedModule("SimpleHTTPServer", "SimpleHTTPServer", "http.server"),
-    MovedModule("cPickle", "cPickle", "pickle"),
-    MovedModule("queue", "Queue"),
-    MovedModule("reprlib", "repr"),
-    MovedModule("socketserver", "SocketServer"),
-    MovedModule("_thread", "thread", "_thread"),
-    MovedModule("tkinter", "Tkinter"),
-    MovedModule("tkinter_dialog", "Dialog", "tkinter.dialog"),
-    MovedModule("tkinter_filedialog", "FileDialog", "tkinter.filedialog"),
-    MovedModule("tkinter_scrolledtext", "ScrolledText", "tkinter.scrolledtext"),
-    MovedModule("tkinter_simpledialog", "SimpleDialog", "tkinter.simpledialog"),
-    MovedModule("tkinter_tix", "Tix", "tkinter.tix"),
-    MovedModule("tkinter_ttk", "ttk", "tkinter.ttk"),
-    MovedModule("tkinter_constants", "Tkconstants", "tkinter.constants"),
-    MovedModule("tkinter_dnd", "Tkdnd", "tkinter.dnd"),
-    MovedModule("tkinter_colorchooser", "tkColorChooser",
-                "tkinter.colorchooser"),
-    MovedModule("tkinter_commondialog", "tkCommonDialog",
-                "tkinter.commondialog"),
-    MovedModule("tkinter_tkfiledialog", "tkFileDialog", "tkinter.filedialog"),
-    MovedModule("tkinter_font", "tkFont", "tkinter.font"),
-    MovedModule("tkinter_messagebox", "tkMessageBox", "tkinter.messagebox"),
-    MovedModule("tkinter_tksimpledialog", "tkSimpleDialog",
-                "tkinter.simpledialog"),
-    MovedModule("urllib_parse", __name__ + ".moves.urllib_parse", "urllib.parse"),
-    MovedModule("urllib_error", __name__ + ".moves.urllib_error", "urllib.error"),
-    MovedModule("urllib", __name__ + ".moves.urllib", __name__ + ".moves.urllib"),
-    MovedModule("urllib_robotparser", "robotparser", "urllib.robotparser"),
-    MovedModule("xmlrpc_client", "xmlrpclib", "xmlrpc.client"),
-    MovedModule("xmlrpc_server", "SimpleXMLRPCServer", "xmlrpc.server"),
-]
-# Add windows specific modules.
-if sys.platform == "win32":
-    _moved_attributes += [
-        MovedModule("winreg", "_winreg"),
-    ]
-
-for attr in _moved_attributes:
-    setattr(_MovedItems, attr.name, attr)
-    if isinstance(attr, MovedModule):
-        _importer._add_module(attr, "moves." + attr.name)
-del attr
-
-_MovedItems._moved_attributes = _moved_attributes
-
-moves = _MovedItems(__name__ + ".moves")
-_importer._add_module(moves, "moves")
-
-
-class Module_six_moves_urllib_parse(_LazyModule):
-
-    """Lazy loading of moved objects in six.moves.urllib_parse"""
-
-
-_urllib_parse_moved_attributes = [
-    MovedAttribute("ParseResult", "urlparse", "urllib.parse"),
-    MovedAttribute("SplitResult", "urlparse", "urllib.parse"),
-    MovedAttribute("parse_qs", "urlparse", "urllib.parse"),
-    MovedAttribute("parse_qsl", "urlparse", "urllib.parse"),
-    MovedAttribute("urldefrag", "urlparse", "urllib.parse"),
-    MovedAttribute("urljoin", "urlparse", "urllib.parse"),
-    MovedAttribute("urlparse", "urlparse", "urllib.parse"),
-    MovedAttribute("urlsplit", "urlparse", "urllib.parse"),
-    MovedAttribute("urlunparse", "urlparse", "urllib.parse"),
-    MovedAttribute("urlunsplit", "urlparse", "urllib.parse"),
-    MovedAttribute("quote", "urllib", "urllib.parse"),
-    MovedAttribute("quote_plus", "urllib", "urllib.parse"),
-    MovedAttribute("unquote", "urllib", "urllib.parse"),
-    MovedAttribute("unquote_plus", "urllib", "urllib.parse"),
-    MovedAttribute("urlencode", "urllib", "urllib.parse"),
-    MovedAttribute("splitquery", "urllib", "urllib.parse"),
-    MovedAttribute("splittag", "urllib", "urllib.parse"),
-    MovedAttribute("splituser", "urllib", "urllib.parse"),
-    MovedAttribute("uses_fragment", "urlparse", "urllib.parse"),
-    MovedAttribute("uses_netloc", "urlparse", "urllib.parse"),
-    MovedAttribute("uses_params", "urlparse", "urllib.parse"),
-    MovedAttribute("uses_query", "urlparse", "urllib.parse"),
-    MovedAttribute("uses_relative", "urlparse", "urllib.parse"),
-]
-for attr in _urllib_parse_moved_attributes:
-    setattr(Module_six_moves_urllib_parse, attr.name, attr)
-del attr
-
-Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes
-
-_importer._add_module(Module_six_moves_urllib_parse(__name__ + ".moves.urllib_parse"),
-                      "moves.urllib_parse", "moves.urllib.parse")
-
-
-class Module_six_moves_urllib_error(_LazyModule):
-
-    """Lazy loading of moved objects in six.moves.urllib_error"""
-
-
-_urllib_error_moved_attributes = [
-    MovedAttribute("URLError", "urllib2", "urllib.error"),
-    MovedAttribute("HTTPError", "urllib2", "urllib.error"),
-    MovedAttribute("ContentTooShortError", "urllib", "urllib.error"),
-]
-for attr in _urllib_error_moved_attributes:
-    setattr(Module_six_moves_urllib_error, attr.name, attr)
-del attr
-
-Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes
-
-_importer._add_module(Module_six_moves_urllib_error(__name__ + ".moves.urllib.error"),
-                      "moves.urllib_error", "moves.urllib.error")
-
-
-class Module_six_moves_urllib_request(_LazyModule):
-
-    """Lazy loading of moved objects in six.moves.urllib_request"""
-
-
-_urllib_request_moved_attributes = [
-    MovedAttribute("urlopen", "urllib2", "urllib.request"),
-    MovedAttribute("install_opener", "urllib2", "urllib.request"),
-    MovedAttribute("build_opener", "urllib2", "urllib.request"),
-    MovedAttribute("pathname2url", "urllib", "urllib.request"),
-    MovedAttribute("url2pathname", "urllib", "urllib.request"),
-    MovedAttribute("getproxies", "urllib", "urllib.request"),
-    MovedAttribute("Request", "urllib2", "urllib.request"),
-    MovedAttribute("OpenerDirector", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPDefaultErrorHandler", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPRedirectHandler", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPCookieProcessor", "urllib2", "urllib.request"),
-    MovedAttribute("ProxyHandler", "urllib2", "urllib.request"),
-    MovedAttribute("BaseHandler", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPPasswordMgr", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPPasswordMgrWithDefaultRealm", "urllib2", "urllib.request"),
-    MovedAttribute("AbstractBasicAuthHandler", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPBasicAuthHandler", "urllib2", "urllib.request"),
-    MovedAttribute("ProxyBasicAuthHandler", "urllib2", "urllib.request"),
-    MovedAttribute("AbstractDigestAuthHandler", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPDigestAuthHandler", "urllib2", "urllib.request"),
-    MovedAttribute("ProxyDigestAuthHandler", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPHandler", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPSHandler", "urllib2", "urllib.request"),
-    MovedAttribute("FileHandler", "urllib2", "urllib.request"),
-    MovedAttribute("FTPHandler", "urllib2", "urllib.request"),
-    MovedAttribute("CacheFTPHandler", "urllib2", "urllib.request"),
-    MovedAttribute("UnknownHandler", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPErrorProcessor", "urllib2", "urllib.request"),
-    MovedAttribute("urlretrieve", "urllib", "urllib.request"),
-    MovedAttribute("urlcleanup", "urllib", "urllib.request"),
-    MovedAttribute("URLopener", "urllib", "urllib.request"),
-    MovedAttribute("FancyURLopener", "urllib", "urllib.request"),
-    MovedAttribute("proxy_bypass", "urllib", "urllib.request"),
-]
-for attr in _urllib_request_moved_attributes:
-    setattr(Module_six_moves_urllib_request, attr.name, attr)
-del attr
-
-Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes
-
-_importer._add_module(Module_six_moves_urllib_request(__name__ + ".moves.urllib.request"),
-                      "moves.urllib_request", "moves.urllib.request")
-
-
-class Module_six_moves_urllib_response(_LazyModule):
-
-    """Lazy loading of moved objects in six.moves.urllib_response"""
-
-
-_urllib_response_moved_attributes = [
-    MovedAttribute("addbase", "urllib", "urllib.response"),
-    MovedAttribute("addclosehook", "urllib", "urllib.response"),
-    MovedAttribute("addinfo", "urllib", "urllib.response"),
-    MovedAttribute("addinfourl", "urllib", "urllib.response"),
-]
-for attr in _urllib_response_moved_attributes:
-    setattr(Module_six_moves_urllib_response, attr.name, attr)
-del attr
-
-Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes
-
-_importer._add_module(Module_six_moves_urllib_response(__name__ + ".moves.urllib.response"),
-                      "moves.urllib_response", "moves.urllib.response")
-
-
-class Module_six_moves_urllib_robotparser(_LazyModule):
-
-    """Lazy loading of moved objects in six.moves.urllib_robotparser"""
-
-
-_urllib_robotparser_moved_attributes = [
-    MovedAttribute("RobotFileParser", "robotparser", "urllib.robotparser"),
-]
-for attr in _urllib_robotparser_moved_attributes:
-    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)
-del attr
-
-Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes
-
-_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + ".moves.urllib.robotparser"),
-                      "moves.urllib_robotparser", "moves.urllib.robotparser")
-
-
-class Module_six_moves_urllib(types.ModuleType):
-
-    """Create a six.moves.urllib namespace that resembles the Python 3 namespace"""
-    __path__ = []  # mark as package
-    parse = _importer._get_module("moves.urllib_parse")
-    error = _importer._get_module("moves.urllib_error")
-    request = _importer._get_module("moves.urllib_request")
-    response = _importer._get_module("moves.urllib_response")
-    robotparser = _importer._get_module("moves.urllib_robotparser")
-
-    def __dir__(self):
-        return ['parse', 'error', 'request', 'response', 'robotparser']
-
-_importer._add_module(Module_six_moves_urllib(__name__ + ".moves.urllib"),
-                      "moves.urllib")
-
-
-def add_move(move):
-    """Add an item to six.moves."""
-    setattr(_MovedItems, move.name, move)
-
-
-def remove_move(name):
-    """Remove item from six.moves."""
-    try:
-        delattr(_MovedItems, name)
-    except AttributeError:
-        try:
-            del moves.__dict__[name]
-        except KeyError:
-            raise AttributeError("no such move, %r" % (name,))
-
-
-if PY3:
-    _meth_func = "__func__"
-    _meth_self = "__self__"
-
-    _func_closure = "__closure__"
-    _func_code = "__code__"
-    _func_defaults = "__defaults__"
-    _func_globals = "__globals__"
-else:
-    _meth_func = "im_func"
-    _meth_self = "im_self"
-
-    _func_closure = "func_closure"
-    _func_code = "func_code"
-    _func_defaults = "func_defaults"
-    _func_globals = "func_globals"
-
-
-try:
-    advance_iterator = next
-except NameError:
-    def advance_iterator(it):
-        return it.next()
-next = advance_iterator
-
-
-try:
-    callable = callable
-except NameError:
-    def callable(obj):
-        return any("__call__" in klass.__dict__ for klass in type(obj).__mro__)
-
-
-if PY3:
-    def get_unbound_function(unbound):
-        return unbound
-
-    create_bound_method = types.MethodType
-
-    def create_unbound_method(func, cls):
-        return func
-
-    Iterator = object
-else:
-    def get_unbound_function(unbound):
-        return unbound.im_func
-
-    def create_bound_method(func, obj):
-        return types.MethodType(func, obj, obj.__class__)
-
-    def create_unbound_method(func, cls):
-        return types.MethodType(func, None, cls)
-
-    class Iterator(object):
-
-        def next(self):
-            return type(self).__next__(self)
-
-    callable = callable
-_add_doc(get_unbound_function,
-         """Get the function out of a possibly unbound function""")
-
-
-get_method_function = operator.attrgetter(_meth_func)
-get_method_self = operator.attrgetter(_meth_self)
-get_function_closure = operator.attrgetter(_func_closure)
-get_function_code = operator.attrgetter(_func_code)
-get_function_defaults = operator.attrgetter(_func_defaults)
-get_function_globals = operator.attrgetter(_func_globals)
-
-
-if PY3:
-    def iterkeys(d, **kw):
-        return iter(d.keys(**kw))
-
-    def itervalues(d, **kw):
-        return iter(d.values(**kw))
-
-    def iteritems(d, **kw):
-        return iter(d.items(**kw))
-
-    def iterlists(d, **kw):
-        return iter(d.lists(**kw))
-
-    viewkeys = operator.methodcaller("keys")
-
-    viewvalues = operator.methodcaller("values")
-
-    viewitems = operator.methodcaller("items")
-else:
-    def iterkeys(d, **kw):
-        return d.iterkeys(**kw)
-
-    def itervalues(d, **kw):
-        return d.itervalues(**kw)
-
-    def iteritems(d, **kw):
-        return d.iteritems(**kw)
-
-    def iterlists(d, **kw):
-        return d.iterlists(**kw)
-
-    viewkeys = operator.methodcaller("viewkeys")
-
-    viewvalues = operator.methodcaller("viewvalues")
-
-    viewitems = operator.methodcaller("viewitems")
-
-_add_doc(iterkeys, "Return an iterator over the keys of a dictionary.")
-_add_doc(itervalues, "Return an iterator over the values of a dictionary.")
-_add_doc(iteritems,
-         "Return an iterator over the (key, value) pairs of a dictionary.")
-_add_doc(iterlists,
-         "Return an iterator over the (key, [values]) pairs of a dictionary.")
-
-
-if PY3:
-    def b(s):
-        return s.encode("latin-1")
-
-    def u(s):
-        return s
-    unichr = chr
-    import struct
-    int2byte = struct.Struct(">B").pack
-    del struct
-    byte2int = operator.itemgetter(0)
-    indexbytes = operator.getitem
-    iterbytes = iter
-    import io
-    StringIO = io.StringIO
-    BytesIO = io.BytesIO
-    _assertCountEqual = "assertCountEqual"
-    if sys.version_info[1] <= 1:
-        _assertRaisesRegex = "assertRaisesRegexp"
-        _assertRegex = "assertRegexpMatches"
-    else:
-        _assertRaisesRegex = "assertRaisesRegex"
-        _assertRegex = "assertRegex"
-else:
-    def b(s):
-        return s
-    # Workaround for standalone backslash
-
-    def u(s):
-        return unicode(s.replace(r'\\', r'\\\\'), "unicode_escape")
-    unichr = unichr
-    int2byte = chr
-
-    def byte2int(bs):
-        return ord(bs[0])
-
-    def indexbytes(buf, i):
-        return ord(buf[i])
-    iterbytes = functools.partial(itertools.imap, ord)
-    import StringIO
-    StringIO = BytesIO = StringIO.StringIO
-    _assertCountEqual = "assertItemsEqual"
-    _assertRaisesRegex = "assertRaisesRegexp"
-    _assertRegex = "assertRegexpMatches"
-_add_doc(b, """Byte literal""")
-_add_doc(u, """Text literal""")
-
-
-def assertCountEqual(self, *args, **kwargs):
-    return getattr(self, _assertCountEqual)(*args, **kwargs)
-
-
-def assertRaisesRegex(self, *args, **kwargs):
-    return getattr(self, _assertRaisesRegex)(*args, **kwargs)
-
-
-def assertRegex(self, *args, **kwargs):
-    return getattr(self, _assertRegex)(*args, **kwargs)
-
-
-if PY3:
-    exec_ = getattr(moves.builtins, "exec")
-
-    def reraise(tp, value, tb=None):
-        if value is None:
-            value = tp()
-        if value.__traceback__ is not tb:
-            raise value.with_traceback(tb)
-        raise value
-
-else:
-    def exec_(_code_, _globs_=None, _locs_=None):
-        """Execute code in a namespace."""
-        if _globs_ is None:
-            frame = sys._getframe(1)
-            _globs_ = frame.f_globals
-            if _locs_ is None:
-                _locs_ = frame.f_locals
-            del frame
-        elif _locs_ is None:
-            _locs_ = _globs_
-        exec("""exec _code_ in _globs_, _locs_""")
-
-    exec_("""def reraise(tp, value, tb=None):
-    raise tp, value, tb
-""")
-
-
-if sys.version_info[:2] == (3, 2):
-    exec_("""def raise_from(value, from_value):
-    if from_value is None:
-        raise value
-    raise value from from_value
-""")
-elif sys.version_info[:2] > (3, 2):
-    exec_("""def raise_from(value, from_value):
-    raise value from from_value
-""")
-else:
-    def raise_from(value, from_value):
-        raise value
-
-
-print_ = getattr(moves.builtins, "print", None)
-if print_ is None:
-    def print_(*args, **kwargs):
-        """The new-style print function for Python 2.4 and 2.5."""
-        fp = kwargs.pop("file", sys.stdout)
-        if fp is None:
-            return
-
-        def write(data):
-            if not isinstance(data, basestring):
-                data = str(data)
-            # If the file has an encoding, encode unicode with it.
-            if (isinstance(fp, file) and
-                    isinstance(data, unicode) and
-                    fp.encoding is not None):
-                errors = getattr(fp, "errors", None)
-                if errors is None:
-                    errors = "strict"
-                data = data.encode(fp.encoding, errors)
-            fp.write(data)
-        want_unicode = False
-        sep = kwargs.pop("sep", None)
-        if sep is not None:
-            if isinstance(sep, unicode):
-                want_unicode = True
-            elif not isinstance(sep, str):
-                raise TypeError("sep must be None or a string")
-        end = kwargs.pop("end", None)
-        if end is not None:
-            if isinstance(end, unicode):
-                want_unicode = True
-            elif not isinstance(end, str):
-                raise TypeError("end must be None or a string")
-        if kwargs:
-            raise TypeError("invalid keyword arguments to print()")
-        if not want_unicode:
-            for arg in args:
-                if isinstance(arg, unicode):
-                    want_unicode = True
-                    break
-        if want_unicode:
-            newline = unicode("\n")
-            space = unicode(" ")
-        else:
-            newline = "\n"
-            space = " "
-        if sep is None:
-            sep = space
-        if end is None:
-            end = newline
-        for i, arg in enumerate(args):
-            if i:
-                write(sep)
-            write(arg)
-        write(end)
-if sys.version_info[:2] < (3, 3):
-    _print = print_
-
-    def print_(*args, **kwargs):
-        fp = kwargs.get("file", sys.stdout)
-        flush = kwargs.pop("flush", False)
-        _print(*args, **kwargs)
-        if flush and fp is not None:
-            fp.flush()
-
-_add_doc(reraise, """Reraise an exception.""")
-
-if sys.version_info[0:2] < (3, 4):
-    def wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS,
-              updated=functools.WRAPPER_UPDATES):
-        def wrapper(f):
-            f = functools.wraps(wrapped, assigned, updated)(f)
-            f.__wrapped__ = wrapped
-            return f
-        return wrapper
-else:
-    wraps = functools.wraps
-
-
-def with_metaclass(meta, *bases):
-    """Create a base class with a metaclass."""
-    # This requires a bit of explanation: the basic idea is to make a dummy
-    # metaclass for one level of class instantiation that replaces itself with
-    # the actual metaclass.
-    class metaclass(meta):
-
-        def __new__(cls, name, this_bases, d):
-            return meta(name, bases, d)
-    return type.__new__(metaclass, 'temporary_class', (), {})
-
-
-def add_metaclass(metaclass):
-    """Class decorator for creating a class with a metaclass."""
-    def wrapper(cls):
-        orig_vars = cls.__dict__.copy()
-        slots = orig_vars.get('__slots__')
-        if slots is not None:
-            if isinstance(slots, str):
-                slots = [slots]
-            for slots_var in slots:
-                orig_vars.pop(slots_var)
-        orig_vars.pop('__dict__', None)
-        orig_vars.pop('__weakref__', None)
-        return metaclass(cls.__name__, cls.__bases__, orig_vars)
-    return wrapper
-
-
-def python_2_unicode_compatible(klass):
-    """
-    A decorator that defines __unicode__ and __str__ methods under Python 2.
-    Under Python 3 it does nothing.
-
-    To support Python 2 and 3 with a single code base, define a __str__ method
-    returning text and apply this decorator to the class.
-    """
-    if PY2:
-        if '__str__' not in klass.__dict__:
-            raise ValueError("@python_2_unicode_compatible cannot be applied "
-                             "to %s because it doesn't define __str__()." %
-                             klass.__name__)
-        klass.__unicode__ = klass.__str__
-        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')
-    return klass
-
-
-# Complete the moves implementation.
-# This code is at the end of this module to speed up module loading.
-# Turn this module into a package.
-__path__ = []  # required for PEP 302 and PEP 451
-__package__ = __name__  # see PEP 366 @ReservedAssignment
-if globals().get("__spec__") is not None:
-    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable
-# Remove other six meta path importers, since they cause problems. This can
-# happen if six is removed from sys.modules and then reloaded. (Setuptools does
-# this for some reason.)
-if sys.meta_path:
-    for i, importer in enumerate(sys.meta_path):
-        # Here's some real nastiness: Another "instance" of the six module might
-        # be floating around. Therefore, we can't use isinstance() to check for
-        # the six meta path importer, since the other six instance will have
-        # inserted an importer with different class.
-        if (type(importer).__name__ == "_SixMetaPathImporter" and
-                importer.name == __name__):
-            del sys.meta_path[i]
-            break
-    del i, importer
-# Finally, add the importer to the meta path import hook.
-sys.meta_path.append(_importer)
Index: env/lib/python3.8/site-packages/setuptools-49.2.1.dist-info/top_level.txt
===================================================================
--- env/lib/python3.8/site-packages/setuptools-49.2.1.dist-info/top_level.txt	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools-49.2.1.dist-info/top_level.txt	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
@@ -1,3 +0,0 @@
-easy_install
-pkg_resources
-setuptools
Index: env/lib/python3.8/site-packages/setuptools/py31compat.py
===================================================================
--- env/lib/python3.8/site-packages/setuptools/py31compat.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/py31compat.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
@@ -1,32 +0,0 @@
-__all__ = []
-
-__metaclass__ = type
-
-
-try:
-    # Python >=3.2
-    from tempfile import TemporaryDirectory
-except ImportError:
-    import shutil
-    import tempfile
-
-    class TemporaryDirectory:
-        """
-        Very simple temporary directory context manager.
-        Will try to delete afterward, but will also ignore OS and similar
-        errors on deletion.
-        """
-
-        def __init__(self, **kwargs):
-            self.name = None  # Handle mkdtemp raising an exception
-            self.name = tempfile.mkdtemp(**kwargs)
-
-        def __enter__(self):
-            return self.name
-
-        def __exit__(self, exctype, excvalue, exctrace):
-            try:
-                shutil.rmtree(self.name, True)
-            except OSError:  # removal errors are not the only possible
-                pass
-            self.name = None
Index: env/lib/python3.8/site-packages/pip-20.2.1.dist-info/entry_points.txt
===================================================================
--- env/lib/python3.8/site-packages/pip-20.2.1.dist-info/entry_points.txt	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pip-20.2.1.dist-info/entry_points.txt	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
@@ -1,5 +0,0 @@
-[console_scripts]
-pip = pip._internal.cli.main:main
-pip3 = pip._internal.cli.main:main
-pip3.8 = pip._internal.cli.main:main
-
Index: env/lib/python3.8/site-packages/setuptools/py33compat.py
===================================================================
--- env/lib/python3.8/site-packages/setuptools/py33compat.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/py33compat.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
@@ -1,59 +0,0 @@
-import dis
-import array
-import collections
-
-try:
-    import html
-except ImportError:
-    html = None
-
-from setuptools.extern import six
-from setuptools.extern.six.moves import html_parser
-
-__metaclass__ = type
-
-OpArg = collections.namedtuple('OpArg', 'opcode arg')
-
-
-class Bytecode_compat:
-    def __init__(self, code):
-        self.code = code
-
-    def __iter__(self):
-        """Yield '(op,arg)' pair for each operation in code object 'code'"""
-
-        bytes = array.array('b', self.code.co_code)
-        eof = len(self.code.co_code)
-
-        ptr = 0
-        extended_arg = 0
-
-        while ptr < eof:
-
-            op = bytes[ptr]
-
-            if op >= dis.HAVE_ARGUMENT:
-
-                arg = bytes[ptr + 1] + bytes[ptr + 2] * 256 + extended_arg
-                ptr += 3
-
-                if op == dis.EXTENDED_ARG:
-                    long_type = six.integer_types[-1]
-                    extended_arg = arg * long_type(65536)
-                    continue
-
-            else:
-                arg = None
-                ptr += 1
-
-            yield OpArg(op, arg)
-
-
-Bytecode = getattr(dis, 'Bytecode', Bytecode_compat)
-
-
-unescape = getattr(html, 'unescape', None)
-if unescape is None:
-    # HTMLParser.unescape is deprecated since Python 3.4, and will be removed
-    # from 3.9.
-    unescape = html_parser.HTMLParser().unescape
Index: env/lib/python3.8/site-packages/pkg_resources/_vendor/six.py
===================================================================
--- env/lib/python3.8/site-packages/pkg_resources/_vendor/six.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pkg_resources/_vendor/six.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
@@ -1,868 +0,0 @@
-"""Utilities for writing code that runs on Python 2 and 3"""
-
-# Copyright (c) 2010-2015 Benjamin Peterson
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in all
-# copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-# SOFTWARE.
-
-from __future__ import absolute_import
-
-import functools
-import itertools
-import operator
-import sys
-import types
-
-__author__ = "Benjamin Peterson <benjamin@python.org>"
-__version__ = "1.10.0"
-
-
-# Useful for very coarse version differentiation.
-PY2 = sys.version_info[0] == 2
-PY3 = sys.version_info[0] == 3
-PY34 = sys.version_info[0:2] >= (3, 4)
-
-if PY3:
-    string_types = str,
-    integer_types = int,
-    class_types = type,
-    text_type = str
-    binary_type = bytes
-
-    MAXSIZE = sys.maxsize
-else:
-    string_types = basestring,
-    integer_types = (int, long)
-    class_types = (type, types.ClassType)
-    text_type = unicode
-    binary_type = str
-
-    if sys.platform.startswith("java"):
-        # Jython always uses 32 bits.
-        MAXSIZE = int((1 << 31) - 1)
-    else:
-        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).
-        class X(object):
-
-            def __len__(self):
-                return 1 << 31
-        try:
-            len(X())
-        except OverflowError:
-            # 32-bit
-            MAXSIZE = int((1 << 31) - 1)
-        else:
-            # 64-bit
-            MAXSIZE = int((1 << 63) - 1)
-        del X
-
-
-def _add_doc(func, doc):
-    """Add documentation to a function."""
-    func.__doc__ = doc
-
-
-def _import_module(name):
-    """Import module, returning the module after the last dot."""
-    __import__(name)
-    return sys.modules[name]
-
-
-class _LazyDescr(object):
-
-    def __init__(self, name):
-        self.name = name
-
-    def __get__(self, obj, tp):
-        result = self._resolve()
-        setattr(obj, self.name, result)  # Invokes __set__.
-        try:
-            # This is a bit ugly, but it avoids running this again by
-            # removing this descriptor.
-            delattr(obj.__class__, self.name)
-        except AttributeError:
-            pass
-        return result
-
-
-class MovedModule(_LazyDescr):
-
-    def __init__(self, name, old, new=None):
-        super(MovedModule, self).__init__(name)
-        if PY3:
-            if new is None:
-                new = name
-            self.mod = new
-        else:
-            self.mod = old
-
-    def _resolve(self):
-        return _import_module(self.mod)
-
-    def __getattr__(self, attr):
-        _module = self._resolve()
-        value = getattr(_module, attr)
-        setattr(self, attr, value)
-        return value
-
-
-class _LazyModule(types.ModuleType):
-
-    def __init__(self, name):
-        super(_LazyModule, self).__init__(name)
-        self.__doc__ = self.__class__.__doc__
-
-    def __dir__(self):
-        attrs = ["__doc__", "__name__"]
-        attrs += [attr.name for attr in self._moved_attributes]
-        return attrs
-
-    # Subclasses should override this
-    _moved_attributes = []
-
-
-class MovedAttribute(_LazyDescr):
-
-    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):
-        super(MovedAttribute, self).__init__(name)
-        if PY3:
-            if new_mod is None:
-                new_mod = name
-            self.mod = new_mod
-            if new_attr is None:
-                if old_attr is None:
-                    new_attr = name
-                else:
-                    new_attr = old_attr
-            self.attr = new_attr
-        else:
-            self.mod = old_mod
-            if old_attr is None:
-                old_attr = name
-            self.attr = old_attr
-
-    def _resolve(self):
-        module = _import_module(self.mod)
-        return getattr(module, self.attr)
-
-
-class _SixMetaPathImporter(object):
-
-    """
-    A meta path importer to import six.moves and its submodules.
-
-    This class implements a PEP302 finder and loader. It should be compatible
-    with Python 2.5 and all existing versions of Python3
-    """
-
-    def __init__(self, six_module_name):
-        self.name = six_module_name
-        self.known_modules = {}
-
-    def _add_module(self, mod, *fullnames):
-        for fullname in fullnames:
-            self.known_modules[self.name + "." + fullname] = mod
-
-    def _get_module(self, fullname):
-        return self.known_modules[self.name + "." + fullname]
-
-    def find_module(self, fullname, path=None):
-        if fullname in self.known_modules:
-            return self
-        return None
-
-    def __get_module(self, fullname):
-        try:
-            return self.known_modules[fullname]
-        except KeyError:
-            raise ImportError("This loader does not know module " + fullname)
-
-    def load_module(self, fullname):
-        try:
-            # in case of a reload
-            return sys.modules[fullname]
-        except KeyError:
-            pass
-        mod = self.__get_module(fullname)
-        if isinstance(mod, MovedModule):
-            mod = mod._resolve()
-        else:
-            mod.__loader__ = self
-        sys.modules[fullname] = mod
-        return mod
-
-    def is_package(self, fullname):
-        """
-        Return true, if the named module is a package.
-
-        We need this method to get correct spec objects with
-        Python 3.4 (see PEP451)
-        """
-        return hasattr(self.__get_module(fullname), "__path__")
-
-    def get_code(self, fullname):
-        """Return None
-
-        Required, if is_package is implemented"""
-        self.__get_module(fullname)  # eventually raises ImportError
-        return None
-    get_source = get_code  # same as get_code
-
-_importer = _SixMetaPathImporter(__name__)
-
-
-class _MovedItems(_LazyModule):
-
-    """Lazy loading of moved objects"""
-    __path__ = []  # mark as package
-
-
-_moved_attributes = [
-    MovedAttribute("cStringIO", "cStringIO", "io", "StringIO"),
-    MovedAttribute("filter", "itertools", "builtins", "ifilter", "filter"),
-    MovedAttribute("filterfalse", "itertools", "itertools", "ifilterfalse", "filterfalse"),
-    MovedAttribute("input", "__builtin__", "builtins", "raw_input", "input"),
-    MovedAttribute("intern", "__builtin__", "sys"),
-    MovedAttribute("map", "itertools", "builtins", "imap", "map"),
-    MovedAttribute("getcwd", "os", "os", "getcwdu", "getcwd"),
-    MovedAttribute("getcwdb", "os", "os", "getcwd", "getcwdb"),
-    MovedAttribute("range", "__builtin__", "builtins", "xrange", "range"),
-    MovedAttribute("reload_module", "__builtin__", "importlib" if PY34 else "imp", "reload"),
-    MovedAttribute("reduce", "__builtin__", "functools"),
-    MovedAttribute("shlex_quote", "pipes", "shlex", "quote"),
-    MovedAttribute("StringIO", "StringIO", "io"),
-    MovedAttribute("UserDict", "UserDict", "collections"),
-    MovedAttribute("UserList", "UserList", "collections"),
-    MovedAttribute("UserString", "UserString", "collections"),
-    MovedAttribute("xrange", "__builtin__", "builtins", "xrange", "range"),
-    MovedAttribute("zip", "itertools", "builtins", "izip", "zip"),
-    MovedAttribute("zip_longest", "itertools", "itertools", "izip_longest", "zip_longest"),
-    MovedModule("builtins", "__builtin__"),
-    MovedModule("configparser", "ConfigParser"),
-    MovedModule("copyreg", "copy_reg"),
-    MovedModule("dbm_gnu", "gdbm", "dbm.gnu"),
-    MovedModule("_dummy_thread", "dummy_thread", "_dummy_thread"),
-    MovedModule("http_cookiejar", "cookielib", "http.cookiejar"),
-    MovedModule("http_cookies", "Cookie", "http.cookies"),
-    MovedModule("html_entities", "htmlentitydefs", "html.entities"),
-    MovedModule("html_parser", "HTMLParser", "html.parser"),
-    MovedModule("http_client", "httplib", "http.client"),
-    MovedModule("email_mime_multipart", "email.MIMEMultipart", "email.mime.multipart"),
-    MovedModule("email_mime_nonmultipart", "email.MIMENonMultipart", "email.mime.nonmultipart"),
-    MovedModule("email_mime_text", "email.MIMEText", "email.mime.text"),
-    MovedModule("email_mime_base", "email.MIMEBase", "email.mime.base"),
-    MovedModule("BaseHTTPServer", "BaseHTTPServer", "http.server"),
-    MovedModule("CGIHTTPServer", "CGIHTTPServer", "http.server"),
-    MovedModule("SimpleHTTPServer", "SimpleHTTPServer", "http.server"),
-    MovedModule("cPickle", "cPickle", "pickle"),
-    MovedModule("queue", "Queue"),
-    MovedModule("reprlib", "repr"),
-    MovedModule("socketserver", "SocketServer"),
-    MovedModule("_thread", "thread", "_thread"),
-    MovedModule("tkinter", "Tkinter"),
-    MovedModule("tkinter_dialog", "Dialog", "tkinter.dialog"),
-    MovedModule("tkinter_filedialog", "FileDialog", "tkinter.filedialog"),
-    MovedModule("tkinter_scrolledtext", "ScrolledText", "tkinter.scrolledtext"),
-    MovedModule("tkinter_simpledialog", "SimpleDialog", "tkinter.simpledialog"),
-    MovedModule("tkinter_tix", "Tix", "tkinter.tix"),
-    MovedModule("tkinter_ttk", "ttk", "tkinter.ttk"),
-    MovedModule("tkinter_constants", "Tkconstants", "tkinter.constants"),
-    MovedModule("tkinter_dnd", "Tkdnd", "tkinter.dnd"),
-    MovedModule("tkinter_colorchooser", "tkColorChooser",
-                "tkinter.colorchooser"),
-    MovedModule("tkinter_commondialog", "tkCommonDialog",
-                "tkinter.commondialog"),
-    MovedModule("tkinter_tkfiledialog", "tkFileDialog", "tkinter.filedialog"),
-    MovedModule("tkinter_font", "tkFont", "tkinter.font"),
-    MovedModule("tkinter_messagebox", "tkMessageBox", "tkinter.messagebox"),
-    MovedModule("tkinter_tksimpledialog", "tkSimpleDialog",
-                "tkinter.simpledialog"),
-    MovedModule("urllib_parse", __name__ + ".moves.urllib_parse", "urllib.parse"),
-    MovedModule("urllib_error", __name__ + ".moves.urllib_error", "urllib.error"),
-    MovedModule("urllib", __name__ + ".moves.urllib", __name__ + ".moves.urllib"),
-    MovedModule("urllib_robotparser", "robotparser", "urllib.robotparser"),
-    MovedModule("xmlrpc_client", "xmlrpclib", "xmlrpc.client"),
-    MovedModule("xmlrpc_server", "SimpleXMLRPCServer", "xmlrpc.server"),
-]
-# Add windows specific modules.
-if sys.platform == "win32":
-    _moved_attributes += [
-        MovedModule("winreg", "_winreg"),
-    ]
-
-for attr in _moved_attributes:
-    setattr(_MovedItems, attr.name, attr)
-    if isinstance(attr, MovedModule):
-        _importer._add_module(attr, "moves." + attr.name)
-del attr
-
-_MovedItems._moved_attributes = _moved_attributes
-
-moves = _MovedItems(__name__ + ".moves")
-_importer._add_module(moves, "moves")
-
-
-class Module_six_moves_urllib_parse(_LazyModule):
-
-    """Lazy loading of moved objects in six.moves.urllib_parse"""
-
-
-_urllib_parse_moved_attributes = [
-    MovedAttribute("ParseResult", "urlparse", "urllib.parse"),
-    MovedAttribute("SplitResult", "urlparse", "urllib.parse"),
-    MovedAttribute("parse_qs", "urlparse", "urllib.parse"),
-    MovedAttribute("parse_qsl", "urlparse", "urllib.parse"),
-    MovedAttribute("urldefrag", "urlparse", "urllib.parse"),
-    MovedAttribute("urljoin", "urlparse", "urllib.parse"),
-    MovedAttribute("urlparse", "urlparse", "urllib.parse"),
-    MovedAttribute("urlsplit", "urlparse", "urllib.parse"),
-    MovedAttribute("urlunparse", "urlparse", "urllib.parse"),
-    MovedAttribute("urlunsplit", "urlparse", "urllib.parse"),
-    MovedAttribute("quote", "urllib", "urllib.parse"),
-    MovedAttribute("quote_plus", "urllib", "urllib.parse"),
-    MovedAttribute("unquote", "urllib", "urllib.parse"),
-    MovedAttribute("unquote_plus", "urllib", "urllib.parse"),
-    MovedAttribute("urlencode", "urllib", "urllib.parse"),
-    MovedAttribute("splitquery", "urllib", "urllib.parse"),
-    MovedAttribute("splittag", "urllib", "urllib.parse"),
-    MovedAttribute("splituser", "urllib", "urllib.parse"),
-    MovedAttribute("uses_fragment", "urlparse", "urllib.parse"),
-    MovedAttribute("uses_netloc", "urlparse", "urllib.parse"),
-    MovedAttribute("uses_params", "urlparse", "urllib.parse"),
-    MovedAttribute("uses_query", "urlparse", "urllib.parse"),
-    MovedAttribute("uses_relative", "urlparse", "urllib.parse"),
-]
-for attr in _urllib_parse_moved_attributes:
-    setattr(Module_six_moves_urllib_parse, attr.name, attr)
-del attr
-
-Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes
-
-_importer._add_module(Module_six_moves_urllib_parse(__name__ + ".moves.urllib_parse"),
-                      "moves.urllib_parse", "moves.urllib.parse")
-
-
-class Module_six_moves_urllib_error(_LazyModule):
-
-    """Lazy loading of moved objects in six.moves.urllib_error"""
-
-
-_urllib_error_moved_attributes = [
-    MovedAttribute("URLError", "urllib2", "urllib.error"),
-    MovedAttribute("HTTPError", "urllib2", "urllib.error"),
-    MovedAttribute("ContentTooShortError", "urllib", "urllib.error"),
-]
-for attr in _urllib_error_moved_attributes:
-    setattr(Module_six_moves_urllib_error, attr.name, attr)
-del attr
-
-Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes
-
-_importer._add_module(Module_six_moves_urllib_error(__name__ + ".moves.urllib.error"),
-                      "moves.urllib_error", "moves.urllib.error")
-
-
-class Module_six_moves_urllib_request(_LazyModule):
-
-    """Lazy loading of moved objects in six.moves.urllib_request"""
-
-
-_urllib_request_moved_attributes = [
-    MovedAttribute("urlopen", "urllib2", "urllib.request"),
-    MovedAttribute("install_opener", "urllib2", "urllib.request"),
-    MovedAttribute("build_opener", "urllib2", "urllib.request"),
-    MovedAttribute("pathname2url", "urllib", "urllib.request"),
-    MovedAttribute("url2pathname", "urllib", "urllib.request"),
-    MovedAttribute("getproxies", "urllib", "urllib.request"),
-    MovedAttribute("Request", "urllib2", "urllib.request"),
-    MovedAttribute("OpenerDirector", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPDefaultErrorHandler", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPRedirectHandler", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPCookieProcessor", "urllib2", "urllib.request"),
-    MovedAttribute("ProxyHandler", "urllib2", "urllib.request"),
-    MovedAttribute("BaseHandler", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPPasswordMgr", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPPasswordMgrWithDefaultRealm", "urllib2", "urllib.request"),
-    MovedAttribute("AbstractBasicAuthHandler", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPBasicAuthHandler", "urllib2", "urllib.request"),
-    MovedAttribute("ProxyBasicAuthHandler", "urllib2", "urllib.request"),
-    MovedAttribute("AbstractDigestAuthHandler", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPDigestAuthHandler", "urllib2", "urllib.request"),
-    MovedAttribute("ProxyDigestAuthHandler", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPHandler", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPSHandler", "urllib2", "urllib.request"),
-    MovedAttribute("FileHandler", "urllib2", "urllib.request"),
-    MovedAttribute("FTPHandler", "urllib2", "urllib.request"),
-    MovedAttribute("CacheFTPHandler", "urllib2", "urllib.request"),
-    MovedAttribute("UnknownHandler", "urllib2", "urllib.request"),
-    MovedAttribute("HTTPErrorProcessor", "urllib2", "urllib.request"),
-    MovedAttribute("urlretrieve", "urllib", "urllib.request"),
-    MovedAttribute("urlcleanup", "urllib", "urllib.request"),
-    MovedAttribute("URLopener", "urllib", "urllib.request"),
-    MovedAttribute("FancyURLopener", "urllib", "urllib.request"),
-    MovedAttribute("proxy_bypass", "urllib", "urllib.request"),
-]
-for attr in _urllib_request_moved_attributes:
-    setattr(Module_six_moves_urllib_request, attr.name, attr)
-del attr
-
-Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes
-
-_importer._add_module(Module_six_moves_urllib_request(__name__ + ".moves.urllib.request"),
-                      "moves.urllib_request", "moves.urllib.request")
-
-
-class Module_six_moves_urllib_response(_LazyModule):
-
-    """Lazy loading of moved objects in six.moves.urllib_response"""
-
-
-_urllib_response_moved_attributes = [
-    MovedAttribute("addbase", "urllib", "urllib.response"),
-    MovedAttribute("addclosehook", "urllib", "urllib.response"),
-    MovedAttribute("addinfo", "urllib", "urllib.response"),
-    MovedAttribute("addinfourl", "urllib", "urllib.response"),
-]
-for attr in _urllib_response_moved_attributes:
-    setattr(Module_six_moves_urllib_response, attr.name, attr)
-del attr
-
-Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes
-
-_importer._add_module(Module_six_moves_urllib_response(__name__ + ".moves.urllib.response"),
-                      "moves.urllib_response", "moves.urllib.response")
-
-
-class Module_six_moves_urllib_robotparser(_LazyModule):
-
-    """Lazy loading of moved objects in six.moves.urllib_robotparser"""
-
-
-_urllib_robotparser_moved_attributes = [
-    MovedAttribute("RobotFileParser", "robotparser", "urllib.robotparser"),
-]
-for attr in _urllib_robotparser_moved_attributes:
-    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)
-del attr
-
-Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes
-
-_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + ".moves.urllib.robotparser"),
-                      "moves.urllib_robotparser", "moves.urllib.robotparser")
-
-
-class Module_six_moves_urllib(types.ModuleType):
-
-    """Create a six.moves.urllib namespace that resembles the Python 3 namespace"""
-    __path__ = []  # mark as package
-    parse = _importer._get_module("moves.urllib_parse")
-    error = _importer._get_module("moves.urllib_error")
-    request = _importer._get_module("moves.urllib_request")
-    response = _importer._get_module("moves.urllib_response")
-    robotparser = _importer._get_module("moves.urllib_robotparser")
-
-    def __dir__(self):
-        return ['parse', 'error', 'request', 'response', 'robotparser']
-
-_importer._add_module(Module_six_moves_urllib(__name__ + ".moves.urllib"),
-                      "moves.urllib")
-
-
-def add_move(move):
-    """Add an item to six.moves."""
-    setattr(_MovedItems, move.name, move)
-
-
-def remove_move(name):
-    """Remove item from six.moves."""
-    try:
-        delattr(_MovedItems, name)
-    except AttributeError:
-        try:
-            del moves.__dict__[name]
-        except KeyError:
-            raise AttributeError("no such move, %r" % (name,))
-
-
-if PY3:
-    _meth_func = "__func__"
-    _meth_self = "__self__"
-
-    _func_closure = "__closure__"
-    _func_code = "__code__"
-    _func_defaults = "__defaults__"
-    _func_globals = "__globals__"
-else:
-    _meth_func = "im_func"
-    _meth_self = "im_self"
-
-    _func_closure = "func_closure"
-    _func_code = "func_code"
-    _func_defaults = "func_defaults"
-    _func_globals = "func_globals"
-
-
-try:
-    advance_iterator = next
-except NameError:
-    def advance_iterator(it):
-        return it.next()
-next = advance_iterator
-
-
-try:
-    callable = callable
-except NameError:
-    def callable(obj):
-        return any("__call__" in klass.__dict__ for klass in type(obj).__mro__)
-
-
-if PY3:
-    def get_unbound_function(unbound):
-        return unbound
-
-    create_bound_method = types.MethodType
-
-    def create_unbound_method(func, cls):
-        return func
-
-    Iterator = object
-else:
-    def get_unbound_function(unbound):
-        return unbound.im_func
-
-    def create_bound_method(func, obj):
-        return types.MethodType(func, obj, obj.__class__)
-
-    def create_unbound_method(func, cls):
-        return types.MethodType(func, None, cls)
-
-    class Iterator(object):
-
-        def next(self):
-            return type(self).__next__(self)
-
-    callable = callable
-_add_doc(get_unbound_function,
-         """Get the function out of a possibly unbound function""")
-
-
-get_method_function = operator.attrgetter(_meth_func)
-get_method_self = operator.attrgetter(_meth_self)
-get_function_closure = operator.attrgetter(_func_closure)
-get_function_code = operator.attrgetter(_func_code)
-get_function_defaults = operator.attrgetter(_func_defaults)
-get_function_globals = operator.attrgetter(_func_globals)
-
-
-if PY3:
-    def iterkeys(d, **kw):
-        return iter(d.keys(**kw))
-
-    def itervalues(d, **kw):
-        return iter(d.values(**kw))
-
-    def iteritems(d, **kw):
-        return iter(d.items(**kw))
-
-    def iterlists(d, **kw):
-        return iter(d.lists(**kw))
-
-    viewkeys = operator.methodcaller("keys")
-
-    viewvalues = operator.methodcaller("values")
-
-    viewitems = operator.methodcaller("items")
-else:
-    def iterkeys(d, **kw):
-        return d.iterkeys(**kw)
-
-    def itervalues(d, **kw):
-        return d.itervalues(**kw)
-
-    def iteritems(d, **kw):
-        return d.iteritems(**kw)
-
-    def iterlists(d, **kw):
-        return d.iterlists(**kw)
-
-    viewkeys = operator.methodcaller("viewkeys")
-
-    viewvalues = operator.methodcaller("viewvalues")
-
-    viewitems = operator.methodcaller("viewitems")
-
-_add_doc(iterkeys, "Return an iterator over the keys of a dictionary.")
-_add_doc(itervalues, "Return an iterator over the values of a dictionary.")
-_add_doc(iteritems,
-         "Return an iterator over the (key, value) pairs of a dictionary.")
-_add_doc(iterlists,
-         "Return an iterator over the (key, [values]) pairs of a dictionary.")
-
-
-if PY3:
-    def b(s):
-        return s.encode("latin-1")
-
-    def u(s):
-        return s
-    unichr = chr
-    import struct
-    int2byte = struct.Struct(">B").pack
-    del struct
-    byte2int = operator.itemgetter(0)
-    indexbytes = operator.getitem
-    iterbytes = iter
-    import io
-    StringIO = io.StringIO
-    BytesIO = io.BytesIO
-    _assertCountEqual = "assertCountEqual"
-    if sys.version_info[1] <= 1:
-        _assertRaisesRegex = "assertRaisesRegexp"
-        _assertRegex = "assertRegexpMatches"
-    else:
-        _assertRaisesRegex = "assertRaisesRegex"
-        _assertRegex = "assertRegex"
-else:
-    def b(s):
-        return s
-    # Workaround for standalone backslash
-
-    def u(s):
-        return unicode(s.replace(r'\\', r'\\\\'), "unicode_escape")
-    unichr = unichr
-    int2byte = chr
-
-    def byte2int(bs):
-        return ord(bs[0])
-
-    def indexbytes(buf, i):
-        return ord(buf[i])
-    iterbytes = functools.partial(itertools.imap, ord)
-    import StringIO
-    StringIO = BytesIO = StringIO.StringIO
-    _assertCountEqual = "assertItemsEqual"
-    _assertRaisesRegex = "assertRaisesRegexp"
-    _assertRegex = "assertRegexpMatches"
-_add_doc(b, """Byte literal""")
-_add_doc(u, """Text literal""")
-
-
-def assertCountEqual(self, *args, **kwargs):
-    return getattr(self, _assertCountEqual)(*args, **kwargs)
-
-
-def assertRaisesRegex(self, *args, **kwargs):
-    return getattr(self, _assertRaisesRegex)(*args, **kwargs)
-
-
-def assertRegex(self, *args, **kwargs):
-    return getattr(self, _assertRegex)(*args, **kwargs)
-
-
-if PY3:
-    exec_ = getattr(moves.builtins, "exec")
-
-    def reraise(tp, value, tb=None):
-        if value is None:
-            value = tp()
-        if value.__traceback__ is not tb:
-            raise value.with_traceback(tb)
-        raise value
-
-else:
-    def exec_(_code_, _globs_=None, _locs_=None):
-        """Execute code in a namespace."""
-        if _globs_ is None:
-            frame = sys._getframe(1)
-            _globs_ = frame.f_globals
-            if _locs_ is None:
-                _locs_ = frame.f_locals
-            del frame
-        elif _locs_ is None:
-            _locs_ = _globs_
-        exec("""exec _code_ in _globs_, _locs_""")
-
-    exec_("""def reraise(tp, value, tb=None):
-    raise tp, value, tb
-""")
-
-
-if sys.version_info[:2] == (3, 2):
-    exec_("""def raise_from(value, from_value):
-    if from_value is None:
-        raise value
-    raise value from from_value
-""")
-elif sys.version_info[:2] > (3, 2):
-    exec_("""def raise_from(value, from_value):
-    raise value from from_value
-""")
-else:
-    def raise_from(value, from_value):
-        raise value
-
-
-print_ = getattr(moves.builtins, "print", None)
-if print_ is None:
-    def print_(*args, **kwargs):
-        """The new-style print function for Python 2.4 and 2.5."""
-        fp = kwargs.pop("file", sys.stdout)
-        if fp is None:
-            return
-
-        def write(data):
-            if not isinstance(data, basestring):
-                data = str(data)
-            # If the file has an encoding, encode unicode with it.
-            if (isinstance(fp, file) and
-                    isinstance(data, unicode) and
-                    fp.encoding is not None):
-                errors = getattr(fp, "errors", None)
-                if errors is None:
-                    errors = "strict"
-                data = data.encode(fp.encoding, errors)
-            fp.write(data)
-        want_unicode = False
-        sep = kwargs.pop("sep", None)
-        if sep is not None:
-            if isinstance(sep, unicode):
-                want_unicode = True
-            elif not isinstance(sep, str):
-                raise TypeError("sep must be None or a string")
-        end = kwargs.pop("end", None)
-        if end is not None:
-            if isinstance(end, unicode):
-                want_unicode = True
-            elif not isinstance(end, str):
-                raise TypeError("end must be None or a string")
-        if kwargs:
-            raise TypeError("invalid keyword arguments to print()")
-        if not want_unicode:
-            for arg in args:
-                if isinstance(arg, unicode):
-                    want_unicode = True
-                    break
-        if want_unicode:
-            newline = unicode("\n")
-            space = unicode(" ")
-        else:
-            newline = "\n"
-            space = " "
-        if sep is None:
-            sep = space
-        if end is None:
-            end = newline
-        for i, arg in enumerate(args):
-            if i:
-                write(sep)
-            write(arg)
-        write(end)
-if sys.version_info[:2] < (3, 3):
-    _print = print_
-
-    def print_(*args, **kwargs):
-        fp = kwargs.get("file", sys.stdout)
-        flush = kwargs.pop("flush", False)
-        _print(*args, **kwargs)
-        if flush and fp is not None:
-            fp.flush()
-
-_add_doc(reraise, """Reraise an exception.""")
-
-if sys.version_info[0:2] < (3, 4):
-    def wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS,
-              updated=functools.WRAPPER_UPDATES):
-        def wrapper(f):
-            f = functools.wraps(wrapped, assigned, updated)(f)
-            f.__wrapped__ = wrapped
-            return f
-        return wrapper
-else:
-    wraps = functools.wraps
-
-
-def with_metaclass(meta, *bases):
-    """Create a base class with a metaclass."""
-    # This requires a bit of explanation: the basic idea is to make a dummy
-    # metaclass for one level of class instantiation that replaces itself with
-    # the actual metaclass.
-    class metaclass(meta):
-
-        def __new__(cls, name, this_bases, d):
-            return meta(name, bases, d)
-    return type.__new__(metaclass, 'temporary_class', (), {})
-
-
-def add_metaclass(metaclass):
-    """Class decorator for creating a class with a metaclass."""
-    def wrapper(cls):
-        orig_vars = cls.__dict__.copy()
-        slots = orig_vars.get('__slots__')
-        if slots is not None:
-            if isinstance(slots, str):
-                slots = [slots]
-            for slots_var in slots:
-                orig_vars.pop(slots_var)
-        orig_vars.pop('__dict__', None)
-        orig_vars.pop('__weakref__', None)
-        return metaclass(cls.__name__, cls.__bases__, orig_vars)
-    return wrapper
-
-
-def python_2_unicode_compatible(klass):
-    """
-    A decorator that defines __unicode__ and __str__ methods under Python 2.
-    Under Python 3 it does nothing.
-
-    To support Python 2 and 3 with a single code base, define a __str__ method
-    returning text and apply this decorator to the class.
-    """
-    if PY2:
-        if '__str__' not in klass.__dict__:
-            raise ValueError("@python_2_unicode_compatible cannot be applied "
-                             "to %s because it doesn't define __str__()." %
-                             klass.__name__)
-        klass.__unicode__ = klass.__str__
-        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')
-    return klass
-
-
-# Complete the moves implementation.
-# This code is at the end of this module to speed up module loading.
-# Turn this module into a package.
-__path__ = []  # required for PEP 302 and PEP 451
-__package__ = __name__  # see PEP 366 @ReservedAssignment
-if globals().get("__spec__") is not None:
-    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable
-# Remove other six meta path importers, since they cause problems. This can
-# happen if six is removed from sys.modules and then reloaded. (Setuptools does
-# this for some reason.)
-if sys.meta_path:
-    for i, importer in enumerate(sys.meta_path):
-        # Here's some real nastiness: Another "instance" of the six module might
-        # be floating around. Therefore, we can't use isinstance() to check for
-        # the six meta path importer, since the other six instance will have
-        # inserted an importer with different class.
-        if (type(importer).__name__ == "_SixMetaPathImporter" and
-                importer.name == __name__):
-            del sys.meta_path[i]
-            break
-    del i, importer
-# Finally, add the importer to the meta path import hook.
-sys.meta_path.append(_importer)
Index: env/lib/python3.8/site-packages/setuptools/py27compat.py
===================================================================
--- env/lib/python3.8/site-packages/setuptools/py27compat.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/py27compat.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
@@ -1,60 +0,0 @@
-"""
-Compatibility Support for Python 2.7 and earlier
-"""
-
-import sys
-import platform
-
-from setuptools.extern import six
-
-
-def get_all_headers(message, key):
-    """
-    Given an HTTPMessage, return all headers matching a given key.
-    """
-    return message.get_all(key)
-
-
-if six.PY2:
-    def get_all_headers(message, key):  # noqa
-        return message.getheaders(key)
-
-
-linux_py2_ascii = (
-    platform.system() == 'Linux' and
-    six.PY2
-)
-
-rmtree_safe = str if linux_py2_ascii else lambda x: x
-"""Workaround for http://bugs.python.org/issue24672"""
-
-
-try:
-    from ._imp import find_module, PY_COMPILED, PY_FROZEN, PY_SOURCE
-    from ._imp import get_frozen_object, get_module
-except ImportError:
-    import imp
-    from imp import PY_COMPILED, PY_FROZEN, PY_SOURCE  # noqa
-
-    def find_module(module, paths=None):
-        """Just like 'imp.find_module()', but with package support"""
-        parts = module.split('.')
-        while parts:
-            part = parts.pop(0)
-            f, path, (suffix, mode, kind) = info = imp.find_module(part, paths)
-
-            if kind == imp.PKG_DIRECTORY:
-                parts = parts or ['__init__']
-                paths = [path]
-
-            elif parts:
-                raise ImportError("Can't find %r in %s" % (parts, module))
-
-        return info
-
-    def get_frozen_object(module, paths):
-        return imp.get_frozen_object(module)
-
-    def get_module(module, paths, info):
-        imp.load_module(module, *info)
-        return sys.modules[module]
Index: env/lib/python3.8/site-packages/setuptools/dist.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># -*- coding: utf-8 -*-\n__all__ = ['Distribution']\n\nimport io\nimport sys\nimport re\nimport os\nimport warnings\nimport numbers\nimport distutils.log\nimport distutils.core\nimport distutils.cmd\nimport distutils.dist\nfrom distutils.util import strtobool\nfrom distutils.debug import DEBUG\nfrom distutils.fancy_getopt import translate_longopt\nimport itertools\n\nfrom collections import defaultdict\nfrom email import message_from_file\n\nfrom distutils.errors import DistutilsOptionError, DistutilsSetupError\nfrom distutils.util import rfc822_escape\nfrom distutils.version import StrictVersion\n\nfrom setuptools.extern import six\nfrom setuptools.extern import packaging\nfrom setuptools.extern import ordered_set\nfrom setuptools.extern.six.moves import map, filter, filterfalse\n\nfrom . import SetuptoolsDeprecationWarning\n\nimport setuptools\nfrom setuptools import windows_support\nfrom setuptools.monkey import get_unpatched\nfrom setuptools.config import parse_configuration\nimport pkg_resources\n\n__import__('setuptools.extern.packaging.specifiers')\n__import__('setuptools.extern.packaging.version')\n\n\ndef _get_unpatched(cls):\n    warnings.warn(\"Do not call this function\", DistDeprecationWarning)\n    return get_unpatched(cls)\n\n\ndef get_metadata_version(self):\n    mv = getattr(self, 'metadata_version', None)\n\n    if mv is None:\n        if self.long_description_content_type or self.provides_extras:\n            mv = StrictVersion('2.1')\n        elif (self.maintainer is not None or\n              self.maintainer_email is not None or\n              getattr(self, 'python_requires', None) is not None or\n              self.project_urls):\n            mv = StrictVersion('1.2')\n        elif (self.provides or self.requires or self.obsoletes or\n                self.classifiers or self.download_url):\n            mv = StrictVersion('1.1')\n        else:\n            mv = StrictVersion('1.0')\n\n        self.metadata_version = mv\n\n    return mv\n\n\ndef read_pkg_file(self, file):\n    \"\"\"Reads the metadata values from a file object.\"\"\"\n    msg = message_from_file(file)\n\n    def _read_field(name):\n        value = msg[name]\n        if value == 'UNKNOWN':\n            return None\n        return value\n\n    def _read_list(name):\n        values = msg.get_all(name, None)\n        if values == []:\n            return None\n        return values\n\n    self.metadata_version = StrictVersion(msg['metadata-version'])\n    self.name = _read_field('name')\n    self.version = _read_field('version')\n    self.description = _read_field('summary')\n    # we are filling author only.\n    self.author = _read_field('author')\n    self.maintainer = None\n    self.author_email = _read_field('author-email')\n    self.maintainer_email = None\n    self.url = _read_field('home-page')\n    self.license = _read_field('license')\n\n    if 'download-url' in msg:\n        self.download_url = _read_field('download-url')\n    else:\n        self.download_url = None\n\n    self.long_description = _read_field('description')\n    self.description = _read_field('summary')\n\n    if 'keywords' in msg:\n        self.keywords = _read_field('keywords').split(',')\n\n    self.platforms = _read_list('platform')\n    self.classifiers = _read_list('classifier')\n\n    # PEP 314 - these fields only exist in 1.1\n    if self.metadata_version == StrictVersion('1.1'):\n        self.requires = _read_list('requires')\n        self.provides = _read_list('provides')\n        self.obsoletes = _read_list('obsoletes')\n    else:\n        self.requires = None\n        self.provides = None\n        self.obsoletes = None\n\n\n# Based on Python 3.5 version\ndef write_pkg_file(self, file):\n    \"\"\"Write the PKG-INFO format data to a file object.\n    \"\"\"\n    version = self.get_metadata_version()\n\n    if six.PY2:\n        def write_field(key, value):\n            file.write(\"%s: %s\\n\" % (key, self._encode_field(value)))\n    else:\n        def write_field(key, value):\n            file.write(\"%s: %s\\n\" % (key, value))\n\n    write_field('Metadata-Version', str(version))\n    write_field('Name', self.get_name())\n    write_field('Version', self.get_version())\n    write_field('Summary', self.get_description())\n    write_field('Home-page', self.get_url())\n\n    if version < StrictVersion('1.2'):\n        write_field('Author', self.get_contact())\n        write_field('Author-email', self.get_contact_email())\n    else:\n        optional_fields = (\n            ('Author', 'author'),\n            ('Author-email', 'author_email'),\n            ('Maintainer', 'maintainer'),\n            ('Maintainer-email', 'maintainer_email'),\n        )\n\n        for field, attr in optional_fields:\n            attr_val = getattr(self, attr)\n\n            if attr_val is not None:\n                write_field(field, attr_val)\n\n    write_field('License', self.get_license())\n    if self.download_url:\n        write_field('Download-URL', self.download_url)\n    for project_url in self.project_urls.items():\n        write_field('Project-URL', '%s, %s' % project_url)\n\n    long_desc = rfc822_escape(self.get_long_description())\n    write_field('Description', long_desc)\n\n    keywords = ','.join(self.get_keywords())\n    if keywords:\n        write_field('Keywords', keywords)\n\n    if version >= StrictVersion('1.2'):\n        for platform in self.get_platforms():\n            write_field('Platform', platform)\n    else:\n        self._write_list(file, 'Platform', self.get_platforms())\n\n    self._write_list(file, 'Classifier', self.get_classifiers())\n\n    # PEP 314\n    self._write_list(file, 'Requires', self.get_requires())\n    self._write_list(file, 'Provides', self.get_provides())\n    self._write_list(file, 'Obsoletes', self.get_obsoletes())\n\n    # Setuptools specific for PEP 345\n    if hasattr(self, 'python_requires'):\n        write_field('Requires-Python', self.python_requires)\n\n    # PEP 566\n    if self.long_description_content_type:\n        write_field(\n            'Description-Content-Type',\n            self.long_description_content_type\n        )\n    if self.provides_extras:\n        for extra in self.provides_extras:\n            write_field('Provides-Extra', extra)\n\n\nsequence = tuple, list\n\n\ndef check_importable(dist, attr, value):\n    try:\n        ep = pkg_resources.EntryPoint.parse('x=' + value)\n        assert not ep.extras\n    except (TypeError, ValueError, AttributeError, AssertionError) as e:\n        raise DistutilsSetupError(\n            \"%r must be importable 'module:attrs' string (got %r)\"\n            % (attr, value)\n        ) from e\n\n\ndef assert_string_list(dist, attr, value):\n    \"\"\"Verify that value is a string list\"\"\"\n    try:\n        # verify that value is a list or tuple to exclude unordered\n        # or single-use iterables\n        assert isinstance(value, (list, tuple))\n        # verify that elements of value are strings\n        assert ''.join(value) != value\n    except (TypeError, ValueError, AttributeError, AssertionError) as e:\n        raise DistutilsSetupError(\n            \"%r must be a list of strings (got %r)\" % (attr, value)\n        ) from e\n\n\ndef check_nsp(dist, attr, value):\n    \"\"\"Verify that namespace packages are valid\"\"\"\n    ns_packages = value\n    assert_string_list(dist, attr, ns_packages)\n    for nsp in ns_packages:\n        if not dist.has_contents_for(nsp):\n            raise DistutilsSetupError(\n                \"Distribution contains no modules or packages for \" +\n                \"namespace package %r\" % nsp\n            )\n        parent, sep, child = nsp.rpartition('.')\n        if parent and parent not in ns_packages:\n            distutils.log.warn(\n                \"WARNING: %r is declared as a package namespace, but %r\"\n                \" is not: please correct this in setup.py\", nsp, parent\n            )\n\n\ndef check_extras(dist, attr, value):\n    \"\"\"Verify that extras_require mapping is valid\"\"\"\n    try:\n        list(itertools.starmap(_check_extra, value.items()))\n    except (TypeError, ValueError, AttributeError) as e:\n        raise DistutilsSetupError(\n            \"'extras_require' must be a dictionary whose values are \"\n            \"strings or lists of strings containing valid project/version \"\n            \"requirement specifiers.\"\n        ) from e\n\n\ndef _check_extra(extra, reqs):\n    name, sep, marker = extra.partition(':')\n    if marker and pkg_resources.invalid_marker(marker):\n        raise DistutilsSetupError(\"Invalid environment marker: \" + marker)\n    list(pkg_resources.parse_requirements(reqs))\n\n\ndef assert_bool(dist, attr, value):\n    \"\"\"Verify that value is True, False, 0, or 1\"\"\"\n    if bool(value) != value:\n        tmpl = \"{attr!r} must be a boolean value (got {value!r})\"\n        raise DistutilsSetupError(tmpl.format(attr=attr, value=value))\n\n\ndef check_requirements(dist, attr, value):\n    \"\"\"Verify that install_requires is a valid requirements list\"\"\"\n    try:\n        list(pkg_resources.parse_requirements(value))\n        if isinstance(value, (dict, set)):\n            raise TypeError(\"Unordered types are not allowed\")\n    except (TypeError, ValueError) as error:\n        tmpl = (\n            \"{attr!r} must be a string or list of strings \"\n            \"containing valid project/version requirement specifiers; {error}\"\n        )\n        raise DistutilsSetupError(\n            tmpl.format(attr=attr, error=error)\n        ) from error\n\n\ndef check_specifier(dist, attr, value):\n    \"\"\"Verify that value is a valid version specifier\"\"\"\n    try:\n        packaging.specifiers.SpecifierSet(value)\n    except packaging.specifiers.InvalidSpecifier as error:\n        tmpl = (\n            \"{attr!r} must be a string \"\n            \"containing valid version specifiers; {error}\"\n        )\n        raise DistutilsSetupError(\n            tmpl.format(attr=attr, error=error)\n        ) from error\n\n\ndef check_entry_points(dist, attr, value):\n    \"\"\"Verify that entry_points map is parseable\"\"\"\n    try:\n        pkg_resources.EntryPoint.parse_map(value)\n    except ValueError as e:\n        raise DistutilsSetupError(e) from e\n\n\ndef check_test_suite(dist, attr, value):\n    if not isinstance(value, six.string_types):\n        raise DistutilsSetupError(\"test_suite must be a string\")\n\n\ndef check_package_data(dist, attr, value):\n    \"\"\"Verify that value is a dictionary of package names to glob lists\"\"\"\n    if not isinstance(value, dict):\n        raise DistutilsSetupError(\n            \"{!r} must be a dictionary mapping package names to lists of \"\n            \"string wildcard patterns\".format(attr))\n    for k, v in value.items():\n        if not isinstance(k, six.string_types):\n            raise DistutilsSetupError(\n                \"keys of {!r} dict must be strings (got {!r})\"\n                .format(attr, k)\n            )\n        assert_string_list(dist, 'values of {!r} dict'.format(attr), v)\n\n\ndef check_packages(dist, attr, value):\n    for pkgname in value:\n        if not re.match(r'\\w+(\\.\\w+)*', pkgname):\n            distutils.log.warn(\n                \"WARNING: %r not a valid package name; please use only \"\n                \".-separated package names in setup.py\", pkgname\n            )\n\n\n_Distribution = get_unpatched(distutils.core.Distribution)\n\n\nclass Distribution(_Distribution):\n    \"\"\"Distribution with support for tests and package data\n\n    This is an enhanced version of 'distutils.dist.Distribution' that\n    effectively adds the following new optional keyword arguments to 'setup()':\n\n     'install_requires' -- a string or sequence of strings specifying project\n        versions that the distribution requires when installed, in the format\n        used by 'pkg_resources.require()'.  They will be installed\n        automatically when the package is installed.  If you wish to use\n        packages that are not available in PyPI, or want to give your users an\n        alternate download location, you can add a 'find_links' option to the\n        '[easy_install]' section of your project's 'setup.cfg' file, and then\n        setuptools will scan the listed web pages for links that satisfy the\n        requirements.\n\n     'extras_require' -- a dictionary mapping names of optional \"extras\" to the\n        additional requirement(s) that using those extras incurs. For example,\n        this::\n\n            extras_require = dict(reST = [\"docutils>=0.3\", \"reSTedit\"])\n\n        indicates that the distribution can optionally provide an extra\n        capability called \"reST\", but it can only be used if docutils and\n        reSTedit are installed.  If the user installs your package using\n        EasyInstall and requests one of your extras, the corresponding\n        additional requirements will be installed if needed.\n\n     'test_suite' -- the name of a test suite to run for the 'test' command.\n        If the user runs 'python setup.py test', the package will be installed,\n        and the named test suite will be run.  The format is the same as\n        would be used on a 'unittest.py' command line.  That is, it is the\n        dotted name of an object to import and call to generate a test suite.\n\n     'package_data' -- a dictionary mapping package names to lists of filenames\n        or globs to use to find data files contained in the named packages.\n        If the dictionary has filenames or globs listed under '\"\"' (the empty\n        string), those names will be searched for in every package, in addition\n        to any names for the specific package.  Data files found using these\n        names/globs will be installed along with the package, in the same\n        location as the package.  Note that globs are allowed to reference\n        the contents of non-package subdirectories, as long as you use '/' as\n        a path separator.  (Globs are automatically converted to\n        platform-specific paths at runtime.)\n\n    In addition to these new keywords, this class also has several new methods\n    for manipulating the distribution's contents.  For example, the 'include()'\n    and 'exclude()' methods can be thought of as in-place add and subtract\n    commands that add or remove packages, modules, extensions, and so on from\n    the distribution.\n    \"\"\"\n\n    _DISTUTILS_UNSUPPORTED_METADATA = {\n        'long_description_content_type': None,\n        'project_urls': dict,\n        'provides_extras': ordered_set.OrderedSet,\n        'license_files': ordered_set.OrderedSet,\n    }\n\n    _patched_dist = None\n\n    def patch_missing_pkg_info(self, attrs):\n        # Fake up a replacement for the data that would normally come from\n        # PKG-INFO, but which might not yet be built if this is a fresh\n        # checkout.\n        #\n        if not attrs or 'name' not in attrs or 'version' not in attrs:\n            return\n        key = pkg_resources.safe_name(str(attrs['name'])).lower()\n        dist = pkg_resources.working_set.by_key.get(key)\n        if dist is not None and not dist.has_metadata('PKG-INFO'):\n            dist._version = pkg_resources.safe_version(str(attrs['version']))\n            self._patched_dist = dist\n\n    def __init__(self, attrs=None):\n        have_package_data = hasattr(self, \"package_data\")\n        if not have_package_data:\n            self.package_data = {}\n        attrs = attrs or {}\n        self.dist_files = []\n        # Filter-out setuptools' specific options.\n        self.src_root = attrs.pop(\"src_root\", None)\n        self.patch_missing_pkg_info(attrs)\n        self.dependency_links = attrs.pop('dependency_links', [])\n        self.setup_requires = attrs.pop('setup_requires', [])\n        for ep in pkg_resources.iter_entry_points('distutils.setup_keywords'):\n            vars(self).setdefault(ep.name, None)\n        _Distribution.__init__(self, {\n            k: v for k, v in attrs.items()\n            if k not in self._DISTUTILS_UNSUPPORTED_METADATA\n        })\n\n        # Fill-in missing metadata fields not supported by distutils.\n        # Note some fields may have been set by other tools (e.g. pbr)\n        # above; they are taken preferrentially to setup() arguments\n        for option, default in self._DISTUTILS_UNSUPPORTED_METADATA.items():\n            for source in self.metadata.__dict__, attrs:\n                if option in source:\n                    value = source[option]\n                    break\n            else:\n                value = default() if default else None\n            setattr(self.metadata, option, value)\n\n        self.metadata.version = self._normalize_version(\n            self._validate_version(self.metadata.version))\n        self._finalize_requires()\n\n    @staticmethod\n    def _normalize_version(version):\n        if isinstance(version, setuptools.sic) or version is None:\n            return version\n\n        normalized = str(packaging.version.Version(version))\n        if version != normalized:\n            tmpl = \"Normalizing '{version}' to '{normalized}'\"\n            warnings.warn(tmpl.format(**locals()))\n            return normalized\n        return version\n\n    @staticmethod\n    def _validate_version(version):\n        if isinstance(version, numbers.Number):\n            # Some people apparently take \"version number\" too literally :)\n            version = str(version)\n\n        if version is not None:\n            try:\n                packaging.version.Version(version)\n            except (packaging.version.InvalidVersion, TypeError):\n                warnings.warn(\n                    \"The version specified (%r) is an invalid version, this \"\n                    \"may not work as expected with newer versions of \"\n                    \"setuptools, pip, and PyPI. Please see PEP 440 for more \"\n                    \"details.\" % version\n                )\n                return setuptools.sic(version)\n        return version\n\n    def _finalize_requires(self):\n        \"\"\"\n        Set `metadata.python_requires` and fix environment markers\n        in `install_requires` and `extras_require`.\n        \"\"\"\n        if getattr(self, 'python_requires', None):\n            self.metadata.python_requires = self.python_requires\n\n        if getattr(self, 'extras_require', None):\n            for extra in self.extras_require.keys():\n                # Since this gets called multiple times at points where the\n                # keys have become 'converted' extras, ensure that we are only\n                # truly adding extras we haven't seen before here.\n                extra = extra.split(':')[0]\n                if extra:\n                    self.metadata.provides_extras.add(extra)\n\n        self._convert_extras_requirements()\n        self._move_install_requirements_markers()\n\n    def _convert_extras_requirements(self):\n        \"\"\"\n        Convert requirements in `extras_require` of the form\n        `\"extra\": [\"barbazquux; {marker}\"]` to\n        `\"extra:{marker}\": [\"barbazquux\"]`.\n        \"\"\"\n        spec_ext_reqs = getattr(self, 'extras_require', None) or {}\n        self._tmp_extras_require = defaultdict(list)\n        for section, v in spec_ext_reqs.items():\n            # Do not strip empty sections.\n            self._tmp_extras_require[section]\n            for r in pkg_resources.parse_requirements(v):\n                suffix = self._suffix_for(r)\n                self._tmp_extras_require[section + suffix].append(r)\n\n    @staticmethod\n    def _suffix_for(req):\n        \"\"\"\n        For a requirement, return the 'extras_require' suffix for\n        that requirement.\n        \"\"\"\n        return ':' + str(req.marker) if req.marker else ''\n\n    def _move_install_requirements_markers(self):\n        \"\"\"\n        Move requirements in `install_requires` that are using environment\n        markers `extras_require`.\n        \"\"\"\n\n        # divide the install_requires into two sets, simple ones still\n        # handled by install_requires and more complex ones handled\n        # by extras_require.\n\n        def is_simple_req(req):\n            return not req.marker\n\n        spec_inst_reqs = getattr(self, 'install_requires', None) or ()\n        inst_reqs = list(pkg_resources.parse_requirements(spec_inst_reqs))\n        simple_reqs = filter(is_simple_req, inst_reqs)\n        complex_reqs = filterfalse(is_simple_req, inst_reqs)\n        self.install_requires = list(map(str, simple_reqs))\n\n        for r in complex_reqs:\n            self._tmp_extras_require[':' + str(r.marker)].append(r)\n        self.extras_require = dict(\n            (k, [str(r) for r in map(self._clean_req, v)])\n            for k, v in self._tmp_extras_require.items()\n        )\n\n    def _clean_req(self, req):\n        \"\"\"\n        Given a Requirement, remove environment markers and return it.\n        \"\"\"\n        req.marker = None\n        return req\n\n    def _parse_config_files(self, filenames=None):\n        \"\"\"\n        Adapted from distutils.dist.Distribution.parse_config_files,\n        this method provides the same functionality in subtly-improved\n        ways.\n        \"\"\"\n        from setuptools.extern.six.moves.configparser import ConfigParser\n\n        # Ignore install directory options if we have a venv\n        if not six.PY2 and sys.prefix != sys.base_prefix:\n            ignore_options = [\n                'install-base', 'install-platbase', 'install-lib',\n                'install-platlib', 'install-purelib', 'install-headers',\n                'install-scripts', 'install-data', 'prefix', 'exec-prefix',\n                'home', 'user', 'root']\n        else:\n            ignore_options = []\n\n        ignore_options = frozenset(ignore_options)\n\n        if filenames is None:\n            filenames = self.find_config_files()\n\n        if DEBUG:\n            self.announce(\"Distribution.parse_config_files():\")\n\n        parser = ConfigParser()\n        for filename in filenames:\n            with io.open(filename, encoding='utf-8') as reader:\n                if DEBUG:\n                    self.announce(\"  reading {filename}\".format(**locals()))\n                (parser.readfp if six.PY2 else parser.read_file)(reader)\n            for section in parser.sections():\n                options = parser.options(section)\n                opt_dict = self.get_option_dict(section)\n\n                for opt in options:\n                    if opt != '__name__' and opt not in ignore_options:\n                        val = self._try_str(parser.get(section, opt))\n                        opt = opt.replace('-', '_')\n                        opt_dict[opt] = (filename, val)\n\n            # Make the ConfigParser forget everything (so we retain\n            # the original filenames that options come from)\n            parser.__init__()\n\n        # If there was a \"global\" section in the config file, use it\n        # to set Distribution options.\n\n        if 'global' in self.command_options:\n            for (opt, (src, val)) in self.command_options['global'].items():\n                alias = self.negative_opt.get(opt)\n                try:\n                    if alias:\n                        setattr(self, alias, not strtobool(val))\n                    elif opt in ('verbose', 'dry_run'):  # ugh!\n                        setattr(self, opt, strtobool(val))\n                    else:\n                        setattr(self, opt, val)\n                except ValueError as e:\n                    raise DistutilsOptionError(e) from e\n\n    @staticmethod\n    def _try_str(val):\n        \"\"\"\n        On Python 2, much of distutils relies on string values being of\n        type 'str' (bytes) and not unicode text. If the value can be safely\n        encoded to bytes using the default encoding, prefer that.\n\n        Why the default encoding? Because that value can be implicitly\n        decoded back to text if needed.\n\n        Ref #1653\n        \"\"\"\n        if not six.PY2:\n            return val\n        try:\n            return val.encode()\n        except UnicodeEncodeError:\n            pass\n        return val\n\n    def _set_command_options(self, command_obj, option_dict=None):\n        \"\"\"\n        Set the options for 'command_obj' from 'option_dict'.  Basically\n        this means copying elements of a dictionary ('option_dict') to\n        attributes of an instance ('command').\n\n        'command_obj' must be a Command instance.  If 'option_dict' is not\n        supplied, uses the standard option dictionary for this command\n        (from 'self.command_options').\n\n        (Adopted from distutils.dist.Distribution._set_command_options)\n        \"\"\"\n        command_name = command_obj.get_command_name()\n        if option_dict is None:\n            option_dict = self.get_option_dict(command_name)\n\n        if DEBUG:\n            self.announce(\"  setting options for '%s' command:\" % command_name)\n        for (option, (source, value)) in option_dict.items():\n            if DEBUG:\n                self.announce(\"    %s = %s (from %s)\" % (option, value,\n                                                         source))\n            try:\n                bool_opts = [translate_longopt(o)\n                             for o in command_obj.boolean_options]\n            except AttributeError:\n                bool_opts = []\n            try:\n                neg_opt = command_obj.negative_opt\n            except AttributeError:\n                neg_opt = {}\n\n            try:\n                is_string = isinstance(value, six.string_types)\n                if option in neg_opt and is_string:\n                    setattr(command_obj, neg_opt[option], not strtobool(value))\n                elif option in bool_opts and is_string:\n                    setattr(command_obj, option, strtobool(value))\n                elif hasattr(command_obj, option):\n                    setattr(command_obj, option, value)\n                else:\n                    raise DistutilsOptionError(\n                        \"error in %s: command '%s' has no such option '%s'\"\n                        % (source, command_name, option))\n            except ValueError as e:\n                raise DistutilsOptionError(e) from e\n\n    def parse_config_files(self, filenames=None, ignore_option_errors=False):\n        \"\"\"Parses configuration files from various levels\n        and loads configuration.\n\n        \"\"\"\n        self._parse_config_files(filenames=filenames)\n\n        parse_configuration(self, self.command_options,\n                            ignore_option_errors=ignore_option_errors)\n        self._finalize_requires()\n\n    def fetch_build_eggs(self, requires):\n        \"\"\"Resolve pre-setup requirements\"\"\"\n        resolved_dists = pkg_resources.working_set.resolve(\n            pkg_resources.parse_requirements(requires),\n            installer=self.fetch_build_egg,\n            replace_conflicting=True,\n        )\n        for dist in resolved_dists:\n            pkg_resources.working_set.add(dist, replace=True)\n        return resolved_dists\n\n    def finalize_options(self):\n        \"\"\"\n        Allow plugins to apply arbitrary operations to the\n        distribution. Each hook may optionally define a 'order'\n        to influence the order of execution. Smaller numbers\n        go first and the default is 0.\n        \"\"\"\n        group = 'setuptools.finalize_distribution_options'\n\n        def by_order(hook):\n            return getattr(hook, 'order', 0)\n        eps = map(lambda e: e.load(), pkg_resources.iter_entry_points(group))\n        for ep in sorted(eps, key=by_order):\n            ep(self)\n\n    def _finalize_setup_keywords(self):\n        for ep in pkg_resources.iter_entry_points('distutils.setup_keywords'):\n            value = getattr(self, ep.name, None)\n            if value is not None:\n                ep.require(installer=self.fetch_build_egg)\n                ep.load()(self, ep.name, value)\n\n    def _finalize_2to3_doctests(self):\n        if getattr(self, 'convert_2to3_doctests', None):\n            # XXX may convert to set here when we can rely on set being builtin\n            self.convert_2to3_doctests = [\n                os.path.abspath(p)\n                for p in self.convert_2to3_doctests\n            ]\n        else:\n            self.convert_2to3_doctests = []\n\n    def get_egg_cache_dir(self):\n        egg_cache_dir = os.path.join(os.curdir, '.eggs')\n        if not os.path.exists(egg_cache_dir):\n            os.mkdir(egg_cache_dir)\n            windows_support.hide_file(egg_cache_dir)\n            readme_txt_filename = os.path.join(egg_cache_dir, 'README.txt')\n            with open(readme_txt_filename, 'w') as f:\n                f.write('This directory contains eggs that were downloaded '\n                        'by setuptools to build, test, and run plug-ins.\\n\\n')\n                f.write('This directory caches those eggs to prevent '\n                        'repeated downloads.\\n\\n')\n                f.write('However, it is safe to delete this directory.\\n\\n')\n\n        return egg_cache_dir\n\n    def fetch_build_egg(self, req):\n        \"\"\"Fetch an egg needed for building\"\"\"\n        from setuptools.installer import fetch_build_egg\n        return fetch_build_egg(self, req)\n\n    def get_command_class(self, command):\n        \"\"\"Pluggable version of get_command_class()\"\"\"\n        if command in self.cmdclass:\n            return self.cmdclass[command]\n\n        eps = pkg_resources.iter_entry_points('distutils.commands', command)\n        for ep in eps:\n            ep.require(installer=self.fetch_build_egg)\n            self.cmdclass[command] = cmdclass = ep.load()\n            return cmdclass\n        else:\n            return _Distribution.get_command_class(self, command)\n\n    def print_commands(self):\n        for ep in pkg_resources.iter_entry_points('distutils.commands'):\n            if ep.name not in self.cmdclass:\n                # don't require extras as the commands won't be invoked\n                cmdclass = ep.resolve()\n                self.cmdclass[ep.name] = cmdclass\n        return _Distribution.print_commands(self)\n\n    def get_command_list(self):\n        for ep in pkg_resources.iter_entry_points('distutils.commands'):\n            if ep.name not in self.cmdclass:\n                # don't require extras as the commands won't be invoked\n                cmdclass = ep.resolve()\n                self.cmdclass[ep.name] = cmdclass\n        return _Distribution.get_command_list(self)\n\n    def include(self, **attrs):\n        \"\"\"Add items to distribution that are named in keyword arguments\n\n        For example, 'dist.include(py_modules=[\"x\"])' would add 'x' to\n        the distribution's 'py_modules' attribute, if it was not already\n        there.\n\n        Currently, this method only supports inclusion for attributes that are\n        lists or tuples.  If you need to add support for adding to other\n        attributes in this or a subclass, you can add an '_include_X' method,\n        where 'X' is the name of the attribute.  The method will be called with\n        the value passed to 'include()'.  So, 'dist.include(foo={\"bar\":\"baz\"})'\n        will try to call 'dist._include_foo({\"bar\":\"baz\"})', which can then\n        handle whatever special inclusion logic is needed.\n        \"\"\"\n        for k, v in attrs.items():\n            include = getattr(self, '_include_' + k, None)\n            if include:\n                include(v)\n            else:\n                self._include_misc(k, v)\n\n    def exclude_package(self, package):\n        \"\"\"Remove packages, modules, and extensions in named package\"\"\"\n\n        pfx = package + '.'\n        if self.packages:\n            self.packages = [\n                p for p in self.packages\n                if p != package and not p.startswith(pfx)\n            ]\n\n        if self.py_modules:\n            self.py_modules = [\n                p for p in self.py_modules\n                if p != package and not p.startswith(pfx)\n            ]\n\n        if self.ext_modules:\n            self.ext_modules = [\n                p for p in self.ext_modules\n                if p.name != package and not p.name.startswith(pfx)\n            ]\n\n    def has_contents_for(self, package):\n        \"\"\"Return true if 'exclude_package(package)' would do something\"\"\"\n\n        pfx = package + '.'\n\n        for p in self.iter_distribution_names():\n            if p == package or p.startswith(pfx):\n                return True\n\n    def _exclude_misc(self, name, value):\n        \"\"\"Handle 'exclude()' for list/tuple attrs without a special handler\"\"\"\n        if not isinstance(value, sequence):\n            raise DistutilsSetupError(\n                \"%s: setting must be a list or tuple (%r)\" % (name, value)\n            )\n        try:\n            old = getattr(self, name)\n        except AttributeError as e:\n            raise DistutilsSetupError(\n                \"%s: No such distribution setting\" % name\n            ) from e\n        if old is not None and not isinstance(old, sequence):\n            raise DistutilsSetupError(\n                name + \": this setting cannot be changed via include/exclude\"\n            )\n        elif old:\n            setattr(self, name, [item for item in old if item not in value])\n\n    def _include_misc(self, name, value):\n        \"\"\"Handle 'include()' for list/tuple attrs without a special handler\"\"\"\n\n        if not isinstance(value, sequence):\n            raise DistutilsSetupError(\n                \"%s: setting must be a list (%r)\" % (name, value)\n            )\n        try:\n            old = getattr(self, name)\n        except AttributeError as e:\n            raise DistutilsSetupError(\n                \"%s: No such distribution setting\" % name\n            ) from e\n        if old is None:\n            setattr(self, name, value)\n        elif not isinstance(old, sequence):\n            raise DistutilsSetupError(\n                name + \": this setting cannot be changed via include/exclude\"\n            )\n        else:\n            new = [item for item in value if item not in old]\n            setattr(self, name, old + new)\n\n    def exclude(self, **attrs):\n        \"\"\"Remove items from distribution that are named in keyword arguments\n\n        For example, 'dist.exclude(py_modules=[\"x\"])' would remove 'x' from\n        the distribution's 'py_modules' attribute.  Excluding packages uses\n        the 'exclude_package()' method, so all of the package's contained\n        packages, modules, and extensions are also excluded.\n\n        Currently, this method only supports exclusion from attributes that are\n        lists or tuples.  If you need to add support for excluding from other\n        attributes in this or a subclass, you can add an '_exclude_X' method,\n        where 'X' is the name of the attribute.  The method will be called with\n        the value passed to 'exclude()'.  So, 'dist.exclude(foo={\"bar\":\"baz\"})'\n        will try to call 'dist._exclude_foo({\"bar\":\"baz\"})', which can then\n        handle whatever special exclusion logic is needed.\n        \"\"\"\n        for k, v in attrs.items():\n            exclude = getattr(self, '_exclude_' + k, None)\n            if exclude:\n                exclude(v)\n            else:\n                self._exclude_misc(k, v)\n\n    def _exclude_packages(self, packages):\n        if not isinstance(packages, sequence):\n            raise DistutilsSetupError(\n                \"packages: setting must be a list or tuple (%r)\" % (packages,)\n            )\n        list(map(self.exclude_package, packages))\n\n    def _parse_command_opts(self, parser, args):\n        # Remove --with-X/--without-X options when processing command args\n        self.global_options = self.__class__.global_options\n        self.negative_opt = self.__class__.negative_opt\n\n        # First, expand any aliases\n        command = args[0]\n        aliases = self.get_option_dict('aliases')\n        while command in aliases:\n            src, alias = aliases[command]\n            del aliases[command]  # ensure each alias can expand only once!\n            import shlex\n            args[:1] = shlex.split(alias, True)\n            command = args[0]\n\n        nargs = _Distribution._parse_command_opts(self, parser, args)\n\n        # Handle commands that want to consume all remaining arguments\n        cmd_class = self.get_command_class(command)\n        if getattr(cmd_class, 'command_consumes_arguments', None):\n            self.get_option_dict(command)['args'] = (\"command line\", nargs)\n            if nargs is not None:\n                return []\n\n        return nargs\n\n    def get_cmdline_options(self):\n        \"\"\"Return a '{cmd: {opt:val}}' map of all command-line options\n\n        Option names are all long, but do not include the leading '--', and\n        contain dashes rather than underscores.  If the option doesn't take\n        an argument (e.g. '--quiet'), the 'val' is 'None'.\n\n        Note that options provided by config files are intentionally excluded.\n        \"\"\"\n\n        d = {}\n\n        for cmd, opts in self.command_options.items():\n\n            for opt, (src, val) in opts.items():\n\n                if src != \"command line\":\n                    continue\n\n                opt = opt.replace('_', '-')\n\n                if val == 0:\n                    cmdobj = self.get_command_obj(cmd)\n                    neg_opt = self.negative_opt.copy()\n                    neg_opt.update(getattr(cmdobj, 'negative_opt', {}))\n                    for neg, pos in neg_opt.items():\n                        if pos == opt:\n                            opt = neg\n                            val = None\n                            break\n                    else:\n                        raise AssertionError(\"Shouldn't be able to get here\")\n\n                elif val == 1:\n                    val = None\n\n                d.setdefault(cmd, {})[opt] = val\n\n        return d\n\n    def iter_distribution_names(self):\n        \"\"\"Yield all packages, modules, and extension names in distribution\"\"\"\n\n        for pkg in self.packages or ():\n            yield pkg\n\n        for module in self.py_modules or ():\n            yield module\n\n        for ext in self.ext_modules or ():\n            if isinstance(ext, tuple):\n                name, buildinfo = ext\n            else:\n                name = ext.name\n            if name.endswith('module'):\n                name = name[:-6]\n            yield name\n\n    def handle_display_options(self, option_order):\n        \"\"\"If there were any non-global \"display-only\" options\n        (--help-commands or the metadata display options) on the command\n        line, display the requested info and return true; else return\n        false.\n        \"\"\"\n        import sys\n\n        if six.PY2 or self.help_commands:\n            return _Distribution.handle_display_options(self, option_order)\n\n        # Stdout may be StringIO (e.g. in tests)\n        if not isinstance(sys.stdout, io.TextIOWrapper):\n            return _Distribution.handle_display_options(self, option_order)\n\n        # Don't wrap stdout if utf-8 is already the encoding. Provides\n        #  workaround for #334.\n        if sys.stdout.encoding.lower() in ('utf-8', 'utf8'):\n            return _Distribution.handle_display_options(self, option_order)\n\n        # Print metadata in UTF-8 no matter the platform\n        encoding = sys.stdout.encoding\n        errors = sys.stdout.errors\n        newline = sys.platform != 'win32' and '\\n' or None\n        line_buffering = sys.stdout.line_buffering\n\n        sys.stdout = io.TextIOWrapper(\n            sys.stdout.detach(), 'utf-8', errors, newline, line_buffering)\n        try:\n            return _Distribution.handle_display_options(self, option_order)\n        finally:\n            sys.stdout = io.TextIOWrapper(\n                sys.stdout.detach(), encoding, errors, newline, line_buffering)\n\n\nclass DistDeprecationWarning(SetuptoolsDeprecationWarning):\n    \"\"\"Class for warning about deprecations in dist in\n    setuptools. Not ignored by default, unlike DeprecationWarning.\"\"\"\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/dist.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/dist.py	(date 1602088701597)
@@ -23,10 +23,8 @@
 from distutils.util import rfc822_escape
 from distutils.version import StrictVersion
 
-from setuptools.extern import six
 from setuptools.extern import packaging
 from setuptools.extern import ordered_set
-from setuptools.extern.six.moves import map, filter, filterfalse
 
 from . import SetuptoolsDeprecationWarning
 
@@ -126,12 +124,8 @@
     """
     version = self.get_metadata_version()
 
-    if six.PY2:
-        def write_field(key, value):
-            file.write("%s: %s\n" % (key, self._encode_field(value)))
-    else:
-        def write_field(key, value):
-            file.write("%s: %s\n" % (key, value))
+    def write_field(key, value):
+        file.write("%s: %s\n" % (key, value))
 
     write_field('Metadata-Version', str(version))
     write_field('Name', self.get_name())
@@ -308,7 +302,7 @@
 
 
 def check_test_suite(dist, attr, value):
-    if not isinstance(value, six.string_types):
+    if not isinstance(value, str):
         raise DistutilsSetupError("test_suite must be a string")
 
 
@@ -319,7 +313,7 @@
             "{!r} must be a dictionary mapping package names to lists of "
             "string wildcard patterns".format(attr))
     for k, v in value.items():
-        if not isinstance(k, six.string_types):
+        if not isinstance(k, str):
             raise DistutilsSetupError(
                 "keys of {!r} dict must be strings (got {!r})"
                 .format(attr, k)
@@ -537,7 +531,7 @@
         spec_inst_reqs = getattr(self, 'install_requires', None) or ()
         inst_reqs = list(pkg_resources.parse_requirements(spec_inst_reqs))
         simple_reqs = filter(is_simple_req, inst_reqs)
-        complex_reqs = filterfalse(is_simple_req, inst_reqs)
+        complex_reqs = itertools.filterfalse(is_simple_req, inst_reqs)
         self.install_requires = list(map(str, simple_reqs))
 
         for r in complex_reqs:
@@ -560,10 +554,10 @@
         this method provides the same functionality in subtly-improved
         ways.
         """
-        from setuptools.extern.six.moves.configparser import ConfigParser
+        from configparser import ConfigParser
 
         # Ignore install directory options if we have a venv
-        if not six.PY2 and sys.prefix != sys.base_prefix:
+        if sys.prefix != sys.base_prefix:
             ignore_options = [
                 'install-base', 'install-platbase', 'install-lib',
                 'install-platlib', 'install-purelib', 'install-headers',
@@ -585,14 +579,14 @@
             with io.open(filename, encoding='utf-8') as reader:
                 if DEBUG:
                     self.announce("  reading {filename}".format(**locals()))
-                (parser.readfp if six.PY2 else parser.read_file)(reader)
+                parser.read_file(reader)
             for section in parser.sections():
                 options = parser.options(section)
                 opt_dict = self.get_option_dict(section)
 
                 for opt in options:
                     if opt != '__name__' and opt not in ignore_options:
-                        val = self._try_str(parser.get(section, opt))
+                        val = parser.get(section, opt)
                         opt = opt.replace('-', '_')
                         opt_dict[opt] = (filename, val)
 
@@ -616,26 +610,6 @@
                 except ValueError as e:
                     raise DistutilsOptionError(e) from e
 
-    @staticmethod
-    def _try_str(val):
-        """
-        On Python 2, much of distutils relies on string values being of
-        type 'str' (bytes) and not unicode text. If the value can be safely
-        encoded to bytes using the default encoding, prefer that.
-
-        Why the default encoding? Because that value can be implicitly
-        decoded back to text if needed.
-
-        Ref #1653
-        """
-        if not six.PY2:
-            return val
-        try:
-            return val.encode()
-        except UnicodeEncodeError:
-            pass
-        return val
-
     def _set_command_options(self, command_obj, option_dict=None):
         """
         Set the options for 'command_obj' from 'option_dict'.  Basically
@@ -669,7 +643,7 @@
                 neg_opt = {}
 
             try:
-                is_string = isinstance(value, six.string_types)
+                is_string = isinstance(value, str)
                 if option in neg_opt and is_string:
                     setattr(command_obj, neg_opt[option], not strtobool(value))
                 elif option in bool_opts and is_string:
@@ -1003,7 +977,7 @@
         """
         import sys
 
-        if six.PY2 or self.help_commands:
+        if self.help_commands:
             return _Distribution.handle_display_options(self, option_order)
 
         # Stdout may be StringIO (e.g. in tests)
Index: env/lib/python3.8/site-packages/setuptools/build_meta.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"A PEP 517 interface to setuptools\n\nPreviously, when a user or a command line tool (let's call it a \"frontend\")\nneeded to make a request of setuptools to take a certain action, for\nexample, generating a list of installation requirements, the frontend would\nwould call \"setup.py egg_info\" or \"setup.py bdist_wheel\" on the command line.\n\nPEP 517 defines a different method of interfacing with setuptools. Rather\nthan calling \"setup.py\" directly, the frontend should:\n\n  1. Set the current directory to the directory with a setup.py file\n  2. Import this module into a safe python interpreter (one in which\n     setuptools can potentially set global variables or crash hard).\n  3. Call one of the functions defined in PEP 517.\n\nWhat each function does is defined in PEP 517. However, here is a \"casual\"\ndefinition of the functions (this definition should not be relied on for\nbug reports or API stability):\n\n  - `build_wheel`: build a wheel in the folder and return the basename\n  - `get_requires_for_build_wheel`: get the `setup_requires` to build\n  - `prepare_metadata_for_build_wheel`: get the `install_requires`\n  - `build_sdist`: build an sdist in the folder and return the basename\n  - `get_requires_for_build_sdist`: get the `setup_requires` to build\n\nAgain, this is not a formal definition! Just a \"taste\" of the module.\n\"\"\"\n\nimport io\nimport os\nimport sys\nimport tokenize\nimport shutil\nimport contextlib\n\nimport setuptools\nimport distutils\nfrom setuptools.py31compat import TemporaryDirectory\n\nfrom pkg_resources import parse_requirements\n\n__all__ = ['get_requires_for_build_sdist',\n           'get_requires_for_build_wheel',\n           'prepare_metadata_for_build_wheel',\n           'build_wheel',\n           'build_sdist',\n           '__legacy__',\n           'SetupRequirementsError']\n\n\nclass SetupRequirementsError(BaseException):\n    def __init__(self, specifiers):\n        self.specifiers = specifiers\n\n\nclass Distribution(setuptools.dist.Distribution):\n    def fetch_build_eggs(self, specifiers):\n        specifier_list = list(map(str, parse_requirements(specifiers)))\n\n        raise SetupRequirementsError(specifier_list)\n\n    @classmethod\n    @contextlib.contextmanager\n    def patch(cls):\n        \"\"\"\n        Replace\n        distutils.dist.Distribution with this class\n        for the duration of this context.\n        \"\"\"\n        orig = distutils.core.Distribution\n        distutils.core.Distribution = cls\n        try:\n            yield\n        finally:\n            distutils.core.Distribution = orig\n\n\ndef _to_str(s):\n    \"\"\"\n    Convert a filename to a string (on Python 2, explicitly\n    a byte string, not Unicode) as distutils checks for the\n    exact type str.\n    \"\"\"\n    if sys.version_info[0] == 2 and not isinstance(s, str):\n        # Assume it's Unicode, as that's what the PEP says\n        # should be provided.\n        return s.encode(sys.getfilesystemencoding())\n    return s\n\n\ndef _get_immediate_subdirectories(a_dir):\n    return [name for name in os.listdir(a_dir)\n            if os.path.isdir(os.path.join(a_dir, name))]\n\n\ndef _file_with_extension(directory, extension):\n    matching = (\n        f for f in os.listdir(directory)\n        if f.endswith(extension)\n    )\n    file, = matching\n    return file\n\n\ndef _open_setup_script(setup_script):\n    if not os.path.exists(setup_script):\n        # Supply a default setup.py\n        return io.StringIO(u\"from setuptools import setup; setup()\")\n\n    return getattr(tokenize, 'open', open)(setup_script)\n\n\nclass _BuildMetaBackend(object):\n\n    def _fix_config(self, config_settings):\n        config_settings = config_settings or {}\n        config_settings.setdefault('--global-option', [])\n        return config_settings\n\n    def _get_build_requires(self, config_settings, requirements):\n        config_settings = self._fix_config(config_settings)\n\n        sys.argv = sys.argv[:1] + ['egg_info'] + \\\n            config_settings[\"--global-option\"]\n        try:\n            with Distribution.patch():\n                self.run_setup()\n        except SetupRequirementsError as e:\n            requirements += e.specifiers\n\n        return requirements\n\n    def run_setup(self, setup_script='setup.py'):\n        # Note that we can reuse our build directory between calls\n        # Correctness comes first, then optimization later\n        __file__ = setup_script\n        __name__ = '__main__'\n\n        with _open_setup_script(__file__) as f:\n            code = f.read().replace(r'\\r\\n', r'\\n')\n\n        exec(compile(code, __file__, 'exec'), locals())\n\n    def get_requires_for_build_wheel(self, config_settings=None):\n        config_settings = self._fix_config(config_settings)\n        return self._get_build_requires(\n            config_settings, requirements=['wheel'])\n\n    def get_requires_for_build_sdist(self, config_settings=None):\n        config_settings = self._fix_config(config_settings)\n        return self._get_build_requires(config_settings, requirements=[])\n\n    def prepare_metadata_for_build_wheel(self, metadata_directory,\n                                         config_settings=None):\n        sys.argv = sys.argv[:1] + ['dist_info', '--egg-base',\n                                   _to_str(metadata_directory)]\n        self.run_setup()\n\n        dist_info_directory = metadata_directory\n        while True:\n            dist_infos = [f for f in os.listdir(dist_info_directory)\n                          if f.endswith('.dist-info')]\n\n            if (\n                len(dist_infos) == 0 and\n                len(_get_immediate_subdirectories(dist_info_directory)) == 1\n            ):\n\n                dist_info_directory = os.path.join(\n                    dist_info_directory, os.listdir(dist_info_directory)[0])\n                continue\n\n            assert len(dist_infos) == 1\n            break\n\n        # PEP 517 requires that the .dist-info directory be placed in the\n        # metadata_directory. To comply, we MUST copy the directory to the root\n        if dist_info_directory != metadata_directory:\n            shutil.move(\n                os.path.join(dist_info_directory, dist_infos[0]),\n                metadata_directory)\n            shutil.rmtree(dist_info_directory, ignore_errors=True)\n\n        return dist_infos[0]\n\n    def _build_with_temp_dir(self, setup_command, result_extension,\n                             result_directory, config_settings):\n        config_settings = self._fix_config(config_settings)\n        result_directory = os.path.abspath(result_directory)\n\n        # Build in a temporary directory, then copy to the target.\n        os.makedirs(result_directory, exist_ok=True)\n        with TemporaryDirectory(dir=result_directory) as tmp_dist_dir:\n            sys.argv = (sys.argv[:1] + setup_command +\n                        ['--dist-dir', tmp_dist_dir] +\n                        config_settings[\"--global-option\"])\n            self.run_setup()\n\n            result_basename = _file_with_extension(\n                tmp_dist_dir, result_extension)\n            result_path = os.path.join(result_directory, result_basename)\n            if os.path.exists(result_path):\n                # os.rename will fail overwriting on non-Unix.\n                os.remove(result_path)\n            os.rename(os.path.join(tmp_dist_dir, result_basename), result_path)\n\n        return result_basename\n\n    def build_wheel(self, wheel_directory, config_settings=None,\n                    metadata_directory=None):\n        return self._build_with_temp_dir(['bdist_wheel'], '.whl',\n                                         wheel_directory, config_settings)\n\n    def build_sdist(self, sdist_directory, config_settings=None):\n        return self._build_with_temp_dir(['sdist', '--formats', 'gztar'],\n                                         '.tar.gz', sdist_directory,\n                                         config_settings)\n\n\nclass _BuildMetaLegacyBackend(_BuildMetaBackend):\n    \"\"\"Compatibility backend for setuptools\n\n    This is a version of setuptools.build_meta that endeavors\n    to maintain backwards\n    compatibility with pre-PEP 517 modes of invocation. It\n    exists as a temporary\n    bridge between the old packaging mechanism and the new\n    packaging mechanism,\n    and will eventually be removed.\n    \"\"\"\n    def run_setup(self, setup_script='setup.py'):\n        # In order to maintain compatibility with scripts assuming that\n        # the setup.py script is in a directory on the PYTHONPATH, inject\n        # '' into sys.path. (pypa/setuptools#1642)\n        sys_path = list(sys.path)           # Save the original path\n\n        script_dir = os.path.dirname(os.path.abspath(setup_script))\n        if script_dir not in sys.path:\n            sys.path.insert(0, script_dir)\n\n        # Some setup.py scripts (e.g. in pygame and numpy) use sys.argv[0] to\n        # get the directory of the source code. They expect it to refer to the\n        # setup.py script.\n        sys_argv_0 = sys.argv[0]\n        sys.argv[0] = setup_script\n\n        try:\n            super(_BuildMetaLegacyBackend,\n                  self).run_setup(setup_script=setup_script)\n        finally:\n            # While PEP 517 frontends should be calling each hook in a fresh\n            # subprocess according to the standard (and thus it should not be\n            # strictly necessary to restore the old sys.path), we'll restore\n            # the original path so that the path manipulation does not persist\n            # within the hook after run_setup is called.\n            sys.path[:] = sys_path\n            sys.argv[0] = sys_argv_0\n\n\n# The primary backend\n_BACKEND = _BuildMetaBackend()\n\nget_requires_for_build_wheel = _BACKEND.get_requires_for_build_wheel\nget_requires_for_build_sdist = _BACKEND.get_requires_for_build_sdist\nprepare_metadata_for_build_wheel = _BACKEND.prepare_metadata_for_build_wheel\nbuild_wheel = _BACKEND.build_wheel\nbuild_sdist = _BACKEND.build_sdist\n\n\n# The legacy backend\n__legacy__ = _BuildMetaLegacyBackend()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/build_meta.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/build_meta.py	(date 1602088701593)
@@ -32,10 +32,10 @@
 import tokenize
 import shutil
 import contextlib
+import tempfile
 
 import setuptools
 import distutils
-from setuptools.py31compat import TemporaryDirectory
 
 from pkg_resources import parse_requirements
 
@@ -75,17 +75,20 @@
             distutils.core.Distribution = orig
 
 
-def _to_str(s):
+@contextlib.contextmanager
+def no_install_setup_requires():
+    """Temporarily disable installing setup_requires
+
+    Under PEP 517, the backend reports build dependencies to the frontend,
+    and the frontend is responsible for ensuring they're installed.
+    So setuptools (acting as a backend) should not try to install them.
     """
-    Convert a filename to a string (on Python 2, explicitly
-    a byte string, not Unicode) as distutils checks for the
-    exact type str.
-    """
-    if sys.version_info[0] == 2 and not isinstance(s, str):
-        # Assume it's Unicode, as that's what the PEP says
-        # should be provided.
-        return s.encode(sys.getfilesystemencoding())
-    return s
+    orig = setuptools._install_setup_requires
+    setuptools._install_setup_requires = lambda attrs: None
+    try:
+        yield
+    finally:
+        setuptools._install_setup_requires = orig
 
 
 def _get_immediate_subdirectories(a_dir):
@@ -152,9 +155,10 @@
 
     def prepare_metadata_for_build_wheel(self, metadata_directory,
                                          config_settings=None):
-        sys.argv = sys.argv[:1] + ['dist_info', '--egg-base',
-                                   _to_str(metadata_directory)]
-        self.run_setup()
+        sys.argv = sys.argv[:1] + [
+            'dist_info', '--egg-base', metadata_directory]
+        with no_install_setup_requires():
+            self.run_setup()
 
         dist_info_directory = metadata_directory
         while True:
@@ -190,11 +194,12 @@
 
         # Build in a temporary directory, then copy to the target.
         os.makedirs(result_directory, exist_ok=True)
-        with TemporaryDirectory(dir=result_directory) as tmp_dist_dir:
+        with tempfile.TemporaryDirectory(dir=result_directory) as tmp_dist_dir:
             sys.argv = (sys.argv[:1] + setup_command +
                         ['--dist-dir', tmp_dist_dir] +
                         config_settings["--global-option"])
-            self.run_setup()
+            with no_install_setup_requires():
+                self.run_setup()
 
             result_basename = _file_with_extension(
                 tmp_dist_dir, result_extension)
Index: env/lib/python3.8/site-packages/setuptools/config.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import absolute_import, unicode_literals\nimport ast\nimport io\nimport os\nimport sys\n\nimport warnings\nimport functools\nimport importlib\nfrom collections import defaultdict\nfrom functools import partial\nfrom functools import wraps\nimport contextlib\n\nfrom distutils.errors import DistutilsOptionError, DistutilsFileError\nfrom setuptools.extern.packaging.version import LegacyVersion, parse\nfrom setuptools.extern.packaging.specifiers import SpecifierSet\nfrom setuptools.extern.six import string_types, PY3\n\n\n__metaclass__ = type\n\n\nclass StaticModule:\n    \"\"\"\n    Attempt to load the module by the name\n    \"\"\"\n    def __init__(self, name):\n        spec = importlib.util.find_spec(name)\n        with open(spec.origin) as strm:\n            src = strm.read()\n        module = ast.parse(src)\n        vars(self).update(locals())\n        del self.self\n\n    def __getattr__(self, attr):\n        try:\n            return next(\n                ast.literal_eval(statement.value)\n                for statement in self.module.body\n                if isinstance(statement, ast.Assign)\n                for target in statement.targets\n                if isinstance(target, ast.Name) and target.id == attr\n            )\n        except Exception as e:\n            raise AttributeError(\n                \"{self.name} has no attribute {attr}\".format(**locals())\n            ) from e\n\n\n@contextlib.contextmanager\ndef patch_path(path):\n    \"\"\"\n    Add path to front of sys.path for the duration of the context.\n    \"\"\"\n    try:\n        sys.path.insert(0, path)\n        yield\n    finally:\n        sys.path.remove(path)\n\n\ndef read_configuration(\n        filepath, find_others=False, ignore_option_errors=False):\n    \"\"\"Read given configuration file and returns options from it as a dict.\n\n    :param str|unicode filepath: Path to configuration file\n        to get options from.\n\n    :param bool find_others: Whether to search for other configuration files\n        which could be on in various places.\n\n    :param bool ignore_option_errors: Whether to silently ignore\n        options, values of which could not be resolved (e.g. due to exceptions\n        in directives such as file:, attr:, etc.).\n        If False exceptions are propagated as expected.\n\n    :rtype: dict\n    \"\"\"\n    from setuptools.dist import Distribution, _Distribution\n\n    filepath = os.path.abspath(filepath)\n\n    if not os.path.isfile(filepath):\n        raise DistutilsFileError(\n            'Configuration file %s does not exist.' % filepath)\n\n    current_directory = os.getcwd()\n    os.chdir(os.path.dirname(filepath))\n\n    try:\n        dist = Distribution()\n\n        filenames = dist.find_config_files() if find_others else []\n        if filepath not in filenames:\n            filenames.append(filepath)\n\n        _Distribution.parse_config_files(dist, filenames=filenames)\n\n        handlers = parse_configuration(\n            dist, dist.command_options,\n            ignore_option_errors=ignore_option_errors)\n\n    finally:\n        os.chdir(current_directory)\n\n    return configuration_to_dict(handlers)\n\n\ndef _get_option(target_obj, key):\n    \"\"\"\n    Given a target object and option key, get that option from\n    the target object, either through a get_{key} method or\n    from an attribute directly.\n    \"\"\"\n    getter_name = 'get_{key}'.format(**locals())\n    by_attribute = functools.partial(getattr, target_obj, key)\n    getter = getattr(target_obj, getter_name, by_attribute)\n    return getter()\n\n\ndef configuration_to_dict(handlers):\n    \"\"\"Returns configuration data gathered by given handlers as a dict.\n\n    :param list[ConfigHandler] handlers: Handlers list,\n        usually from parse_configuration()\n\n    :rtype: dict\n    \"\"\"\n    config_dict = defaultdict(dict)\n\n    for handler in handlers:\n        for option in handler.set_options:\n            value = _get_option(handler.target_obj, option)\n            config_dict[handler.section_prefix][option] = value\n\n    return config_dict\n\n\ndef parse_configuration(\n        distribution, command_options, ignore_option_errors=False):\n    \"\"\"Performs additional parsing of configuration options\n    for a distribution.\n\n    Returns a list of used option handlers.\n\n    :param Distribution distribution:\n    :param dict command_options:\n    :param bool ignore_option_errors: Whether to silently ignore\n        options, values of which could not be resolved (e.g. due to exceptions\n        in directives such as file:, attr:, etc.).\n        If False exceptions are propagated as expected.\n    :rtype: list\n    \"\"\"\n    options = ConfigOptionsHandler(\n        distribution, command_options, ignore_option_errors)\n    options.parse()\n\n    meta = ConfigMetadataHandler(\n        distribution.metadata, command_options, ignore_option_errors,\n        distribution.package_dir)\n    meta.parse()\n\n    return meta, options\n\n\nclass ConfigHandler:\n    \"\"\"Handles metadata supplied in configuration files.\"\"\"\n\n    section_prefix = None\n    \"\"\"Prefix for config sections handled by this handler.\n    Must be provided by class heirs.\n\n    \"\"\"\n\n    aliases = {}\n    \"\"\"Options aliases.\n    For compatibility with various packages. E.g.: d2to1 and pbr.\n    Note: `-` in keys is replaced with `_` by config parser.\n\n    \"\"\"\n\n    def __init__(self, target_obj, options, ignore_option_errors=False):\n        sections = {}\n\n        section_prefix = self.section_prefix\n        for section_name, section_options in options.items():\n            if not section_name.startswith(section_prefix):\n                continue\n\n            section_name = section_name.replace(section_prefix, '').strip('.')\n            sections[section_name] = section_options\n\n        self.ignore_option_errors = ignore_option_errors\n        self.target_obj = target_obj\n        self.sections = sections\n        self.set_options = []\n\n    @property\n    def parsers(self):\n        \"\"\"Metadata item name to parser function mapping.\"\"\"\n        raise NotImplementedError(\n            '%s must provide .parsers property' % self.__class__.__name__)\n\n    def __setitem__(self, option_name, value):\n        unknown = tuple()\n        target_obj = self.target_obj\n\n        # Translate alias into real name.\n        option_name = self.aliases.get(option_name, option_name)\n\n        current_value = getattr(target_obj, option_name, unknown)\n\n        if current_value is unknown:\n            raise KeyError(option_name)\n\n        if current_value:\n            # Already inhabited. Skipping.\n            return\n\n        skip_option = False\n        parser = self.parsers.get(option_name)\n        if parser:\n            try:\n                value = parser(value)\n\n            except Exception:\n                skip_option = True\n                if not self.ignore_option_errors:\n                    raise\n\n        if skip_option:\n            return\n\n        setter = getattr(target_obj, 'set_%s' % option_name, None)\n        if setter is None:\n            setattr(target_obj, option_name, value)\n        else:\n            setter(value)\n\n        self.set_options.append(option_name)\n\n    @classmethod\n    def _parse_list(cls, value, separator=','):\n        \"\"\"Represents value as a list.\n\n        Value is split either by separator (defaults to comma) or by lines.\n\n        :param value:\n        :param separator: List items separator character.\n        :rtype: list\n        \"\"\"\n        if isinstance(value, list):  # _get_parser_compound case\n            return value\n\n        if '\\n' in value:\n            value = value.splitlines()\n        else:\n            value = value.split(separator)\n\n        return [chunk.strip() for chunk in value if chunk.strip()]\n\n    @classmethod\n    def _parse_dict(cls, value):\n        \"\"\"Represents value as a dict.\n\n        :param value:\n        :rtype: dict\n        \"\"\"\n        separator = '='\n        result = {}\n        for line in cls._parse_list(value):\n            key, sep, val = line.partition(separator)\n            if sep != separator:\n                raise DistutilsOptionError(\n                    'Unable to parse option value to dict: %s' % value)\n            result[key.strip()] = val.strip()\n\n        return result\n\n    @classmethod\n    def _parse_bool(cls, value):\n        \"\"\"Represents value as boolean.\n\n        :param value:\n        :rtype: bool\n        \"\"\"\n        value = value.lower()\n        return value in ('1', 'true', 'yes')\n\n    @classmethod\n    def _exclude_files_parser(cls, key):\n        \"\"\"Returns a parser function to make sure field inputs\n        are not files.\n\n        Parses a value after getting the key so error messages are\n        more informative.\n\n        :param key:\n        :rtype: callable\n        \"\"\"\n        def parser(value):\n            exclude_directive = 'file:'\n            if value.startswith(exclude_directive):\n                raise ValueError(\n                    'Only strings are accepted for the {0} field, '\n                    'files are not accepted'.format(key))\n            return value\n        return parser\n\n    @classmethod\n    def _parse_file(cls, value):\n        \"\"\"Represents value as a string, allowing including text\n        from nearest files using `file:` directive.\n\n        Directive is sandboxed and won't reach anything outside\n        directory with setup.py.\n\n        Examples:\n            file: README.rst, CHANGELOG.md, src/file.txt\n\n        :param str value:\n        :rtype: str\n        \"\"\"\n        include_directive = 'file:'\n\n        if not isinstance(value, string_types):\n            return value\n\n        if not value.startswith(include_directive):\n            return value\n\n        spec = value[len(include_directive):]\n        filepaths = (os.path.abspath(path.strip()) for path in spec.split(','))\n        return '\\n'.join(\n            cls._read_file(path)\n            for path in filepaths\n            if (cls._assert_local(path) or True)\n            and os.path.isfile(path)\n        )\n\n    @staticmethod\n    def _assert_local(filepath):\n        if not filepath.startswith(os.getcwd()):\n            raise DistutilsOptionError(\n                '`file:` directive can not access %s' % filepath)\n\n    @staticmethod\n    def _read_file(filepath):\n        with io.open(filepath, encoding='utf-8') as f:\n            return f.read()\n\n    @classmethod\n    def _parse_attr(cls, value, package_dir=None):\n        \"\"\"Represents value as a module attribute.\n\n        Examples:\n            attr: package.attr\n            attr: package.module.attr\n\n        :param str value:\n        :rtype: str\n        \"\"\"\n        attr_directive = 'attr:'\n        if not value.startswith(attr_directive):\n            return value\n\n        attrs_path = value.replace(attr_directive, '').strip().split('.')\n        attr_name = attrs_path.pop()\n\n        module_name = '.'.join(attrs_path)\n        module_name = module_name or '__init__'\n\n        parent_path = os.getcwd()\n        if package_dir:\n            if attrs_path[0] in package_dir:\n                # A custom path was specified for the module we want to import\n                custom_path = package_dir[attrs_path[0]]\n                parts = custom_path.rsplit('/', 1)\n                if len(parts) > 1:\n                    parent_path = os.path.join(os.getcwd(), parts[0])\n                    module_name = parts[1]\n                else:\n                    module_name = custom_path\n            elif '' in package_dir:\n                # A custom parent directory was specified for all root modules\n                parent_path = os.path.join(os.getcwd(), package_dir[''])\n\n        with patch_path(parent_path):\n            try:\n                # attempt to load value statically\n                return getattr(StaticModule(module_name), attr_name)\n            except Exception:\n                # fallback to simple import\n                module = importlib.import_module(module_name)\n\n        return getattr(module, attr_name)\n\n    @classmethod\n    def _get_parser_compound(cls, *parse_methods):\n        \"\"\"Returns parser function to represents value as a list.\n\n        Parses a value applying given methods one after another.\n\n        :param parse_methods:\n        :rtype: callable\n        \"\"\"\n        def parse(value):\n            parsed = value\n\n            for method in parse_methods:\n                parsed = method(parsed)\n\n            return parsed\n\n        return parse\n\n    @classmethod\n    def _parse_section_to_dict(cls, section_options, values_parser=None):\n        \"\"\"Parses section options into a dictionary.\n\n        Optionally applies a given parser to values.\n\n        :param dict section_options:\n        :param callable values_parser:\n        :rtype: dict\n        \"\"\"\n        value = {}\n        values_parser = values_parser or (lambda val: val)\n        for key, (_, val) in section_options.items():\n            value[key] = values_parser(val)\n        return value\n\n    def parse_section(self, section_options):\n        \"\"\"Parses configuration file section.\n\n        :param dict section_options:\n        \"\"\"\n        for (name, (_, value)) in section_options.items():\n            try:\n                self[name] = value\n\n            except KeyError:\n                pass  # Keep silent for a new option may appear anytime.\n\n    def parse(self):\n        \"\"\"Parses configuration file items from one\n        or more related sections.\n\n        \"\"\"\n        for section_name, section_options in self.sections.items():\n\n            method_postfix = ''\n            if section_name:  # [section.option] variant\n                method_postfix = '_%s' % section_name\n\n            section_parser_method = getattr(\n                self,\n                # Dots in section names are translated into dunderscores.\n                ('parse_section%s' % method_postfix).replace('.', '__'),\n                None)\n\n            if section_parser_method is None:\n                raise DistutilsOptionError(\n                    'Unsupported distribution option section: [%s.%s]' % (\n                        self.section_prefix, section_name))\n\n            section_parser_method(section_options)\n\n    def _deprecated_config_handler(self, func, msg, warning_class):\n        \"\"\" this function will wrap around parameters that are deprecated\n\n        :param msg: deprecation message\n        :param warning_class: class of warning exception to be raised\n        :param func: function to be wrapped around\n        \"\"\"\n        @wraps(func)\n        def config_handler(*args, **kwargs):\n            warnings.warn(msg, warning_class)\n            return func(*args, **kwargs)\n\n        return config_handler\n\n\nclass ConfigMetadataHandler(ConfigHandler):\n\n    section_prefix = 'metadata'\n\n    aliases = {\n        'home_page': 'url',\n        'summary': 'description',\n        'classifier': 'classifiers',\n        'platform': 'platforms',\n    }\n\n    strict_mode = False\n    \"\"\"We need to keep it loose, to be partially compatible with\n    `pbr` and `d2to1` packages which also uses `metadata` section.\n\n    \"\"\"\n\n    def __init__(self, target_obj, options, ignore_option_errors=False,\n                 package_dir=None):\n        super(ConfigMetadataHandler, self).__init__(target_obj, options,\n                                                    ignore_option_errors)\n        self.package_dir = package_dir\n\n    @property\n    def parsers(self):\n        \"\"\"Metadata item name to parser function mapping.\"\"\"\n        parse_list = self._parse_list\n        parse_file = self._parse_file\n        parse_dict = self._parse_dict\n        exclude_files_parser = self._exclude_files_parser\n\n        return {\n            'platforms': parse_list,\n            'keywords': parse_list,\n            'provides': parse_list,\n            'requires': self._deprecated_config_handler(\n                parse_list,\n                \"The requires parameter is deprecated, please use \"\n                \"install_requires for runtime dependencies.\",\n                DeprecationWarning),\n            'obsoletes': parse_list,\n            'classifiers': self._get_parser_compound(parse_file, parse_list),\n            'license': exclude_files_parser('license'),\n            'license_files': parse_list,\n            'description': parse_file,\n            'long_description': parse_file,\n            'version': self._parse_version,\n            'project_urls': parse_dict,\n        }\n\n    def _parse_version(self, value):\n        \"\"\"Parses `version` option value.\n\n        :param value:\n        :rtype: str\n\n        \"\"\"\n        version = self._parse_file(value)\n\n        if version != value:\n            version = version.strip()\n            # Be strict about versions loaded from file because it's easy to\n            # accidentally include newlines and other unintended content\n            if isinstance(parse(version), LegacyVersion):\n                tmpl = (\n                    'Version loaded from {value} does not '\n                    'comply with PEP 440: {version}'\n                )\n                raise DistutilsOptionError(tmpl.format(**locals()))\n\n            return version\n\n        version = self._parse_attr(value, self.package_dir)\n\n        if callable(version):\n            version = version()\n\n        if not isinstance(version, string_types):\n            if hasattr(version, '__iter__'):\n                version = '.'.join(map(str, version))\n            else:\n                version = '%s' % version\n\n        return version\n\n\nclass ConfigOptionsHandler(ConfigHandler):\n\n    section_prefix = 'options'\n\n    @property\n    def parsers(self):\n        \"\"\"Metadata item name to parser function mapping.\"\"\"\n        parse_list = self._parse_list\n        parse_list_semicolon = partial(self._parse_list, separator=';')\n        parse_bool = self._parse_bool\n        parse_dict = self._parse_dict\n\n        return {\n            'zip_safe': parse_bool,\n            'use_2to3': parse_bool,\n            'include_package_data': parse_bool,\n            'package_dir': parse_dict,\n            'use_2to3_fixers': parse_list,\n            'use_2to3_exclude_fixers': parse_list,\n            'convert_2to3_doctests': parse_list,\n            'scripts': parse_list,\n            'eager_resources': parse_list,\n            'dependency_links': parse_list,\n            'namespace_packages': parse_list,\n            'install_requires': parse_list_semicolon,\n            'setup_requires': parse_list_semicolon,\n            'tests_require': parse_list_semicolon,\n            'packages': self._parse_packages,\n            'entry_points': self._parse_file,\n            'py_modules': parse_list,\n            'python_requires': SpecifierSet,\n        }\n\n    def _parse_packages(self, value):\n        \"\"\"Parses `packages` option value.\n\n        :param value:\n        :rtype: list\n        \"\"\"\n        find_directives = ['find:', 'find_namespace:']\n        trimmed_value = value.strip()\n\n        if trimmed_value not in find_directives:\n            return self._parse_list(value)\n\n        findns = trimmed_value == find_directives[1]\n        if findns and not PY3:\n            raise DistutilsOptionError(\n                'find_namespace: directive is unsupported on Python < 3.3')\n\n        # Read function arguments from a dedicated section.\n        find_kwargs = self.parse_section_packages__find(\n            self.sections.get('packages.find', {}))\n\n        if findns:\n            from setuptools import find_namespace_packages as find_packages\n        else:\n            from setuptools import find_packages\n\n        return find_packages(**find_kwargs)\n\n    def parse_section_packages__find(self, section_options):\n        \"\"\"Parses `packages.find` configuration file section.\n\n        To be used in conjunction with _parse_packages().\n\n        :param dict section_options:\n        \"\"\"\n        section_data = self._parse_section_to_dict(\n            section_options, self._parse_list)\n\n        valid_keys = ['where', 'include', 'exclude']\n\n        find_kwargs = dict(\n            [(k, v) for k, v in section_data.items() if k in valid_keys and v])\n\n        where = find_kwargs.get('where')\n        if where is not None:\n            find_kwargs['where'] = where[0]  # cast list to single val\n\n        return find_kwargs\n\n    def parse_section_entry_points(self, section_options):\n        \"\"\"Parses `entry_points` configuration file section.\n\n        :param dict section_options:\n        \"\"\"\n        parsed = self._parse_section_to_dict(section_options, self._parse_list)\n        self['entry_points'] = parsed\n\n    def _parse_package_data(self, section_options):\n        parsed = self._parse_section_to_dict(section_options, self._parse_list)\n\n        root = parsed.get('*')\n        if root:\n            parsed[''] = root\n            del parsed['*']\n\n        return parsed\n\n    def parse_section_package_data(self, section_options):\n        \"\"\"Parses `package_data` configuration file section.\n\n        :param dict section_options:\n        \"\"\"\n        self['package_data'] = self._parse_package_data(section_options)\n\n    def parse_section_exclude_package_data(self, section_options):\n        \"\"\"Parses `exclude_package_data` configuration file section.\n\n        :param dict section_options:\n        \"\"\"\n        self['exclude_package_data'] = self._parse_package_data(\n            section_options)\n\n    def parse_section_extras_require(self, section_options):\n        \"\"\"Parses `extras_require` configuration file section.\n\n        :param dict section_options:\n        \"\"\"\n        parse_list = partial(self._parse_list, separator=';')\n        self['extras_require'] = self._parse_section_to_dict(\n            section_options, parse_list)\n\n    def parse_section_data_files(self, section_options):\n        \"\"\"Parses `data_files` configuration file section.\n\n        :param dict section_options:\n        \"\"\"\n        parsed = self._parse_section_to_dict(section_options, self._parse_list)\n        self['data_files'] = [(k, v) for k, v in parsed.items()]\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/config.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/config.py	(date 1602088701597)
@@ -1,4 +1,3 @@
-from __future__ import absolute_import, unicode_literals
 import ast
 import io
 import os
@@ -15,10 +14,6 @@
 from distutils.errors import DistutilsOptionError, DistutilsFileError
 from setuptools.extern.packaging.version import LegacyVersion, parse
 from setuptools.extern.packaging.specifiers import SpecifierSet
-from setuptools.extern.six import string_types, PY3
-
-
-__metaclass__ = type
 
 
 class StaticModule:
@@ -324,7 +319,7 @@
         """
         include_directive = 'file:'
 
-        if not isinstance(value, string_types):
+        if not isinstance(value, str):
             return value
 
         if not value.startswith(include_directive):
@@ -559,7 +554,7 @@
         if callable(version):
             version = version()
 
-        if not isinstance(version, string_types):
+        if not isinstance(version, str):
             if hasattr(version, '__iter__'):
                 version = '.'.join(map(str, version))
             else:
@@ -614,9 +609,6 @@
             return self._parse_list(value)
 
         findns = trimmed_value == find_directives[1]
-        if findns and not PY3:
-            raise DistutilsOptionError(
-                'find_namespace: directive is unsupported on Python < 3.3')
 
         # Read function arguments from a dedicated section.
         find_kwargs = self.parse_section_packages__find(
Index: env/lib/python3.8/site-packages/setuptools/extension.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import re\nimport functools\nimport distutils.core\nimport distutils.errors\nimport distutils.extension\n\nfrom setuptools.extern.six.moves import map\n\nfrom .monkey import get_unpatched\n\n\ndef _have_cython():\n    \"\"\"\n    Return True if Cython can be imported.\n    \"\"\"\n    cython_impl = 'Cython.Distutils.build_ext'\n    try:\n        # from (cython_impl) import build_ext\n        __import__(cython_impl, fromlist=['build_ext']).build_ext\n        return True\n    except Exception:\n        pass\n    return False\n\n\n# for compatibility\nhave_pyrex = _have_cython\n\n_Extension = get_unpatched(distutils.core.Extension)\n\n\nclass Extension(_Extension):\n    \"\"\"Extension that uses '.c' files in place of '.pyx' files\"\"\"\n\n    def __init__(self, name, sources, *args, **kw):\n        # The *args is needed for compatibility as calls may use positional\n        # arguments. py_limited_api may be set only via keyword.\n        self.py_limited_api = kw.pop(\"py_limited_api\", False)\n        _Extension.__init__(self, name, sources, *args, **kw)\n\n    def _convert_pyx_sources_to_lang(self):\n        \"\"\"\n        Replace sources with .pyx extensions to sources with the target\n        language extension. This mechanism allows language authors to supply\n        pre-converted sources but to prefer the .pyx sources.\n        \"\"\"\n        if _have_cython():\n            # the build has Cython, so allow it to compile the .pyx files\n            return\n        lang = self.language or ''\n        target_ext = '.cpp' if lang.lower() == 'c++' else '.c'\n        sub = functools.partial(re.sub, '.pyx$', target_ext)\n        self.sources = list(map(sub, self.sources))\n\n\nclass Library(Extension):\n    \"\"\"Just like a regular Extension, but built as a library instead\"\"\"\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/extension.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/extension.py	(date 1602088701597)
@@ -4,8 +4,6 @@
 import distutils.errors
 import distutils.extension
 
-from setuptools.extern.six.moves import map
-
 from .monkey import get_unpatched
 
 
Index: env/lib/python3.8/site-packages/setuptools/msvc.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\nImproved support for Microsoft Visual C++ compilers.\n\nKnown supported compilers:\n--------------------------\nMicrosoft Visual C++ 9.0:\n    Microsoft Visual C++ Compiler for Python 2.7 (x86, amd64)\n    Microsoft Windows SDK 6.1 (x86, x64, ia64)\n    Microsoft Windows SDK 7.0 (x86, x64, ia64)\n\nMicrosoft Visual C++ 10.0:\n    Microsoft Windows SDK 7.1 (x86, x64, ia64)\n\nMicrosoft Visual C++ 14.X:\n    Microsoft Visual C++ Build Tools 2015 (x86, x64, arm)\n    Microsoft Visual Studio Build Tools 2017 (x86, x64, arm, arm64)\n    Microsoft Visual Studio Build Tools 2019 (x86, x64, arm, arm64)\n\nThis may also support compilers shipped with compatible Visual Studio versions.\n\"\"\"\n\nimport json\nfrom io import open\nfrom os import listdir, pathsep\nfrom os.path import join, isfile, isdir, dirname\nimport sys\nimport platform\nimport itertools\nimport subprocess\nimport distutils.errors\nfrom setuptools.extern.packaging.version import LegacyVersion\n\nfrom setuptools.extern.six.moves import filterfalse\n\nfrom .monkey import get_unpatched\n\nif platform.system() == 'Windows':\n    from setuptools.extern.six.moves import winreg\n    from os import environ\nelse:\n    # Mock winreg and environ so the module can be imported on this platform.\n\n    class winreg:\n        HKEY_USERS = None\n        HKEY_CURRENT_USER = None\n        HKEY_LOCAL_MACHINE = None\n        HKEY_CLASSES_ROOT = None\n\n    environ = dict()\n\n_msvc9_suppress_errors = (\n    # msvc9compiler isn't available on some platforms\n    ImportError,\n\n    # msvc9compiler raises DistutilsPlatformError in some\n    # environments. See #1118.\n    distutils.errors.DistutilsPlatformError,\n)\n\ntry:\n    from distutils.msvc9compiler import Reg\nexcept _msvc9_suppress_errors:\n    pass\n\n\ndef msvc9_find_vcvarsall(version):\n    \"\"\"\n    Patched \"distutils.msvc9compiler.find_vcvarsall\" to use the standalone\n    compiler build for Python\n    (VCForPython / Microsoft Visual C++ Compiler for Python 2.7).\n\n    Fall back to original behavior when the standalone compiler is not\n    available.\n\n    Redirect the path of \"vcvarsall.bat\".\n\n    Parameters\n    ----------\n    version: float\n        Required Microsoft Visual C++ version.\n\n    Return\n    ------\n    str\n        vcvarsall.bat path\n    \"\"\"\n    vc_base = r'Software\\%sMicrosoft\\DevDiv\\VCForPython\\%0.1f'\n    key = vc_base % ('', version)\n    try:\n        # Per-user installs register the compiler path here\n        productdir = Reg.get_value(key, \"installdir\")\n    except KeyError:\n        try:\n            # All-user installs on a 64-bit system register here\n            key = vc_base % ('Wow6432Node\\\\', version)\n            productdir = Reg.get_value(key, \"installdir\")\n        except KeyError:\n            productdir = None\n\n    if productdir:\n        vcvarsall = join(productdir, \"vcvarsall.bat\")\n        if isfile(vcvarsall):\n            return vcvarsall\n\n    return get_unpatched(msvc9_find_vcvarsall)(version)\n\n\ndef msvc9_query_vcvarsall(ver, arch='x86', *args, **kwargs):\n    \"\"\"\n    Patched \"distutils.msvc9compiler.query_vcvarsall\" for support extra\n    Microsoft Visual C++ 9.0 and 10.0 compilers.\n\n    Set environment without use of \"vcvarsall.bat\".\n\n    Parameters\n    ----------\n    ver: float\n        Required Microsoft Visual C++ version.\n    arch: str\n        Target architecture.\n\n    Return\n    ------\n    dict\n        environment\n    \"\"\"\n    # Try to get environment from vcvarsall.bat (Classical way)\n    try:\n        orig = get_unpatched(msvc9_query_vcvarsall)\n        return orig(ver, arch, *args, **kwargs)\n    except distutils.errors.DistutilsPlatformError:\n        # Pass error if Vcvarsall.bat is missing\n        pass\n    except ValueError:\n        # Pass error if environment not set after executing vcvarsall.bat\n        pass\n\n    # If error, try to set environment directly\n    try:\n        return EnvironmentInfo(arch, ver).return_env()\n    except distutils.errors.DistutilsPlatformError as exc:\n        _augment_exception(exc, ver, arch)\n        raise\n\n\ndef _msvc14_find_vc2015():\n    \"\"\"Python 3.8 \"distutils/_msvccompiler.py\" backport\"\"\"\n    try:\n        key = winreg.OpenKey(\n            winreg.HKEY_LOCAL_MACHINE,\n            r\"Software\\Microsoft\\VisualStudio\\SxS\\VC7\",\n            0,\n            winreg.KEY_READ | winreg.KEY_WOW64_32KEY\n        )\n    except OSError:\n        return None, None\n\n    best_version = 0\n    best_dir = None\n    with key:\n        for i in itertools.count():\n            try:\n                v, vc_dir, vt = winreg.EnumValue(key, i)\n            except OSError:\n                break\n            if v and vt == winreg.REG_SZ and isdir(vc_dir):\n                try:\n                    version = int(float(v))\n                except (ValueError, TypeError):\n                    continue\n                if version >= 14 and version > best_version:\n                    best_version, best_dir = version, vc_dir\n    return best_version, best_dir\n\n\ndef _msvc14_find_vc2017():\n    \"\"\"Python 3.8 \"distutils/_msvccompiler.py\" backport\n\n    Returns \"15, path\" based on the result of invoking vswhere.exe\n    If no install is found, returns \"None, None\"\n\n    The version is returned to avoid unnecessarily changing the function\n    result. It may be ignored when the path is not None.\n\n    If vswhere.exe is not available, by definition, VS 2017 is not\n    installed.\n    \"\"\"\n    root = environ.get(\"ProgramFiles(x86)\") or environ.get(\"ProgramFiles\")\n    if not root:\n        return None, None\n\n    try:\n        path = subprocess.check_output([\n            join(root, \"Microsoft Visual Studio\", \"Installer\", \"vswhere.exe\"),\n            \"-latest\",\n            \"-prerelease\",\n            \"-requires\", \"Microsoft.VisualStudio.Component.VC.Tools.x86.x64\",\n            \"-property\", \"installationPath\",\n            \"-products\", \"*\",\n        ]).decode(encoding=\"mbcs\", errors=\"strict\").strip()\n    except (subprocess.CalledProcessError, OSError, UnicodeDecodeError):\n        return None, None\n\n    path = join(path, \"VC\", \"Auxiliary\", \"Build\")\n    if isdir(path):\n        return 15, path\n\n    return None, None\n\n\nPLAT_SPEC_TO_RUNTIME = {\n    'x86': 'x86',\n    'x86_amd64': 'x64',\n    'x86_arm': 'arm',\n    'x86_arm64': 'arm64'\n}\n\n\ndef _msvc14_find_vcvarsall(plat_spec):\n    \"\"\"Python 3.8 \"distutils/_msvccompiler.py\" backport\"\"\"\n    _, best_dir = _msvc14_find_vc2017()\n    vcruntime = None\n\n    if plat_spec in PLAT_SPEC_TO_RUNTIME:\n        vcruntime_plat = PLAT_SPEC_TO_RUNTIME[plat_spec]\n    else:\n        vcruntime_plat = 'x64' if 'amd64' in plat_spec else 'x86'\n\n    if best_dir:\n        vcredist = join(best_dir, \"..\", \"..\", \"redist\", \"MSVC\", \"**\",\n                        vcruntime_plat, \"Microsoft.VC14*.CRT\",\n                        \"vcruntime140.dll\")\n        try:\n            import glob\n            vcruntime = glob.glob(vcredist, recursive=True)[-1]\n        except (ImportError, OSError, LookupError):\n            vcruntime = None\n\n    if not best_dir:\n        best_version, best_dir = _msvc14_find_vc2015()\n        if best_version:\n            vcruntime = join(best_dir, 'redist', vcruntime_plat,\n                             \"Microsoft.VC140.CRT\", \"vcruntime140.dll\")\n\n    if not best_dir:\n        return None, None\n\n    vcvarsall = join(best_dir, \"vcvarsall.bat\")\n    if not isfile(vcvarsall):\n        return None, None\n\n    if not vcruntime or not isfile(vcruntime):\n        vcruntime = None\n\n    return vcvarsall, vcruntime\n\n\ndef _msvc14_get_vc_env(plat_spec):\n    \"\"\"Python 3.8 \"distutils/_msvccompiler.py\" backport\"\"\"\n    if \"DISTUTILS_USE_SDK\" in environ:\n        return {\n            key.lower(): value\n            for key, value in environ.items()\n        }\n\n    vcvarsall, vcruntime = _msvc14_find_vcvarsall(plat_spec)\n    if not vcvarsall:\n        raise distutils.errors.DistutilsPlatformError(\n            \"Unable to find vcvarsall.bat\"\n        )\n\n    try:\n        out = subprocess.check_output(\n            'cmd /u /c \"{}\" {} && set'.format(vcvarsall, plat_spec),\n            stderr=subprocess.STDOUT,\n        ).decode('utf-16le', errors='replace')\n    except subprocess.CalledProcessError as exc:\n        raise distutils.errors.DistutilsPlatformError(\n            \"Error executing {}\".format(exc.cmd)\n        ) from exc\n\n    env = {\n        key.lower(): value\n        for key, _, value in\n        (line.partition('=') for line in out.splitlines())\n        if key and value\n    }\n\n    if vcruntime:\n        env['py_vcruntime_redist'] = vcruntime\n    return env\n\n\ndef msvc14_get_vc_env(plat_spec):\n    \"\"\"\n    Patched \"distutils._msvccompiler._get_vc_env\" for support extra\n    Microsoft Visual C++ 14.X compilers.\n\n    Set environment without use of \"vcvarsall.bat\".\n\n    Parameters\n    ----------\n    plat_spec: str\n        Target architecture.\n\n    Return\n    ------\n    dict\n        environment\n    \"\"\"\n\n    # Always use backport from CPython 3.8\n    try:\n        return _msvc14_get_vc_env(plat_spec)\n    except distutils.errors.DistutilsPlatformError as exc:\n        _augment_exception(exc, 14.0)\n        raise\n\n\ndef msvc14_gen_lib_options(*args, **kwargs):\n    \"\"\"\n    Patched \"distutils._msvccompiler.gen_lib_options\" for fix\n    compatibility between \"numpy.distutils\" and \"distutils._msvccompiler\"\n    (for Numpy < 1.11.2)\n    \"\"\"\n    if \"numpy.distutils\" in sys.modules:\n        import numpy as np\n        if LegacyVersion(np.__version__) < LegacyVersion('1.11.2'):\n            return np.distutils.ccompiler.gen_lib_options(*args, **kwargs)\n    return get_unpatched(msvc14_gen_lib_options)(*args, **kwargs)\n\n\ndef _augment_exception(exc, version, arch=''):\n    \"\"\"\n    Add details to the exception message to help guide the user\n    as to what action will resolve it.\n    \"\"\"\n    # Error if MSVC++ directory not found or environment not set\n    message = exc.args[0]\n\n    if \"vcvarsall\" in message.lower() or \"visual c\" in message.lower():\n        # Special error message if MSVC++ not installed\n        tmpl = 'Microsoft Visual C++ {version:0.1f} is required.'\n        message = tmpl.format(**locals())\n        msdownload = 'www.microsoft.com/download/details.aspx?id=%d'\n        if version == 9.0:\n            if arch.lower().find('ia64') > -1:\n                # For VC++ 9.0, if IA64 support is needed, redirect user\n                # to Windows SDK 7.0.\n                # Note: No download link available from Microsoft.\n                message += ' Get it with \"Microsoft Windows SDK 7.0\"'\n            else:\n                # For VC++ 9.0 redirect user to Vc++ for Python 2.7 :\n                # This redirection link is maintained by Microsoft.\n                # Contact vspython@microsoft.com if it needs updating.\n                message += ' Get it from http://aka.ms/vcpython27'\n        elif version == 10.0:\n            # For VC++ 10.0 Redirect user to Windows SDK 7.1\n            message += ' Get it with \"Microsoft Windows SDK 7.1\": '\n            message += msdownload % 8279\n        elif version >= 14.0:\n            # For VC++ 14.X Redirect user to latest Visual C++ Build Tools\n            message += (' Get it with \"Build Tools for Visual Studio\": '\n                        r'https://visualstudio.microsoft.com/downloads/')\n\n    exc.args = (message, )\n\n\nclass PlatformInfo:\n    \"\"\"\n    Current and Target Architectures information.\n\n    Parameters\n    ----------\n    arch: str\n        Target architecture.\n    \"\"\"\n    current_cpu = environ.get('processor_architecture', '').lower()\n\n    def __init__(self, arch):\n        self.arch = arch.lower().replace('x64', 'amd64')\n\n    @property\n    def target_cpu(self):\n        \"\"\"\n        Return Target CPU architecture.\n\n        Return\n        ------\n        str\n            Target CPU\n        \"\"\"\n        return self.arch[self.arch.find('_') + 1:]\n\n    def target_is_x86(self):\n        \"\"\"\n        Return True if target CPU is x86 32 bits..\n\n        Return\n        ------\n        bool\n            CPU is x86 32 bits\n        \"\"\"\n        return self.target_cpu == 'x86'\n\n    def current_is_x86(self):\n        \"\"\"\n        Return True if current CPU is x86 32 bits..\n\n        Return\n        ------\n        bool\n            CPU is x86 32 bits\n        \"\"\"\n        return self.current_cpu == 'x86'\n\n    def current_dir(self, hidex86=False, x64=False):\n        \"\"\"\n        Current platform specific subfolder.\n\n        Parameters\n        ----------\n        hidex86: bool\n            return '' and not '\\x86' if architecture is x86.\n        x64: bool\n            return '\\x64' and not '\\amd64' if architecture is amd64.\n\n        Return\n        ------\n        str\n            subfolder: '\\target', or '' (see hidex86 parameter)\n        \"\"\"\n        return (\n            '' if (self.current_cpu == 'x86' and hidex86) else\n            r'\\x64' if (self.current_cpu == 'amd64' and x64) else\n            r'\\%s' % self.current_cpu\n        )\n\n    def target_dir(self, hidex86=False, x64=False):\n        r\"\"\"\n        Target platform specific subfolder.\n\n        Parameters\n        ----------\n        hidex86: bool\n            return '' and not '\\x86' if architecture is x86.\n        x64: bool\n            return '\\x64' and not '\\amd64' if architecture is amd64.\n\n        Return\n        ------\n        str\n            subfolder: '\\current', or '' (see hidex86 parameter)\n        \"\"\"\n        return (\n            '' if (self.target_cpu == 'x86' and hidex86) else\n            r'\\x64' if (self.target_cpu == 'amd64' and x64) else\n            r'\\%s' % self.target_cpu\n        )\n\n    def cross_dir(self, forcex86=False):\n        r\"\"\"\n        Cross platform specific subfolder.\n\n        Parameters\n        ----------\n        forcex86: bool\n            Use 'x86' as current architecture even if current architecture is\n            not x86.\n\n        Return\n        ------\n        str\n            subfolder: '' if target architecture is current architecture,\n            '\\current_target' if not.\n        \"\"\"\n        current = 'x86' if forcex86 else self.current_cpu\n        return (\n            '' if self.target_cpu == current else\n            self.target_dir().replace('\\\\', '\\\\%s_' % current)\n        )\n\n\nclass RegistryInfo:\n    \"\"\"\n    Microsoft Visual Studio related registry information.\n\n    Parameters\n    ----------\n    platform_info: PlatformInfo\n        \"PlatformInfo\" instance.\n    \"\"\"\n    HKEYS = (winreg.HKEY_USERS,\n             winreg.HKEY_CURRENT_USER,\n             winreg.HKEY_LOCAL_MACHINE,\n             winreg.HKEY_CLASSES_ROOT)\n\n    def __init__(self, platform_info):\n        self.pi = platform_info\n\n    @property\n    def visualstudio(self):\n        \"\"\"\n        Microsoft Visual Studio root registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return 'VisualStudio'\n\n    @property\n    def sxs(self):\n        \"\"\"\n        Microsoft Visual Studio SxS registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return join(self.visualstudio, 'SxS')\n\n    @property\n    def vc(self):\n        \"\"\"\n        Microsoft Visual C++ VC7 registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return join(self.sxs, 'VC7')\n\n    @property\n    def vs(self):\n        \"\"\"\n        Microsoft Visual Studio VS7 registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return join(self.sxs, 'VS7')\n\n    @property\n    def vc_for_python(self):\n        \"\"\"\n        Microsoft Visual C++ for Python registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return r'DevDiv\\VCForPython'\n\n    @property\n    def microsoft_sdk(self):\n        \"\"\"\n        Microsoft SDK registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return 'Microsoft SDKs'\n\n    @property\n    def windows_sdk(self):\n        \"\"\"\n        Microsoft Windows/Platform SDK registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return join(self.microsoft_sdk, 'Windows')\n\n    @property\n    def netfx_sdk(self):\n        \"\"\"\n        Microsoft .NET Framework SDK registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return join(self.microsoft_sdk, 'NETFXSDK')\n\n    @property\n    def windows_kits_roots(self):\n        \"\"\"\n        Microsoft Windows Kits Roots registry key.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        return r'Windows Kits\\Installed Roots'\n\n    def microsoft(self, key, x86=False):\n        \"\"\"\n        Return key in Microsoft software registry.\n\n        Parameters\n        ----------\n        key: str\n            Registry key path where look.\n        x86: str\n            Force x86 software registry.\n\n        Return\n        ------\n        str\n            Registry key\n        \"\"\"\n        node64 = '' if self.pi.current_is_x86() or x86 else 'Wow6432Node'\n        return join('Software', node64, 'Microsoft', key)\n\n    def lookup(self, key, name):\n        \"\"\"\n        Look for values in registry in Microsoft software registry.\n\n        Parameters\n        ----------\n        key: str\n            Registry key path where look.\n        name: str\n            Value name to find.\n\n        Return\n        ------\n        str\n            value\n        \"\"\"\n        key_read = winreg.KEY_READ\n        openkey = winreg.OpenKey\n        closekey = winreg.CloseKey\n        ms = self.microsoft\n        for hkey in self.HKEYS:\n            bkey = None\n            try:\n                bkey = openkey(hkey, ms(key), 0, key_read)\n            except (OSError, IOError):\n                if not self.pi.current_is_x86():\n                    try:\n                        bkey = openkey(hkey, ms(key, True), 0, key_read)\n                    except (OSError, IOError):\n                        continue\n                else:\n                    continue\n            try:\n                return winreg.QueryValueEx(bkey, name)[0]\n            except (OSError, IOError):\n                pass\n            finally:\n                if bkey:\n                    closekey(bkey)\n\n\nclass SystemInfo:\n    \"\"\"\n    Microsoft Windows and Visual Studio related system information.\n\n    Parameters\n    ----------\n    registry_info: RegistryInfo\n        \"RegistryInfo\" instance.\n    vc_ver: float\n        Required Microsoft Visual C++ version.\n    \"\"\"\n\n    # Variables and properties in this class use originals CamelCase variables\n    # names from Microsoft source files for more easy comparison.\n    WinDir = environ.get('WinDir', '')\n    ProgramFiles = environ.get('ProgramFiles', '')\n    ProgramFilesx86 = environ.get('ProgramFiles(x86)', ProgramFiles)\n\n    def __init__(self, registry_info, vc_ver=None):\n        self.ri = registry_info\n        self.pi = self.ri.pi\n\n        self.known_vs_paths = self.find_programdata_vs_vers()\n\n        # Except for VS15+, VC version is aligned with VS version\n        self.vs_ver = self.vc_ver = (\n            vc_ver or self._find_latest_available_vs_ver())\n\n    def _find_latest_available_vs_ver(self):\n        \"\"\"\n        Find the latest VC version\n\n        Return\n        ------\n        float\n            version\n        \"\"\"\n        reg_vc_vers = self.find_reg_vs_vers()\n\n        if not (reg_vc_vers or self.known_vs_paths):\n            raise distutils.errors.DistutilsPlatformError(\n                'No Microsoft Visual C++ version found')\n\n        vc_vers = set(reg_vc_vers)\n        vc_vers.update(self.known_vs_paths)\n        return sorted(vc_vers)[-1]\n\n    def find_reg_vs_vers(self):\n        \"\"\"\n        Find Microsoft Visual Studio versions available in registry.\n\n        Return\n        ------\n        list of float\n            Versions\n        \"\"\"\n        ms = self.ri.microsoft\n        vckeys = (self.ri.vc, self.ri.vc_for_python, self.ri.vs)\n        vs_vers = []\n        for hkey in self.ri.HKEYS:\n            for key in vckeys:\n                try:\n                    bkey = winreg.OpenKey(hkey, ms(key), 0, winreg.KEY_READ)\n                except (OSError, IOError):\n                    continue\n                with bkey:\n                    subkeys, values, _ = winreg.QueryInfoKey(bkey)\n                    for i in range(values):\n                        try:\n                            ver = float(winreg.EnumValue(bkey, i)[0])\n                            if ver not in vs_vers:\n                                vs_vers.append(ver)\n                        except ValueError:\n                            pass\n                    for i in range(subkeys):\n                        try:\n                            ver = float(winreg.EnumKey(bkey, i))\n                            if ver not in vs_vers:\n                                vs_vers.append(ver)\n                        except ValueError:\n                            pass\n        return sorted(vs_vers)\n\n    def find_programdata_vs_vers(self):\n        r\"\"\"\n        Find Visual studio 2017+ versions from information in\n        \"C:\\ProgramData\\Microsoft\\VisualStudio\\Packages\\_Instances\".\n\n        Return\n        ------\n        dict\n            float version as key, path as value.\n        \"\"\"\n        vs_versions = {}\n        instances_dir = \\\n            r'C:\\ProgramData\\Microsoft\\VisualStudio\\Packages\\_Instances'\n\n        try:\n            hashed_names = listdir(instances_dir)\n\n        except (OSError, IOError):\n            # Directory not exists with all Visual Studio versions\n            return vs_versions\n\n        for name in hashed_names:\n            try:\n                # Get VS installation path from \"state.json\" file\n                state_path = join(instances_dir, name, 'state.json')\n                with open(state_path, 'rt', encoding='utf-8') as state_file:\n                    state = json.load(state_file)\n                vs_path = state['installationPath']\n\n                # Raises OSError if this VS installation does not contain VC\n                listdir(join(vs_path, r'VC\\Tools\\MSVC'))\n\n                # Store version and path\n                vs_versions[self._as_float_version(\n                    state['installationVersion'])] = vs_path\n\n            except (OSError, IOError, KeyError):\n                # Skip if \"state.json\" file is missing or bad format\n                continue\n\n        return vs_versions\n\n    @staticmethod\n    def _as_float_version(version):\n        \"\"\"\n        Return a string version as a simplified float version (major.minor)\n\n        Parameters\n        ----------\n        version: str\n            Version.\n\n        Return\n        ------\n        float\n            version\n        \"\"\"\n        return float('.'.join(version.split('.')[:2]))\n\n    @property\n    def VSInstallDir(self):\n        \"\"\"\n        Microsoft Visual Studio directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        # Default path\n        default = join(self.ProgramFilesx86,\n                       'Microsoft Visual Studio %0.1f' % self.vs_ver)\n\n        # Try to get path from registry, if fail use default path\n        return self.ri.lookup(self.ri.vs, '%0.1f' % self.vs_ver) or default\n\n    @property\n    def VCInstallDir(self):\n        \"\"\"\n        Microsoft Visual C++ directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        path = self._guess_vc() or self._guess_vc_legacy()\n\n        if not isdir(path):\n            msg = 'Microsoft Visual C++ directory not found'\n            raise distutils.errors.DistutilsPlatformError(msg)\n\n        return path\n\n    def _guess_vc(self):\n        \"\"\"\n        Locate Visual C++ for VS2017+.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        if self.vs_ver <= 14.0:\n            return ''\n\n        try:\n            # First search in known VS paths\n            vs_dir = self.known_vs_paths[self.vs_ver]\n        except KeyError:\n            # Else, search with path from registry\n            vs_dir = self.VSInstallDir\n\n        guess_vc = join(vs_dir, r'VC\\Tools\\MSVC')\n\n        # Subdir with VC exact version as name\n        try:\n            # Update the VC version with real one instead of VS version\n            vc_ver = listdir(guess_vc)[-1]\n            self.vc_ver = self._as_float_version(vc_ver)\n            return join(guess_vc, vc_ver)\n        except (OSError, IOError, IndexError):\n            return ''\n\n    def _guess_vc_legacy(self):\n        \"\"\"\n        Locate Visual C++ for versions prior to 2017.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        default = join(self.ProgramFilesx86,\n                       r'Microsoft Visual Studio %0.1f\\VC' % self.vs_ver)\n\n        # Try to get \"VC++ for Python\" path from registry as default path\n        reg_path = join(self.ri.vc_for_python, '%0.1f' % self.vs_ver)\n        python_vc = self.ri.lookup(reg_path, 'installdir')\n        default_vc = join(python_vc, 'VC') if python_vc else default\n\n        # Try to get path from registry, if fail use default path\n        return self.ri.lookup(self.ri.vc, '%0.1f' % self.vs_ver) or default_vc\n\n    @property\n    def WindowsSdkVersion(self):\n        \"\"\"\n        Microsoft Windows SDK versions for specified MSVC++ version.\n\n        Return\n        ------\n        tuple of str\n            versions\n        \"\"\"\n        if self.vs_ver <= 9.0:\n            return '7.0', '6.1', '6.0a'\n        elif self.vs_ver == 10.0:\n            return '7.1', '7.0a'\n        elif self.vs_ver == 11.0:\n            return '8.0', '8.0a'\n        elif self.vs_ver == 12.0:\n            return '8.1', '8.1a'\n        elif self.vs_ver >= 14.0:\n            return '10.0', '8.1'\n\n    @property\n    def WindowsSdkLastVersion(self):\n        \"\"\"\n        Microsoft Windows SDK last version.\n\n        Return\n        ------\n        str\n            version\n        \"\"\"\n        return self._use_last_dir_name(join(self.WindowsSdkDir, 'lib'))\n\n    @property\n    def WindowsSdkDir(self):\n        \"\"\"\n        Microsoft Windows SDK directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        sdkdir = ''\n        for ver in self.WindowsSdkVersion:\n            # Try to get it from registry\n            loc = join(self.ri.windows_sdk, 'v%s' % ver)\n            sdkdir = self.ri.lookup(loc, 'installationfolder')\n            if sdkdir:\n                break\n        if not sdkdir or not isdir(sdkdir):\n            # Try to get \"VC++ for Python\" version from registry\n            path = join(self.ri.vc_for_python, '%0.1f' % self.vc_ver)\n            install_base = self.ri.lookup(path, 'installdir')\n            if install_base:\n                sdkdir = join(install_base, 'WinSDK')\n        if not sdkdir or not isdir(sdkdir):\n            # If fail, use default new path\n            for ver in self.WindowsSdkVersion:\n                intver = ver[:ver.rfind('.')]\n                path = r'Microsoft SDKs\\Windows Kits\\%s' % intver\n                d = join(self.ProgramFiles, path)\n                if isdir(d):\n                    sdkdir = d\n        if not sdkdir or not isdir(sdkdir):\n            # If fail, use default old path\n            for ver in self.WindowsSdkVersion:\n                path = r'Microsoft SDKs\\Windows\\v%s' % ver\n                d = join(self.ProgramFiles, path)\n                if isdir(d):\n                    sdkdir = d\n        if not sdkdir:\n            # If fail, use Platform SDK\n            sdkdir = join(self.VCInstallDir, 'PlatformSDK')\n        return sdkdir\n\n    @property\n    def WindowsSDKExecutablePath(self):\n        \"\"\"\n        Microsoft Windows SDK executable directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        # Find WinSDK NetFx Tools registry dir name\n        if self.vs_ver <= 11.0:\n            netfxver = 35\n            arch = ''\n        else:\n            netfxver = 40\n            hidex86 = True if self.vs_ver <= 12.0 else False\n            arch = self.pi.current_dir(x64=True, hidex86=hidex86)\n        fx = 'WinSDK-NetFx%dTools%s' % (netfxver, arch.replace('\\\\', '-'))\n\n        # list all possibles registry paths\n        regpaths = []\n        if self.vs_ver >= 14.0:\n            for ver in self.NetFxSdkVersion:\n                regpaths += [join(self.ri.netfx_sdk, ver, fx)]\n\n        for ver in self.WindowsSdkVersion:\n            regpaths += [join(self.ri.windows_sdk, 'v%sA' % ver, fx)]\n\n        # Return installation folder from the more recent path\n        for path in regpaths:\n            execpath = self.ri.lookup(path, 'installationfolder')\n            if execpath:\n                return execpath\n\n    @property\n    def FSharpInstallDir(self):\n        \"\"\"\n        Microsoft Visual F# directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        path = join(self.ri.visualstudio, r'%0.1f\\Setup\\F#' % self.vs_ver)\n        return self.ri.lookup(path, 'productdir') or ''\n\n    @property\n    def UniversalCRTSdkDir(self):\n        \"\"\"\n        Microsoft Universal CRT SDK directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        # Set Kit Roots versions for specified MSVC++ version\n        vers = ('10', '81') if self.vs_ver >= 14.0 else ()\n\n        # Find path of the more recent Kit\n        for ver in vers:\n            sdkdir = self.ri.lookup(self.ri.windows_kits_roots,\n                                    'kitsroot%s' % ver)\n            if sdkdir:\n                return sdkdir or ''\n\n    @property\n    def UniversalCRTSdkLastVersion(self):\n        \"\"\"\n        Microsoft Universal C Runtime SDK last version.\n\n        Return\n        ------\n        str\n            version\n        \"\"\"\n        return self._use_last_dir_name(join(self.UniversalCRTSdkDir, 'lib'))\n\n    @property\n    def NetFxSdkVersion(self):\n        \"\"\"\n        Microsoft .NET Framework SDK versions.\n\n        Return\n        ------\n        tuple of str\n            versions\n        \"\"\"\n        # Set FxSdk versions for specified VS version\n        return (('4.7.2', '4.7.1', '4.7',\n                 '4.6.2', '4.6.1', '4.6',\n                 '4.5.2', '4.5.1', '4.5')\n                if self.vs_ver >= 14.0 else ())\n\n    @property\n    def NetFxSdkDir(self):\n        \"\"\"\n        Microsoft .NET Framework SDK directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        sdkdir = ''\n        for ver in self.NetFxSdkVersion:\n            loc = join(self.ri.netfx_sdk, ver)\n            sdkdir = self.ri.lookup(loc, 'kitsinstallationfolder')\n            if sdkdir:\n                break\n        return sdkdir\n\n    @property\n    def FrameworkDir32(self):\n        \"\"\"\n        Microsoft .NET Framework 32bit directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        # Default path\n        guess_fw = join(self.WinDir, r'Microsoft.NET\\Framework')\n\n        # Try to get path from registry, if fail use default path\n        return self.ri.lookup(self.ri.vc, 'frameworkdir32') or guess_fw\n\n    @property\n    def FrameworkDir64(self):\n        \"\"\"\n        Microsoft .NET Framework 64bit directory.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        # Default path\n        guess_fw = join(self.WinDir, r'Microsoft.NET\\Framework64')\n\n        # Try to get path from registry, if fail use default path\n        return self.ri.lookup(self.ri.vc, 'frameworkdir64') or guess_fw\n\n    @property\n    def FrameworkVersion32(self):\n        \"\"\"\n        Microsoft .NET Framework 32bit versions.\n\n        Return\n        ------\n        tuple of str\n            versions\n        \"\"\"\n        return self._find_dot_net_versions(32)\n\n    @property\n    def FrameworkVersion64(self):\n        \"\"\"\n        Microsoft .NET Framework 64bit versions.\n\n        Return\n        ------\n        tuple of str\n            versions\n        \"\"\"\n        return self._find_dot_net_versions(64)\n\n    def _find_dot_net_versions(self, bits):\n        \"\"\"\n        Find Microsoft .NET Framework versions.\n\n        Parameters\n        ----------\n        bits: int\n            Platform number of bits: 32 or 64.\n\n        Return\n        ------\n        tuple of str\n            versions\n        \"\"\"\n        # Find actual .NET version in registry\n        reg_ver = self.ri.lookup(self.ri.vc, 'frameworkver%d' % bits)\n        dot_net_dir = getattr(self, 'FrameworkDir%d' % bits)\n        ver = reg_ver or self._use_last_dir_name(dot_net_dir, 'v') or ''\n\n        # Set .NET versions for specified MSVC++ version\n        if self.vs_ver >= 12.0:\n            return ver, 'v4.0'\n        elif self.vs_ver >= 10.0:\n            return 'v4.0.30319' if ver.lower()[:2] != 'v4' else ver, 'v3.5'\n        elif self.vs_ver == 9.0:\n            return 'v3.5', 'v2.0.50727'\n        elif self.vs_ver == 8.0:\n            return 'v3.0', 'v2.0.50727'\n\n    @staticmethod\n    def _use_last_dir_name(path, prefix=''):\n        \"\"\"\n        Return name of the last dir in path or '' if no dir found.\n\n        Parameters\n        ----------\n        path: str\n            Use dirs in this path\n        prefix: str\n            Use only dirs starting by this prefix\n\n        Return\n        ------\n        str\n            name\n        \"\"\"\n        matching_dirs = (\n            dir_name\n            for dir_name in reversed(listdir(path))\n            if isdir(join(path, dir_name)) and\n            dir_name.startswith(prefix)\n        )\n        return next(matching_dirs, None) or ''\n\n\nclass EnvironmentInfo:\n    \"\"\"\n    Return environment variables for specified Microsoft Visual C++ version\n    and platform : Lib, Include, Path and libpath.\n\n    This function is compatible with Microsoft Visual C++ 9.0 to 14.X.\n\n    Script created by analysing Microsoft environment configuration files like\n    \"vcvars[...].bat\", \"SetEnv.Cmd\", \"vcbuildtools.bat\", ...\n\n    Parameters\n    ----------\n    arch: str\n        Target architecture.\n    vc_ver: float\n        Required Microsoft Visual C++ version. If not set, autodetect the last\n        version.\n    vc_min_ver: float\n        Minimum Microsoft Visual C++ version.\n    \"\"\"\n\n    # Variables and properties in this class use originals CamelCase variables\n    # names from Microsoft source files for more easy comparison.\n\n    def __init__(self, arch, vc_ver=None, vc_min_ver=0):\n        self.pi = PlatformInfo(arch)\n        self.ri = RegistryInfo(self.pi)\n        self.si = SystemInfo(self.ri, vc_ver)\n\n        if self.vc_ver < vc_min_ver:\n            err = 'No suitable Microsoft Visual C++ version found'\n            raise distutils.errors.DistutilsPlatformError(err)\n\n    @property\n    def vs_ver(self):\n        \"\"\"\n        Microsoft Visual Studio.\n\n        Return\n        ------\n        float\n            version\n        \"\"\"\n        return self.si.vs_ver\n\n    @property\n    def vc_ver(self):\n        \"\"\"\n        Microsoft Visual C++ version.\n\n        Return\n        ------\n        float\n            version\n        \"\"\"\n        return self.si.vc_ver\n\n    @property\n    def VSTools(self):\n        \"\"\"\n        Microsoft Visual Studio Tools.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        paths = [r'Common7\\IDE', r'Common7\\Tools']\n\n        if self.vs_ver >= 14.0:\n            arch_subdir = self.pi.current_dir(hidex86=True, x64=True)\n            paths += [r'Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow']\n            paths += [r'Team Tools\\Performance Tools']\n            paths += [r'Team Tools\\Performance Tools%s' % arch_subdir]\n\n        return [join(self.si.VSInstallDir, path) for path in paths]\n\n    @property\n    def VCIncludes(self):\n        \"\"\"\n        Microsoft Visual C++ & Microsoft Foundation Class Includes.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        return [join(self.si.VCInstallDir, 'Include'),\n                join(self.si.VCInstallDir, r'ATLMFC\\Include')]\n\n    @property\n    def VCLibraries(self):\n        \"\"\"\n        Microsoft Visual C++ & Microsoft Foundation Class Libraries.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver >= 15.0:\n            arch_subdir = self.pi.target_dir(x64=True)\n        else:\n            arch_subdir = self.pi.target_dir(hidex86=True)\n        paths = ['Lib%s' % arch_subdir, r'ATLMFC\\Lib%s' % arch_subdir]\n\n        if self.vs_ver >= 14.0:\n            paths += [r'Lib\\store%s' % arch_subdir]\n\n        return [join(self.si.VCInstallDir, path) for path in paths]\n\n    @property\n    def VCStoreRefs(self):\n        \"\"\"\n        Microsoft Visual C++ store references Libraries.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver < 14.0:\n            return []\n        return [join(self.si.VCInstallDir, r'Lib\\store\\references')]\n\n    @property\n    def VCTools(self):\n        \"\"\"\n        Microsoft Visual C++ Tools.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        si = self.si\n        tools = [join(si.VCInstallDir, 'VCPackages')]\n\n        forcex86 = True if self.vs_ver <= 10.0 else False\n        arch_subdir = self.pi.cross_dir(forcex86)\n        if arch_subdir:\n            tools += [join(si.VCInstallDir, 'Bin%s' % arch_subdir)]\n\n        if self.vs_ver == 14.0:\n            path = 'Bin%s' % self.pi.current_dir(hidex86=True)\n            tools += [join(si.VCInstallDir, path)]\n\n        elif self.vs_ver >= 15.0:\n            host_dir = (r'bin\\HostX86%s' if self.pi.current_is_x86() else\n                        r'bin\\HostX64%s')\n            tools += [join(\n                si.VCInstallDir, host_dir % self.pi.target_dir(x64=True))]\n\n            if self.pi.current_cpu != self.pi.target_cpu:\n                tools += [join(\n                    si.VCInstallDir, host_dir % self.pi.current_dir(x64=True))]\n\n        else:\n            tools += [join(si.VCInstallDir, 'Bin')]\n\n        return tools\n\n    @property\n    def OSLibraries(self):\n        \"\"\"\n        Microsoft Windows SDK Libraries.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver <= 10.0:\n            arch_subdir = self.pi.target_dir(hidex86=True, x64=True)\n            return [join(self.si.WindowsSdkDir, 'Lib%s' % arch_subdir)]\n\n        else:\n            arch_subdir = self.pi.target_dir(x64=True)\n            lib = join(self.si.WindowsSdkDir, 'lib')\n            libver = self._sdk_subdir\n            return [join(lib, '%sum%s' % (libver, arch_subdir))]\n\n    @property\n    def OSIncludes(self):\n        \"\"\"\n        Microsoft Windows SDK Include.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        include = join(self.si.WindowsSdkDir, 'include')\n\n        if self.vs_ver <= 10.0:\n            return [include, join(include, 'gl')]\n\n        else:\n            if self.vs_ver >= 14.0:\n                sdkver = self._sdk_subdir\n            else:\n                sdkver = ''\n            return [join(include, '%sshared' % sdkver),\n                    join(include, '%sum' % sdkver),\n                    join(include, '%swinrt' % sdkver)]\n\n    @property\n    def OSLibpath(self):\n        \"\"\"\n        Microsoft Windows SDK Libraries Paths.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        ref = join(self.si.WindowsSdkDir, 'References')\n        libpath = []\n\n        if self.vs_ver <= 9.0:\n            libpath += self.OSLibraries\n\n        if self.vs_ver >= 11.0:\n            libpath += [join(ref, r'CommonConfiguration\\Neutral')]\n\n        if self.vs_ver >= 14.0:\n            libpath += [\n                ref,\n                join(self.si.WindowsSdkDir, 'UnionMetadata'),\n                join(\n                    ref, 'Windows.Foundation.UniversalApiContract', '1.0.0.0'),\n                join(ref, 'Windows.Foundation.FoundationContract', '1.0.0.0'),\n                join(\n                    ref, 'Windows.Networking.Connectivity.WwanContract',\n                    '1.0.0.0'),\n                join(\n                    self.si.WindowsSdkDir, 'ExtensionSDKs', 'Microsoft.VCLibs',\n                    '%0.1f' % self.vs_ver, 'References', 'CommonConfiguration',\n                    'neutral'),\n            ]\n        return libpath\n\n    @property\n    def SdkTools(self):\n        \"\"\"\n        Microsoft Windows SDK Tools.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        return list(self._sdk_tools())\n\n    def _sdk_tools(self):\n        \"\"\"\n        Microsoft Windows SDK Tools paths generator.\n\n        Return\n        ------\n        generator of str\n            paths\n        \"\"\"\n        if self.vs_ver < 15.0:\n            bin_dir = 'Bin' if self.vs_ver <= 11.0 else r'Bin\\x86'\n            yield join(self.si.WindowsSdkDir, bin_dir)\n\n        if not self.pi.current_is_x86():\n            arch_subdir = self.pi.current_dir(x64=True)\n            path = 'Bin%s' % arch_subdir\n            yield join(self.si.WindowsSdkDir, path)\n\n        if self.vs_ver in (10.0, 11.0):\n            if self.pi.target_is_x86():\n                arch_subdir = ''\n            else:\n                arch_subdir = self.pi.current_dir(hidex86=True, x64=True)\n            path = r'Bin\\NETFX 4.0 Tools%s' % arch_subdir\n            yield join(self.si.WindowsSdkDir, path)\n\n        elif self.vs_ver >= 15.0:\n            path = join(self.si.WindowsSdkDir, 'Bin')\n            arch_subdir = self.pi.current_dir(x64=True)\n            sdkver = self.si.WindowsSdkLastVersion\n            yield join(path, '%s%s' % (sdkver, arch_subdir))\n\n        if self.si.WindowsSDKExecutablePath:\n            yield self.si.WindowsSDKExecutablePath\n\n    @property\n    def _sdk_subdir(self):\n        \"\"\"\n        Microsoft Windows SDK version subdir.\n\n        Return\n        ------\n        str\n            subdir\n        \"\"\"\n        ucrtver = self.si.WindowsSdkLastVersion\n        return ('%s\\\\' % ucrtver) if ucrtver else ''\n\n    @property\n    def SdkSetup(self):\n        \"\"\"\n        Microsoft Windows SDK Setup.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver > 9.0:\n            return []\n\n        return [join(self.si.WindowsSdkDir, 'Setup')]\n\n    @property\n    def FxTools(self):\n        \"\"\"\n        Microsoft .NET Framework Tools.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        pi = self.pi\n        si = self.si\n\n        if self.vs_ver <= 10.0:\n            include32 = True\n            include64 = not pi.target_is_x86() and not pi.current_is_x86()\n        else:\n            include32 = pi.target_is_x86() or pi.current_is_x86()\n            include64 = pi.current_cpu == 'amd64' or pi.target_cpu == 'amd64'\n\n        tools = []\n        if include32:\n            tools += [join(si.FrameworkDir32, ver)\n                      for ver in si.FrameworkVersion32]\n        if include64:\n            tools += [join(si.FrameworkDir64, ver)\n                      for ver in si.FrameworkVersion64]\n        return tools\n\n    @property\n    def NetFxSDKLibraries(self):\n        \"\"\"\n        Microsoft .Net Framework SDK Libraries.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver < 14.0 or not self.si.NetFxSdkDir:\n            return []\n\n        arch_subdir = self.pi.target_dir(x64=True)\n        return [join(self.si.NetFxSdkDir, r'lib\\um%s' % arch_subdir)]\n\n    @property\n    def NetFxSDKIncludes(self):\n        \"\"\"\n        Microsoft .Net Framework SDK Includes.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver < 14.0 or not self.si.NetFxSdkDir:\n            return []\n\n        return [join(self.si.NetFxSdkDir, r'include\\um')]\n\n    @property\n    def VsTDb(self):\n        \"\"\"\n        Microsoft Visual Studio Team System Database.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        return [join(self.si.VSInstallDir, r'VSTSDB\\Deploy')]\n\n    @property\n    def MSBuild(self):\n        \"\"\"\n        Microsoft Build Engine.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver < 12.0:\n            return []\n        elif self.vs_ver < 15.0:\n            base_path = self.si.ProgramFilesx86\n            arch_subdir = self.pi.current_dir(hidex86=True)\n        else:\n            base_path = self.si.VSInstallDir\n            arch_subdir = ''\n\n        path = r'MSBuild\\%0.1f\\bin%s' % (self.vs_ver, arch_subdir)\n        build = [join(base_path, path)]\n\n        if self.vs_ver >= 15.0:\n            # Add Roslyn C# & Visual Basic Compiler\n            build += [join(base_path, path, 'Roslyn')]\n\n        return build\n\n    @property\n    def HTMLHelpWorkshop(self):\n        \"\"\"\n        Microsoft HTML Help Workshop.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver < 11.0:\n            return []\n\n        return [join(self.si.ProgramFilesx86, 'HTML Help Workshop')]\n\n    @property\n    def UCRTLibraries(self):\n        \"\"\"\n        Microsoft Universal C Runtime SDK Libraries.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver < 14.0:\n            return []\n\n        arch_subdir = self.pi.target_dir(x64=True)\n        lib = join(self.si.UniversalCRTSdkDir, 'lib')\n        ucrtver = self._ucrt_subdir\n        return [join(lib, '%sucrt%s' % (ucrtver, arch_subdir))]\n\n    @property\n    def UCRTIncludes(self):\n        \"\"\"\n        Microsoft Universal C Runtime SDK Include.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if self.vs_ver < 14.0:\n            return []\n\n        include = join(self.si.UniversalCRTSdkDir, 'include')\n        return [join(include, '%sucrt' % self._ucrt_subdir)]\n\n    @property\n    def _ucrt_subdir(self):\n        \"\"\"\n        Microsoft Universal C Runtime SDK version subdir.\n\n        Return\n        ------\n        str\n            subdir\n        \"\"\"\n        ucrtver = self.si.UniversalCRTSdkLastVersion\n        return ('%s\\\\' % ucrtver) if ucrtver else ''\n\n    @property\n    def FSharp(self):\n        \"\"\"\n        Microsoft Visual F#.\n\n        Return\n        ------\n        list of str\n            paths\n        \"\"\"\n        if 11.0 > self.vs_ver > 12.0:\n            return []\n\n        return [self.si.FSharpInstallDir]\n\n    @property\n    def VCRuntimeRedist(self):\n        \"\"\"\n        Microsoft Visual C++ runtime redistributable dll.\n\n        Return\n        ------\n        str\n            path\n        \"\"\"\n        vcruntime = 'vcruntime%d0.dll' % self.vc_ver\n        arch_subdir = self.pi.target_dir(x64=True).strip('\\\\')\n\n        # Installation prefixes candidates\n        prefixes = []\n        tools_path = self.si.VCInstallDir\n        redist_path = dirname(tools_path.replace(r'\\Tools', r'\\Redist'))\n        if isdir(redist_path):\n            # Redist version may not be exactly the same as tools\n            redist_path = join(redist_path, listdir(redist_path)[-1])\n            prefixes += [redist_path, join(redist_path, 'onecore')]\n\n        prefixes += [join(tools_path, 'redist')]  # VS14 legacy path\n\n        # CRT directory\n        crt_dirs = ('Microsoft.VC%d.CRT' % (self.vc_ver * 10),\n                    # Sometime store in directory with VS version instead of VC\n                    'Microsoft.VC%d.CRT' % (int(self.vs_ver) * 10))\n\n        # vcruntime path\n        for prefix, crt_dir in itertools.product(prefixes, crt_dirs):\n            path = join(prefix, arch_subdir, crt_dir, vcruntime)\n            if isfile(path):\n                return path\n\n    def return_env(self, exists=True):\n        \"\"\"\n        Return environment dict.\n\n        Parameters\n        ----------\n        exists: bool\n            It True, only return existing paths.\n\n        Return\n        ------\n        dict\n            environment\n        \"\"\"\n        env = dict(\n            include=self._build_paths('include',\n                                      [self.VCIncludes,\n                                       self.OSIncludes,\n                                       self.UCRTIncludes,\n                                       self.NetFxSDKIncludes],\n                                      exists),\n            lib=self._build_paths('lib',\n                                  [self.VCLibraries,\n                                   self.OSLibraries,\n                                   self.FxTools,\n                                   self.UCRTLibraries,\n                                   self.NetFxSDKLibraries],\n                                  exists),\n            libpath=self._build_paths('libpath',\n                                      [self.VCLibraries,\n                                       self.FxTools,\n                                       self.VCStoreRefs,\n                                       self.OSLibpath],\n                                      exists),\n            path=self._build_paths('path',\n                                   [self.VCTools,\n                                    self.VSTools,\n                                    self.VsTDb,\n                                    self.SdkTools,\n                                    self.SdkSetup,\n                                    self.FxTools,\n                                    self.MSBuild,\n                                    self.HTMLHelpWorkshop,\n                                    self.FSharp],\n                                   exists),\n        )\n        if self.vs_ver >= 14 and isfile(self.VCRuntimeRedist):\n            env['py_vcruntime_redist'] = self.VCRuntimeRedist\n        return env\n\n    def _build_paths(self, name, spec_path_lists, exists):\n        \"\"\"\n        Given an environment variable name and specified paths,\n        return a pathsep-separated string of paths containing\n        unique, extant, directories from those paths and from\n        the environment variable. Raise an error if no paths\n        are resolved.\n\n        Parameters\n        ----------\n        name: str\n            Environment variable name\n        spec_path_lists: list of str\n            Paths\n        exists: bool\n            It True, only return existing paths.\n\n        Return\n        ------\n        str\n            Pathsep-separated paths\n        \"\"\"\n        # flatten spec_path_lists\n        spec_paths = itertools.chain.from_iterable(spec_path_lists)\n        env_paths = environ.get(name, '').split(pathsep)\n        paths = itertools.chain(spec_paths, env_paths)\n        extant_paths = list(filter(isdir, paths)) if exists else paths\n        if not extant_paths:\n            msg = \"%s environment variable is empty\" % name.upper()\n            raise distutils.errors.DistutilsPlatformError(msg)\n        unique_paths = self._unique_everseen(extant_paths)\n        return pathsep.join(unique_paths)\n\n    # from Python docs\n    @staticmethod\n    def _unique_everseen(iterable, key=None):\n        \"\"\"\n        List unique elements, preserving order.\n        Remember all elements ever seen.\n\n        _unique_everseen('AAAABBBCCDAABBB') --> A B C D\n\n        _unique_everseen('ABBCcAD', str.lower) --> A B C D\n        \"\"\"\n        seen = set()\n        seen_add = seen.add\n        if key is None:\n            for element in filterfalse(seen.__contains__, iterable):\n                seen_add(element)\n                yield element\n        else:\n            for element in iterable:\n                k = key(element)\n                if k not in seen:\n                    seen_add(k)\n                    yield element\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/msvc.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/msvc.py	(date 1602088701601)
@@ -30,12 +30,10 @@
 import distutils.errors
 from setuptools.extern.packaging.version import LegacyVersion
 
-from setuptools.extern.six.moves import filterfalse
-
 from .monkey import get_unpatched
 
 if platform.system() == 'Windows':
-    from setuptools.extern.six.moves import winreg
+    import winreg
     from os import environ
 else:
     # Mock winreg and environ so the module can be imported on this platform.
@@ -340,7 +338,7 @@
 
     if "vcvarsall" in message.lower() or "visual c" in message.lower():
         # Special error message if MSVC++ not installed
-        tmpl = 'Microsoft Visual C++ {version:0.1f} is required.'
+        tmpl = 'Microsoft Visual C++ {version:0.1f} or greater is required.'
         message = tmpl.format(**locals())
         msdownload = 'www.microsoft.com/download/details.aspx?id=%d'
         if version == 9.0:
@@ -360,8 +358,9 @@
             message += msdownload % 8279
         elif version >= 14.0:
             # For VC++ 14.X Redirect user to latest Visual C++ Build Tools
-            message += (' Get it with "Build Tools for Visual Studio": '
-                        r'https://visualstudio.microsoft.com/downloads/')
+            message += (' Get it with "Microsoft C++ Build Tools": '
+                        r'https://visualstudio.microsoft.com'
+                        r'/visual-cpp-build-tools/')
 
     exc.args = (message, )
 
@@ -1820,7 +1819,7 @@
         seen = set()
         seen_add = seen.add
         if key is None:
-            for element in filterfalse(seen.__contains__, iterable):
+            for element in itertools.filterfalse(seen.__contains__, iterable):
                 seen_add(element)
                 yield element
         else:
Index: env/lib/python3.8/site-packages/setuptools/_vendor/packaging/version.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import absolute_import, division, print_function\n\nimport collections\nimport itertools\nimport re\n\nfrom ._structures import Infinity\n\n\n__all__ = [\"parse\", \"Version\", \"LegacyVersion\", \"InvalidVersion\", \"VERSION_PATTERN\"]\n\n\n_Version = collections.namedtuple(\n    \"_Version\", [\"epoch\", \"release\", \"dev\", \"pre\", \"post\", \"local\"]\n)\n\n\ndef parse(version):\n    \"\"\"\n    Parse the given version string and return either a :class:`Version` object\n    or a :class:`LegacyVersion` object depending on if the given version is\n    a valid PEP 440 version or a legacy version.\n    \"\"\"\n    try:\n        return Version(version)\n    except InvalidVersion:\n        return LegacyVersion(version)\n\n\nclass InvalidVersion(ValueError):\n    \"\"\"\n    An invalid version was found, users should refer to PEP 440.\n    \"\"\"\n\n\nclass _BaseVersion(object):\n    def __hash__(self):\n        return hash(self._key)\n\n    def __lt__(self, other):\n        return self._compare(other, lambda s, o: s < o)\n\n    def __le__(self, other):\n        return self._compare(other, lambda s, o: s <= o)\n\n    def __eq__(self, other):\n        return self._compare(other, lambda s, o: s == o)\n\n    def __ge__(self, other):\n        return self._compare(other, lambda s, o: s >= o)\n\n    def __gt__(self, other):\n        return self._compare(other, lambda s, o: s > o)\n\n    def __ne__(self, other):\n        return self._compare(other, lambda s, o: s != o)\n\n    def _compare(self, other, method):\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return method(self._key, other._key)\n\n\nclass LegacyVersion(_BaseVersion):\n    def __init__(self, version):\n        self._version = str(version)\n        self._key = _legacy_cmpkey(self._version)\n\n    def __str__(self):\n        return self._version\n\n    def __repr__(self):\n        return \"<LegacyVersion({0})>\".format(repr(str(self)))\n\n    @property\n    def public(self):\n        return self._version\n\n    @property\n    def base_version(self):\n        return self._version\n\n    @property\n    def epoch(self):\n        return -1\n\n    @property\n    def release(self):\n        return None\n\n    @property\n    def pre(self):\n        return None\n\n    @property\n    def post(self):\n        return None\n\n    @property\n    def dev(self):\n        return None\n\n    @property\n    def local(self):\n        return None\n\n    @property\n    def is_prerelease(self):\n        return False\n\n    @property\n    def is_postrelease(self):\n        return False\n\n    @property\n    def is_devrelease(self):\n        return False\n\n\n_legacy_version_component_re = re.compile(r\"(\\d+ | [a-z]+ | \\.| -)\", re.VERBOSE)\n\n_legacy_version_replacement_map = {\n    \"pre\": \"c\",\n    \"preview\": \"c\",\n    \"-\": \"final-\",\n    \"rc\": \"c\",\n    \"dev\": \"@\",\n}\n\n\ndef _parse_version_parts(s):\n    for part in _legacy_version_component_re.split(s):\n        part = _legacy_version_replacement_map.get(part, part)\n\n        if not part or part == \".\":\n            continue\n\n        if part[:1] in \"0123456789\":\n            # pad for numeric comparison\n            yield part.zfill(8)\n        else:\n            yield \"*\" + part\n\n    # ensure that alpha/beta/candidate are before final\n    yield \"*final\"\n\n\ndef _legacy_cmpkey(version):\n    # We hardcode an epoch of -1 here. A PEP 440 version can only have a epoch\n    # greater than or equal to 0. This will effectively put the LegacyVersion,\n    # which uses the defacto standard originally implemented by setuptools,\n    # as before all PEP 440 versions.\n    epoch = -1\n\n    # This scheme is taken from pkg_resources.parse_version setuptools prior to\n    # it's adoption of the packaging library.\n    parts = []\n    for part in _parse_version_parts(version.lower()):\n        if part.startswith(\"*\"):\n            # remove \"-\" before a prerelease tag\n            if part < \"*final\":\n                while parts and parts[-1] == \"*final-\":\n                    parts.pop()\n\n            # remove trailing zeros from each series of numeric parts\n            while parts and parts[-1] == \"00000000\":\n                parts.pop()\n\n        parts.append(part)\n    parts = tuple(parts)\n\n    return epoch, parts\n\n\n# Deliberately not anchored to the start and end of the string, to make it\n# easier for 3rd party code to reuse\nVERSION_PATTERN = r\"\"\"\n    v?\n    (?:\n        (?:(?P<epoch>[0-9]+)!)?                           # epoch\n        (?P<release>[0-9]+(?:\\.[0-9]+)*)                  # release segment\n        (?P<pre>                                          # pre-release\n            [-_\\.]?\n            (?P<pre_l>(a|b|c|rc|alpha|beta|pre|preview))\n            [-_\\.]?\n            (?P<pre_n>[0-9]+)?\n        )?\n        (?P<post>                                         # post release\n            (?:-(?P<post_n1>[0-9]+))\n            |\n            (?:\n                [-_\\.]?\n                (?P<post_l>post|rev|r)\n                [-_\\.]?\n                (?P<post_n2>[0-9]+)?\n            )\n        )?\n        (?P<dev>                                          # dev release\n            [-_\\.]?\n            (?P<dev_l>dev)\n            [-_\\.]?\n            (?P<dev_n>[0-9]+)?\n        )?\n    )\n    (?:\\+(?P<local>[a-z0-9]+(?:[-_\\.][a-z0-9]+)*))?       # local version\n\"\"\"\n\n\nclass Version(_BaseVersion):\n\n    _regex = re.compile(r\"^\\s*\" + VERSION_PATTERN + r\"\\s*$\", re.VERBOSE | re.IGNORECASE)\n\n    def __init__(self, version):\n        # Validate the version and parse it into pieces\n        match = self._regex.search(version)\n        if not match:\n            raise InvalidVersion(\"Invalid version: '{0}'\".format(version))\n\n        # Store the parsed out pieces of the version\n        self._version = _Version(\n            epoch=int(match.group(\"epoch\")) if match.group(\"epoch\") else 0,\n            release=tuple(int(i) for i in match.group(\"release\").split(\".\")),\n            pre=_parse_letter_version(match.group(\"pre_l\"), match.group(\"pre_n\")),\n            post=_parse_letter_version(\n                match.group(\"post_l\"), match.group(\"post_n1\") or match.group(\"post_n2\")\n            ),\n            dev=_parse_letter_version(match.group(\"dev_l\"), match.group(\"dev_n\")),\n            local=_parse_local_version(match.group(\"local\")),\n        )\n\n        # Generate a key which will be used for sorting\n        self._key = _cmpkey(\n            self._version.epoch,\n            self._version.release,\n            self._version.pre,\n            self._version.post,\n            self._version.dev,\n            self._version.local,\n        )\n\n    def __repr__(self):\n        return \"<Version({0})>\".format(repr(str(self)))\n\n    def __str__(self):\n        parts = []\n\n        # Epoch\n        if self.epoch != 0:\n            parts.append(\"{0}!\".format(self.epoch))\n\n        # Release segment\n        parts.append(\".\".join(str(x) for x in self.release))\n\n        # Pre-release\n        if self.pre is not None:\n            parts.append(\"\".join(str(x) for x in self.pre))\n\n        # Post-release\n        if self.post is not None:\n            parts.append(\".post{0}\".format(self.post))\n\n        # Development release\n        if self.dev is not None:\n            parts.append(\".dev{0}\".format(self.dev))\n\n        # Local version segment\n        if self.local is not None:\n            parts.append(\"+{0}\".format(self.local))\n\n        return \"\".join(parts)\n\n    @property\n    def epoch(self):\n        return self._version.epoch\n\n    @property\n    def release(self):\n        return self._version.release\n\n    @property\n    def pre(self):\n        return self._version.pre\n\n    @property\n    def post(self):\n        return self._version.post[1] if self._version.post else None\n\n    @property\n    def dev(self):\n        return self._version.dev[1] if self._version.dev else None\n\n    @property\n    def local(self):\n        if self._version.local:\n            return \".\".join(str(x) for x in self._version.local)\n        else:\n            return None\n\n    @property\n    def public(self):\n        return str(self).split(\"+\", 1)[0]\n\n    @property\n    def base_version(self):\n        parts = []\n\n        # Epoch\n        if self.epoch != 0:\n            parts.append(\"{0}!\".format(self.epoch))\n\n        # Release segment\n        parts.append(\".\".join(str(x) for x in self.release))\n\n        return \"\".join(parts)\n\n    @property\n    def is_prerelease(self):\n        return self.dev is not None or self.pre is not None\n\n    @property\n    def is_postrelease(self):\n        return self.post is not None\n\n    @property\n    def is_devrelease(self):\n        return self.dev is not None\n\n\ndef _parse_letter_version(letter, number):\n    if letter:\n        # We consider there to be an implicit 0 in a pre-release if there is\n        # not a numeral associated with it.\n        if number is None:\n            number = 0\n\n        # We normalize any letters to their lower case form\n        letter = letter.lower()\n\n        # We consider some words to be alternate spellings of other words and\n        # in those cases we want to normalize the spellings to our preferred\n        # spelling.\n        if letter == \"alpha\":\n            letter = \"a\"\n        elif letter == \"beta\":\n            letter = \"b\"\n        elif letter in [\"c\", \"pre\", \"preview\"]:\n            letter = \"rc\"\n        elif letter in [\"rev\", \"r\"]:\n            letter = \"post\"\n\n        return letter, int(number)\n    if not letter and number:\n        # We assume if we are given a number, but we are not given a letter\n        # then this is using the implicit post release syntax (e.g. 1.0-1)\n        letter = \"post\"\n\n        return letter, int(number)\n\n\n_local_version_separators = re.compile(r\"[\\._-]\")\n\n\ndef _parse_local_version(local):\n    \"\"\"\n    Takes a string like abc.1.twelve and turns it into (\"abc\", 1, \"twelve\").\n    \"\"\"\n    if local is not None:\n        return tuple(\n            part.lower() if not part.isdigit() else int(part)\n            for part in _local_version_separators.split(local)\n        )\n\n\ndef _cmpkey(epoch, release, pre, post, dev, local):\n    # When we compare a release version, we want to compare it with all of the\n    # trailing zeros removed. So we'll use a reverse the list, drop all the now\n    # leading zeros until we come to something non zero, then take the rest\n    # re-reverse it back into the correct order and make it a tuple and use\n    # that for our sorting key.\n    release = tuple(\n        reversed(list(itertools.dropwhile(lambda x: x == 0, reversed(release))))\n    )\n\n    # We need to \"trick\" the sorting algorithm to put 1.0.dev0 before 1.0a0.\n    # We'll do this by abusing the pre segment, but we _only_ want to do this\n    # if there is not a pre or a post segment. If we have one of those then\n    # the normal sorting rules will handle this case correctly.\n    if pre is None and post is None and dev is not None:\n        pre = -Infinity\n    # Versions without a pre-release (except as noted above) should sort after\n    # those with one.\n    elif pre is None:\n        pre = Infinity\n\n    # Versions without a post segment should sort before those with one.\n    if post is None:\n        post = -Infinity\n\n    # Versions without a development segment should sort after those with one.\n    if dev is None:\n        dev = Infinity\n\n    if local is None:\n        # Versions without a local segment should sort before those with one.\n        local = -Infinity\n    else:\n        # Versions with a local segment need that segment parsed to implement\n        # the sorting rules in PEP440.\n        # - Alpha numeric segments sort before numeric segments\n        # - Alpha numeric segments sort lexicographically\n        # - Numeric segments sort numerically\n        # - Shorter versions sort before longer versions when the prefixes\n        #   match exactly\n        local = tuple((i, \"\") if isinstance(i, int) else (-Infinity, i) for i in local)\n\n    return epoch, release, pre, post, dev, local\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/_vendor/packaging/version.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/_vendor/packaging/version.py	(date 1602088701617)
@@ -7,8 +7,35 @@
 import itertools
 import re
 
-from ._structures import Infinity
+from ._structures import Infinity, NegativeInfinity
+from ._typing import TYPE_CHECKING
+
+if TYPE_CHECKING:  # pragma: no cover
+    from typing import Callable, Iterator, List, Optional, SupportsInt, Tuple, Union
 
+    from ._structures import InfinityType, NegativeInfinityType
+
+    InfiniteTypes = Union[InfinityType, NegativeInfinityType]
+    PrePostDevType = Union[InfiniteTypes, Tuple[str, int]]
+    SubLocalType = Union[InfiniteTypes, int, str]
+    LocalType = Union[
+        NegativeInfinityType,
+        Tuple[
+            Union[
+                SubLocalType,
+                Tuple[SubLocalType, str],
+                Tuple[NegativeInfinityType, SubLocalType],
+            ],
+            ...,
+        ],
+    ]
+    CmpKey = Tuple[
+        int, Tuple[int, ...], PrePostDevType, PrePostDevType, PrePostDevType, LocalType
+    ]
+    LegacyCmpKey = Tuple[int, Tuple[str, ...]]
+    VersionComparisonMethod = Callable[
+        [Union[CmpKey, LegacyCmpKey], Union[CmpKey, LegacyCmpKey]], bool
+    ]
 
 __all__ = ["parse", "Version", "LegacyVersion", "InvalidVersion", "VERSION_PATTERN"]
 
@@ -19,6 +46,7 @@
 
 
 def parse(version):
+    # type: (str) -> Union[LegacyVersion, Version]
     """
     Parse the given version string and return either a :class:`Version` object
     or a :class:`LegacyVersion` object depending on if the given version is
@@ -37,28 +65,38 @@
 
 
 class _BaseVersion(object):
+    _key = None  # type: Union[CmpKey, LegacyCmpKey]
+
     def __hash__(self):
+        # type: () -> int
         return hash(self._key)
 
     def __lt__(self, other):
+        # type: (_BaseVersion) -> bool
         return self._compare(other, lambda s, o: s < o)
 
     def __le__(self, other):
+        # type: (_BaseVersion) -> bool
         return self._compare(other, lambda s, o: s <= o)
 
     def __eq__(self, other):
+        # type: (object) -> bool
         return self._compare(other, lambda s, o: s == o)
 
     def __ge__(self, other):
+        # type: (_BaseVersion) -> bool
         return self._compare(other, lambda s, o: s >= o)
 
     def __gt__(self, other):
+        # type: (_BaseVersion) -> bool
         return self._compare(other, lambda s, o: s > o)
 
     def __ne__(self, other):
+        # type: (object) -> bool
         return self._compare(other, lambda s, o: s != o)
 
     def _compare(self, other, method):
+        # type: (object, VersionComparisonMethod) -> Union[bool, NotImplemented]
         if not isinstance(other, _BaseVersion):
             return NotImplemented
 
@@ -67,57 +105,71 @@
 
 class LegacyVersion(_BaseVersion):
     def __init__(self, version):
+        # type: (str) -> None
         self._version = str(version)
         self._key = _legacy_cmpkey(self._version)
 
     def __str__(self):
+        # type: () -> str
         return self._version
 
     def __repr__(self):
+        # type: () -> str
         return "<LegacyVersion({0})>".format(repr(str(self)))
 
     @property
     def public(self):
+        # type: () -> str
         return self._version
 
     @property
     def base_version(self):
+        # type: () -> str
         return self._version
 
     @property
     def epoch(self):
+        # type: () -> int
         return -1
 
     @property
     def release(self):
+        # type: () -> None
         return None
 
     @property
     def pre(self):
+        # type: () -> None
         return None
 
     @property
     def post(self):
+        # type: () -> None
         return None
 
     @property
     def dev(self):
+        # type: () -> None
         return None
 
     @property
     def local(self):
+        # type: () -> None
         return None
 
     @property
     def is_prerelease(self):
+        # type: () -> bool
         return False
 
     @property
     def is_postrelease(self):
+        # type: () -> bool
         return False
 
     @property
     def is_devrelease(self):
+        # type: () -> bool
         return False
 
 
@@ -133,6 +185,7 @@
 
 
 def _parse_version_parts(s):
+    # type: (str) -> Iterator[str]
     for part in _legacy_version_component_re.split(s):
         part = _legacy_version_replacement_map.get(part, part)
 
@@ -150,6 +203,8 @@
 
 
 def _legacy_cmpkey(version):
+    # type: (str) -> LegacyCmpKey
+
     # We hardcode an epoch of -1 here. A PEP 440 version can only have a epoch
     # greater than or equal to 0. This will effectively put the LegacyVersion,
     # which uses the defacto standard originally implemented by setuptools,
@@ -158,7 +213,7 @@
 
     # This scheme is taken from pkg_resources.parse_version setuptools prior to
     # it's adoption of the packaging library.
-    parts = []
+    parts = []  # type: List[str]
     for part in _parse_version_parts(version.lower()):
         if part.startswith("*"):
             # remove "-" before a prerelease tag
@@ -171,9 +226,8 @@
                 parts.pop()
 
         parts.append(part)
-    parts = tuple(parts)
 
-    return epoch, parts
+    return epoch, tuple(parts)
 
 
 # Deliberately not anchored to the start and end of the string, to make it
@@ -215,6 +269,8 @@
     _regex = re.compile(r"^\s*" + VERSION_PATTERN + r"\s*$", re.VERBOSE | re.IGNORECASE)
 
     def __init__(self, version):
+        # type: (str) -> None
+
         # Validate the version and parse it into pieces
         match = self._regex.search(version)
         if not match:
@@ -243,9 +299,11 @@
         )
 
     def __repr__(self):
+        # type: () -> str
         return "<Version({0})>".format(repr(str(self)))
 
     def __str__(self):
+        # type: () -> str
         parts = []
 
         # Epoch
@@ -275,26 +333,35 @@
 
     @property
     def epoch(self):
-        return self._version.epoch
+        # type: () -> int
+        _epoch = self._version.epoch  # type: int
+        return _epoch
 
     @property
     def release(self):
-        return self._version.release
+        # type: () -> Tuple[int, ...]
+        _release = self._version.release  # type: Tuple[int, ...]
+        return _release
 
     @property
     def pre(self):
-        return self._version.pre
+        # type: () -> Optional[Tuple[str, int]]
+        _pre = self._version.pre  # type: Optional[Tuple[str, int]]
+        return _pre
 
     @property
     def post(self):
+        # type: () -> Optional[Tuple[str, int]]
         return self._version.post[1] if self._version.post else None
 
     @property
     def dev(self):
+        # type: () -> Optional[Tuple[str, int]]
         return self._version.dev[1] if self._version.dev else None
 
     @property
     def local(self):
+        # type: () -> Optional[str]
         if self._version.local:
             return ".".join(str(x) for x in self._version.local)
         else:
@@ -302,10 +369,12 @@
 
     @property
     def public(self):
+        # type: () -> str
         return str(self).split("+", 1)[0]
 
     @property
     def base_version(self):
+        # type: () -> str
         parts = []
 
         # Epoch
@@ -319,18 +388,41 @@
 
     @property
     def is_prerelease(self):
+        # type: () -> bool
         return self.dev is not None or self.pre is not None
 
     @property
     def is_postrelease(self):
+        # type: () -> bool
         return self.post is not None
 
     @property
     def is_devrelease(self):
+        # type: () -> bool
         return self.dev is not None
 
+    @property
+    def major(self):
+        # type: () -> int
+        return self.release[0] if len(self.release) >= 1 else 0
+
+    @property
+    def minor(self):
+        # type: () -> int
+        return self.release[1] if len(self.release) >= 2 else 0
+
+    @property
+    def micro(self):
+        # type: () -> int
+        return self.release[2] if len(self.release) >= 3 else 0
+
 
-def _parse_letter_version(letter, number):
+def _parse_letter_version(
+    letter,  # type: str
+    number,  # type: Union[str, bytes, SupportsInt]
+):
+    # type: (...) -> Optional[Tuple[str, int]]
+
     if letter:
         # We consider there to be an implicit 0 in a pre-release if there is
         # not a numeral associated with it.
@@ -360,11 +452,14 @@
 
         return letter, int(number)
 
+    return None
+
 
 _local_version_separators = re.compile(r"[\._-]")
 
 
 def _parse_local_version(local):
+    # type: (str) -> Optional[LocalType]
     """
     Takes a string like abc.1.twelve and turns it into ("abc", 1, "twelve").
     """
@@ -373,15 +468,25 @@
             part.lower() if not part.isdigit() else int(part)
             for part in _local_version_separators.split(local)
         )
+    return None
 
 
-def _cmpkey(epoch, release, pre, post, dev, local):
+def _cmpkey(
+    epoch,  # type: int
+    release,  # type: Tuple[int, ...]
+    pre,  # type: Optional[Tuple[str, int]]
+    post,  # type: Optional[Tuple[str, int]]
+    dev,  # type: Optional[Tuple[str, int]]
+    local,  # type: Optional[Tuple[SubLocalType]]
+):
+    # type: (...) -> CmpKey
+
     # When we compare a release version, we want to compare it with all of the
     # trailing zeros removed. So we'll use a reverse the list, drop all the now
     # leading zeros until we come to something non zero, then take the rest
     # re-reverse it back into the correct order and make it a tuple and use
     # that for our sorting key.
-    release = tuple(
+    _release = tuple(
         reversed(list(itertools.dropwhile(lambda x: x == 0, reversed(release))))
     )
 
@@ -390,23 +495,31 @@
     # if there is not a pre or a post segment. If we have one of those then
     # the normal sorting rules will handle this case correctly.
     if pre is None and post is None and dev is not None:
-        pre = -Infinity
+        _pre = NegativeInfinity  # type: PrePostDevType
     # Versions without a pre-release (except as noted above) should sort after
     # those with one.
     elif pre is None:
-        pre = Infinity
+        _pre = Infinity
+    else:
+        _pre = pre
 
     # Versions without a post segment should sort before those with one.
     if post is None:
-        post = -Infinity
+        _post = NegativeInfinity  # type: PrePostDevType
+
+    else:
+        _post = post
 
     # Versions without a development segment should sort after those with one.
     if dev is None:
-        dev = Infinity
+        _dev = Infinity  # type: PrePostDevType
+
+    else:
+        _dev = dev
 
     if local is None:
         # Versions without a local segment should sort before those with one.
-        local = -Infinity
+        _local = NegativeInfinity  # type: LocalType
     else:
         # Versions with a local segment need that segment parsed to implement
         # the sorting rules in PEP440.
@@ -415,6 +528,8 @@
         # - Numeric segments sort numerically
         # - Shorter versions sort before longer versions when the prefixes
         #   match exactly
-        local = tuple((i, "") if isinstance(i, int) else (-Infinity, i) for i in local)
+        _local = tuple(
+            (i, "") if isinstance(i, int) else (NegativeInfinity, i) for i in local
+        )
 
-    return epoch, release, pre, post, dev, local
+    return epoch, _release, _pre, _post, _dev, _local
Index: env/lib/python3.8/site-packages/setuptools/_vendor/packaging/markers.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import absolute_import, division, print_function\n\nimport operator\nimport os\nimport platform\nimport sys\n\nfrom setuptools.extern.pyparsing import ParseException, ParseResults, stringStart, stringEnd\nfrom setuptools.extern.pyparsing import ZeroOrMore, Group, Forward, QuotedString\nfrom setuptools.extern.pyparsing import Literal as L  # noqa\n\nfrom ._compat import string_types\nfrom .specifiers import Specifier, InvalidSpecifier\n\n\n__all__ = [\n    \"InvalidMarker\",\n    \"UndefinedComparison\",\n    \"UndefinedEnvironmentName\",\n    \"Marker\",\n    \"default_environment\",\n]\n\n\nclass InvalidMarker(ValueError):\n    \"\"\"\n    An invalid marker was found, users should refer to PEP 508.\n    \"\"\"\n\n\nclass UndefinedComparison(ValueError):\n    \"\"\"\n    An invalid operation was attempted on a value that doesn't support it.\n    \"\"\"\n\n\nclass UndefinedEnvironmentName(ValueError):\n    \"\"\"\n    A name was attempted to be used that does not exist inside of the\n    environment.\n    \"\"\"\n\n\nclass Node(object):\n    def __init__(self, value):\n        self.value = value\n\n    def __str__(self):\n        return str(self.value)\n\n    def __repr__(self):\n        return \"<{0}({1!r})>\".format(self.__class__.__name__, str(self))\n\n    def serialize(self):\n        raise NotImplementedError\n\n\nclass Variable(Node):\n    def serialize(self):\n        return str(self)\n\n\nclass Value(Node):\n    def serialize(self):\n        return '\"{0}\"'.format(self)\n\n\nclass Op(Node):\n    def serialize(self):\n        return str(self)\n\n\nVARIABLE = (\n    L(\"implementation_version\")\n    | L(\"platform_python_implementation\")\n    | L(\"implementation_name\")\n    | L(\"python_full_version\")\n    | L(\"platform_release\")\n    | L(\"platform_version\")\n    | L(\"platform_machine\")\n    | L(\"platform_system\")\n    | L(\"python_version\")\n    | L(\"sys_platform\")\n    | L(\"os_name\")\n    | L(\"os.name\")\n    | L(\"sys.platform\")  # PEP-345\n    | L(\"platform.version\")  # PEP-345\n    | L(\"platform.machine\")  # PEP-345\n    | L(\"platform.python_implementation\")  # PEP-345\n    | L(\"python_implementation\")  # PEP-345\n    | L(\"extra\")  # undocumented setuptools legacy\n)\nALIASES = {\n    \"os.name\": \"os_name\",\n    \"sys.platform\": \"sys_platform\",\n    \"platform.version\": \"platform_version\",\n    \"platform.machine\": \"platform_machine\",\n    \"platform.python_implementation\": \"platform_python_implementation\",\n    \"python_implementation\": \"platform_python_implementation\",\n}\nVARIABLE.setParseAction(lambda s, l, t: Variable(ALIASES.get(t[0], t[0])))\n\nVERSION_CMP = (\n    L(\"===\") | L(\"==\") | L(\">=\") | L(\"<=\") | L(\"!=\") | L(\"~=\") | L(\">\") | L(\"<\")\n)\n\nMARKER_OP = VERSION_CMP | L(\"not in\") | L(\"in\")\nMARKER_OP.setParseAction(lambda s, l, t: Op(t[0]))\n\nMARKER_VALUE = QuotedString(\"'\") | QuotedString('\"')\nMARKER_VALUE.setParseAction(lambda s, l, t: Value(t[0]))\n\nBOOLOP = L(\"and\") | L(\"or\")\n\nMARKER_VAR = VARIABLE | MARKER_VALUE\n\nMARKER_ITEM = Group(MARKER_VAR + MARKER_OP + MARKER_VAR)\nMARKER_ITEM.setParseAction(lambda s, l, t: tuple(t[0]))\n\nLPAREN = L(\"(\").suppress()\nRPAREN = L(\")\").suppress()\n\nMARKER_EXPR = Forward()\nMARKER_ATOM = MARKER_ITEM | Group(LPAREN + MARKER_EXPR + RPAREN)\nMARKER_EXPR << MARKER_ATOM + ZeroOrMore(BOOLOP + MARKER_EXPR)\n\nMARKER = stringStart + MARKER_EXPR + stringEnd\n\n\ndef _coerce_parse_result(results):\n    if isinstance(results, ParseResults):\n        return [_coerce_parse_result(i) for i in results]\n    else:\n        return results\n\n\ndef _format_marker(marker, first=True):\n    assert isinstance(marker, (list, tuple, string_types))\n\n    # Sometimes we have a structure like [[...]] which is a single item list\n    # where the single item is itself it's own list. In that case we want skip\n    # the rest of this function so that we don't get extraneous () on the\n    # outside.\n    if (\n        isinstance(marker, list)\n        and len(marker) == 1\n        and isinstance(marker[0], (list, tuple))\n    ):\n        return _format_marker(marker[0])\n\n    if isinstance(marker, list):\n        inner = (_format_marker(m, first=False) for m in marker)\n        if first:\n            return \" \".join(inner)\n        else:\n            return \"(\" + \" \".join(inner) + \")\"\n    elif isinstance(marker, tuple):\n        return \" \".join([m.serialize() for m in marker])\n    else:\n        return marker\n\n\n_operators = {\n    \"in\": lambda lhs, rhs: lhs in rhs,\n    \"not in\": lambda lhs, rhs: lhs not in rhs,\n    \"<\": operator.lt,\n    \"<=\": operator.le,\n    \"==\": operator.eq,\n    \"!=\": operator.ne,\n    \">=\": operator.ge,\n    \">\": operator.gt,\n}\n\n\ndef _eval_op(lhs, op, rhs):\n    try:\n        spec = Specifier(\"\".join([op.serialize(), rhs]))\n    except InvalidSpecifier:\n        pass\n    else:\n        return spec.contains(lhs)\n\n    oper = _operators.get(op.serialize())\n    if oper is None:\n        raise UndefinedComparison(\n            \"Undefined {0!r} on {1!r} and {2!r}.\".format(op, lhs, rhs)\n        )\n\n    return oper(lhs, rhs)\n\n\n_undefined = object()\n\n\ndef _get_env(environment, name):\n    value = environment.get(name, _undefined)\n\n    if value is _undefined:\n        raise UndefinedEnvironmentName(\n            \"{0!r} does not exist in evaluation environment.\".format(name)\n        )\n\n    return value\n\n\ndef _evaluate_markers(markers, environment):\n    groups = [[]]\n\n    for marker in markers:\n        assert isinstance(marker, (list, tuple, string_types))\n\n        if isinstance(marker, list):\n            groups[-1].append(_evaluate_markers(marker, environment))\n        elif isinstance(marker, tuple):\n            lhs, op, rhs = marker\n\n            if isinstance(lhs, Variable):\n                lhs_value = _get_env(environment, lhs.value)\n                rhs_value = rhs.value\n            else:\n                lhs_value = lhs.value\n                rhs_value = _get_env(environment, rhs.value)\n\n            groups[-1].append(_eval_op(lhs_value, op, rhs_value))\n        else:\n            assert marker in [\"and\", \"or\"]\n            if marker == \"or\":\n                groups.append([])\n\n    return any(all(item) for item in groups)\n\n\ndef format_full_version(info):\n    version = \"{0.major}.{0.minor}.{0.micro}\".format(info)\n    kind = info.releaselevel\n    if kind != \"final\":\n        version += kind[0] + str(info.serial)\n    return version\n\n\ndef default_environment():\n    if hasattr(sys, \"implementation\"):\n        iver = format_full_version(sys.implementation.version)\n        implementation_name = sys.implementation.name\n    else:\n        iver = \"0\"\n        implementation_name = \"\"\n\n    return {\n        \"implementation_name\": implementation_name,\n        \"implementation_version\": iver,\n        \"os_name\": os.name,\n        \"platform_machine\": platform.machine(),\n        \"platform_release\": platform.release(),\n        \"platform_system\": platform.system(),\n        \"platform_version\": platform.version(),\n        \"python_full_version\": platform.python_version(),\n        \"platform_python_implementation\": platform.python_implementation(),\n        \"python_version\": \".\".join(platform.python_version_tuple()[:2]),\n        \"sys_platform\": sys.platform,\n    }\n\n\nclass Marker(object):\n    def __init__(self, marker):\n        try:\n            self._markers = _coerce_parse_result(MARKER.parseString(marker))\n        except ParseException as e:\n            err_str = \"Invalid marker: {0!r}, parse error at {1!r}\".format(\n                marker, marker[e.loc : e.loc + 8]\n            )\n            raise InvalidMarker(err_str)\n\n    def __str__(self):\n        return _format_marker(self._markers)\n\n    def __repr__(self):\n        return \"<Marker({0!r})>\".format(str(self))\n\n    def evaluate(self, environment=None):\n        \"\"\"Evaluate a marker.\n\n        Return the boolean from evaluating the given marker against the\n        environment. environment is an optional argument to override all or\n        part of the determined environment.\n\n        The environment is determined from the current Python process.\n        \"\"\"\n        current_environment = default_environment()\n        if environment is not None:\n            current_environment.update(environment)\n\n        return _evaluate_markers(self._markers, current_environment)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/_vendor/packaging/markers.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/_vendor/packaging/markers.py	(date 1602088701617)
@@ -13,8 +13,14 @@
 from setuptools.extern.pyparsing import Literal as L  # noqa
 
 from ._compat import string_types
+from ._typing import TYPE_CHECKING
 from .specifiers import Specifier, InvalidSpecifier
 
+if TYPE_CHECKING:  # pragma: no cover
+    from typing import Any, Callable, Dict, List, Optional, Tuple, Union
+
+    Operator = Callable[[str, str], bool]
+
 
 __all__ = [
     "InvalidMarker",
@@ -46,30 +52,37 @@
 
 class Node(object):
     def __init__(self, value):
+        # type: (Any) -> None
         self.value = value
 
     def __str__(self):
+        # type: () -> str
         return str(self.value)
 
     def __repr__(self):
+        # type: () -> str
         return "<{0}({1!r})>".format(self.__class__.__name__, str(self))
 
     def serialize(self):
+        # type: () -> str
         raise NotImplementedError
 
 
 class Variable(Node):
     def serialize(self):
+        # type: () -> str
         return str(self)
 
 
 class Value(Node):
     def serialize(self):
+        # type: () -> str
         return '"{0}"'.format(self)
 
 
 class Op(Node):
     def serialize(self):
+        # type: () -> str
         return str(self)
 
 
@@ -85,13 +98,13 @@
     | L("python_version")
     | L("sys_platform")
     | L("os_name")
-    | L("os.name")
+    | L("os.name")  # PEP-345
     | L("sys.platform")  # PEP-345
     | L("platform.version")  # PEP-345
     | L("platform.machine")  # PEP-345
     | L("platform.python_implementation")  # PEP-345
-    | L("python_implementation")  # PEP-345
-    | L("extra")  # undocumented setuptools legacy
+    | L("python_implementation")  # undocumented setuptools legacy
+    | L("extra")  # PEP-508
 )
 ALIASES = {
     "os.name": "os_name",
@@ -131,6 +144,7 @@
 
 
 def _coerce_parse_result(results):
+    # type: (Union[ParseResults, List[Any]]) -> List[Any]
     if isinstance(results, ParseResults):
         return [_coerce_parse_result(i) for i in results]
     else:
@@ -138,6 +152,8 @@
 
 
 def _format_marker(marker, first=True):
+    # type: (Union[List[str], Tuple[Node, ...], str], Optional[bool]) -> str
+
     assert isinstance(marker, (list, tuple, string_types))
 
     # Sometimes we have a structure like [[...]] which is a single item list
@@ -172,10 +188,11 @@
     "!=": operator.ne,
     ">=": operator.ge,
     ">": operator.gt,
-}
+}  # type: Dict[str, Operator]
 
 
 def _eval_op(lhs, op, rhs):
+    # type: (str, Op, str) -> bool
     try:
         spec = Specifier("".join([op.serialize(), rhs]))
     except InvalidSpecifier:
@@ -183,7 +200,7 @@
     else:
         return spec.contains(lhs)
 
-    oper = _operators.get(op.serialize())
+    oper = _operators.get(op.serialize())  # type: Optional[Operator]
     if oper is None:
         raise UndefinedComparison(
             "Undefined {0!r} on {1!r} and {2!r}.".format(op, lhs, rhs)
@@ -192,13 +209,18 @@
     return oper(lhs, rhs)
 
 
-_undefined = object()
+class Undefined(object):
+    pass
+
+
+_undefined = Undefined()
 
 
 def _get_env(environment, name):
-    value = environment.get(name, _undefined)
+    # type: (Dict[str, str], str) -> str
+    value = environment.get(name, _undefined)  # type: Union[str, Undefined]
 
-    if value is _undefined:
+    if isinstance(value, Undefined):
         raise UndefinedEnvironmentName(
             "{0!r} does not exist in evaluation environment.".format(name)
         )
@@ -207,7 +229,8 @@
 
 
 def _evaluate_markers(markers, environment):
-    groups = [[]]
+    # type: (List[Any], Dict[str, str]) -> bool
+    groups = [[]]  # type: List[List[bool]]
 
     for marker in markers:
         assert isinstance(marker, (list, tuple, string_types))
@@ -234,6 +257,7 @@
 
 
 def format_full_version(info):
+    # type: (sys._version_info) -> str
     version = "{0.major}.{0.minor}.{0.micro}".format(info)
     kind = info.releaselevel
     if kind != "final":
@@ -242,9 +266,13 @@
 
 
 def default_environment():
+    # type: () -> Dict[str, str]
     if hasattr(sys, "implementation"):
-        iver = format_full_version(sys.implementation.version)
-        implementation_name = sys.implementation.name
+        # Ignoring the `sys.implementation` reference for type checking due to
+        # mypy not liking that the attribute doesn't exist in Python 2.7 when
+        # run with the `--py27` flag.
+        iver = format_full_version(sys.implementation.version)  # type: ignore
+        implementation_name = sys.implementation.name  # type: ignore
     else:
         iver = "0"
         implementation_name = ""
@@ -266,6 +294,7 @@
 
 class Marker(object):
     def __init__(self, marker):
+        # type: (str) -> None
         try:
             self._markers = _coerce_parse_result(MARKER.parseString(marker))
         except ParseException as e:
@@ -275,12 +304,15 @@
             raise InvalidMarker(err_str)
 
     def __str__(self):
+        # type: () -> str
         return _format_marker(self._markers)
 
     def __repr__(self):
+        # type: () -> str
         return "<Marker({0!r})>".format(str(self))
 
     def evaluate(self, environment=None):
+        # type: (Optional[Dict[str, str]]) -> bool
         """Evaluate a marker.
 
         Return the boolean from evaluating the given marker against the
Index: env/lib/python3.8/site-packages/setuptools/_vendor/packaging/requirements.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import absolute_import, division, print_function\n\nimport string\nimport re\n\nfrom setuptools.extern.pyparsing import stringStart, stringEnd, originalTextFor, ParseException\nfrom setuptools.extern.pyparsing import ZeroOrMore, Word, Optional, Regex, Combine\nfrom setuptools.extern.pyparsing import Literal as L  # noqa\nfrom setuptools.extern.six.moves.urllib import parse as urlparse\n\nfrom .markers import MARKER_EXPR, Marker\nfrom .specifiers import LegacySpecifier, Specifier, SpecifierSet\n\n\nclass InvalidRequirement(ValueError):\n    \"\"\"\n    An invalid requirement was found, users should refer to PEP 508.\n    \"\"\"\n\n\nALPHANUM = Word(string.ascii_letters + string.digits)\n\nLBRACKET = L(\"[\").suppress()\nRBRACKET = L(\"]\").suppress()\nLPAREN = L(\"(\").suppress()\nRPAREN = L(\")\").suppress()\nCOMMA = L(\",\").suppress()\nSEMICOLON = L(\";\").suppress()\nAT = L(\"@\").suppress()\n\nPUNCTUATION = Word(\"-_.\")\nIDENTIFIER_END = ALPHANUM | (ZeroOrMore(PUNCTUATION) + ALPHANUM)\nIDENTIFIER = Combine(ALPHANUM + ZeroOrMore(IDENTIFIER_END))\n\nNAME = IDENTIFIER(\"name\")\nEXTRA = IDENTIFIER\n\nURI = Regex(r\"[^ ]+\")(\"url\")\nURL = AT + URI\n\nEXTRAS_LIST = EXTRA + ZeroOrMore(COMMA + EXTRA)\nEXTRAS = (LBRACKET + Optional(EXTRAS_LIST) + RBRACKET)(\"extras\")\n\nVERSION_PEP440 = Regex(Specifier._regex_str, re.VERBOSE | re.IGNORECASE)\nVERSION_LEGACY = Regex(LegacySpecifier._regex_str, re.VERBOSE | re.IGNORECASE)\n\nVERSION_ONE = VERSION_PEP440 ^ VERSION_LEGACY\nVERSION_MANY = Combine(\n    VERSION_ONE + ZeroOrMore(COMMA + VERSION_ONE), joinString=\",\", adjacent=False\n)(\"_raw_spec\")\n_VERSION_SPEC = Optional(((LPAREN + VERSION_MANY + RPAREN) | VERSION_MANY))\n_VERSION_SPEC.setParseAction(lambda s, l, t: t._raw_spec or \"\")\n\nVERSION_SPEC = originalTextFor(_VERSION_SPEC)(\"specifier\")\nVERSION_SPEC.setParseAction(lambda s, l, t: t[1])\n\nMARKER_EXPR = originalTextFor(MARKER_EXPR())(\"marker\")\nMARKER_EXPR.setParseAction(\n    lambda s, l, t: Marker(s[t._original_start : t._original_end])\n)\nMARKER_SEPARATOR = SEMICOLON\nMARKER = MARKER_SEPARATOR + MARKER_EXPR\n\nVERSION_AND_MARKER = VERSION_SPEC + Optional(MARKER)\nURL_AND_MARKER = URL + Optional(MARKER)\n\nNAMED_REQUIREMENT = NAME + Optional(EXTRAS) + (URL_AND_MARKER | VERSION_AND_MARKER)\n\nREQUIREMENT = stringStart + NAMED_REQUIREMENT + stringEnd\n# setuptools.extern.pyparsing isn't thread safe during initialization, so we do it eagerly, see\n# issue #104\nREQUIREMENT.parseString(\"x[]\")\n\n\nclass Requirement(object):\n    \"\"\"Parse a requirement.\n\n    Parse a given requirement string into its parts, such as name, specifier,\n    URL, and extras. Raises InvalidRequirement on a badly-formed requirement\n    string.\n    \"\"\"\n\n    # TODO: Can we test whether something is contained within a requirement?\n    #       If so how do we do that? Do we need to test against the _name_ of\n    #       the thing as well as the version? What about the markers?\n    # TODO: Can we normalize the name and extra name?\n\n    def __init__(self, requirement_string):\n        try:\n            req = REQUIREMENT.parseString(requirement_string)\n        except ParseException as e:\n            raise InvalidRequirement(\n                'Parse error at \"{0!r}\": {1}'.format(\n                    requirement_string[e.loc : e.loc + 8], e.msg\n                )\n            )\n\n        self.name = req.name\n        if req.url:\n            parsed_url = urlparse.urlparse(req.url)\n            if parsed_url.scheme == \"file\":\n                if urlparse.urlunparse(parsed_url) != req.url:\n                    raise InvalidRequirement(\"Invalid URL given\")\n            elif not (parsed_url.scheme and parsed_url.netloc) or (\n                not parsed_url.scheme and not parsed_url.netloc\n            ):\n                raise InvalidRequirement(\"Invalid URL: {0}\".format(req.url))\n            self.url = req.url\n        else:\n            self.url = None\n        self.extras = set(req.extras.asList() if req.extras else [])\n        self.specifier = SpecifierSet(req.specifier)\n        self.marker = req.marker if req.marker else None\n\n    def __str__(self):\n        parts = [self.name]\n\n        if self.extras:\n            parts.append(\"[{0}]\".format(\",\".join(sorted(self.extras))))\n\n        if self.specifier:\n            parts.append(str(self.specifier))\n\n        if self.url:\n            parts.append(\"@ {0}\".format(self.url))\n            if self.marker:\n                parts.append(\" \")\n\n        if self.marker:\n            parts.append(\"; {0}\".format(self.marker))\n\n        return \"\".join(parts)\n\n    def __repr__(self):\n        return \"<Requirement({0!r})>\".format(str(self))\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/_vendor/packaging/requirements.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/_vendor/packaging/requirements.py	(date 1602088701617)
@@ -9,11 +9,15 @@
 from setuptools.extern.pyparsing import stringStart, stringEnd, originalTextFor, ParseException
 from setuptools.extern.pyparsing import ZeroOrMore, Word, Optional, Regex, Combine
 from setuptools.extern.pyparsing import Literal as L  # noqa
-from setuptools.extern.six.moves.urllib import parse as urlparse
+from urllib import parse as urlparse
 
+from ._typing import TYPE_CHECKING
 from .markers import MARKER_EXPR, Marker
 from .specifiers import LegacySpecifier, Specifier, SpecifierSet
 
+if TYPE_CHECKING:  # pragma: no cover
+    from typing import List
+
 
 class InvalidRequirement(ValueError):
     """
@@ -89,6 +93,7 @@
     # TODO: Can we normalize the name and extra name?
 
     def __init__(self, requirement_string):
+        # type: (str) -> None
         try:
             req = REQUIREMENT.parseString(requirement_string)
         except ParseException as e:
@@ -116,7 +121,8 @@
         self.marker = req.marker if req.marker else None
 
     def __str__(self):
-        parts = [self.name]
+        # type: () -> str
+        parts = [self.name]  # type: List[str]
 
         if self.extras:
             parts.append("[{0}]".format(",".join(sorted(self.extras))))
@@ -135,4 +141,5 @@
         return "".join(parts)
 
     def __repr__(self):
+        # type: () -> str
         return "<Requirement({0!r})>".format(str(self))
Index: env/lib/python3.8/site-packages/setuptools/_vendor/packaging/_compat.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import absolute_import, division, print_function\n\nimport sys\n\n\nPY2 = sys.version_info[0] == 2\nPY3 = sys.version_info[0] == 3\n\n# flake8: noqa\n\nif PY3:\n    string_types = (str,)\nelse:\n    string_types = (basestring,)\n\n\ndef with_metaclass(meta, *bases):\n    \"\"\"\n    Create a base class with a metaclass.\n    \"\"\"\n    # This requires a bit of explanation: the basic idea is to make a dummy\n    # metaclass for one level of class instantiation that replaces itself with\n    # the actual metaclass.\n    class metaclass(meta):\n        def __new__(cls, name, this_bases, d):\n            return meta(name, bases, d)\n\n    return type.__new__(metaclass, \"temporary_class\", (), {})\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/_vendor/packaging/_compat.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/_vendor/packaging/_compat.py	(date 1602088701617)
@@ -5,6 +5,11 @@
 
 import sys
 
+from ._typing import TYPE_CHECKING
+
+if TYPE_CHECKING:  # pragma: no cover
+    from typing import Any, Dict, Tuple, Type
+
 
 PY2 = sys.version_info[0] == 2
 PY3 = sys.version_info[0] == 3
@@ -18,14 +23,16 @@
 
 
 def with_metaclass(meta, *bases):
+    # type: (Type[Any], Tuple[Type[Any], ...]) -> Any
     """
     Create a base class with a metaclass.
     """
     # This requires a bit of explanation: the basic idea is to make a dummy
     # metaclass for one level of class instantiation that replaces itself with
     # the actual metaclass.
-    class metaclass(meta):
+    class metaclass(meta):  # type: ignore
         def __new__(cls, name, this_bases, d):
+            # type: (Type[Any], str, Tuple[Any], Dict[Any, Any]) -> Any
             return meta(name, bases, d)
 
     return type.__new__(metaclass, "temporary_class", (), {})
Index: env/lib/python3.8/site-packages/setuptools/wheel.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"Wheels support.\"\"\"\n\nfrom distutils.util import get_platform\nfrom distutils import log\nimport email\nimport itertools\nimport os\nimport posixpath\nimport re\nimport zipfile\n\nimport pkg_resources\nimport setuptools\nfrom pkg_resources import parse_version\nfrom setuptools.extern.packaging.tags import sys_tags\nfrom setuptools.extern.packaging.utils import canonicalize_name\nfrom setuptools.extern.six import PY3\nfrom setuptools.command.egg_info import write_requirements\n\n\n__metaclass__ = type\n\n\nWHEEL_NAME = re.compile(\n    r\"\"\"^(?P<project_name>.+?)-(?P<version>\\d.*?)\n    ((-(?P<build>\\d.*?))?-(?P<py_version>.+?)-(?P<abi>.+?)-(?P<platform>.+?)\n    )\\.whl$\"\"\",\n    re.VERBOSE).match\n\nNAMESPACE_PACKAGE_INIT = \\\n    \"__import__('pkg_resources').declare_namespace(__name__)\\n\"\n\n\ndef unpack(src_dir, dst_dir):\n    '''Move everything under `src_dir` to `dst_dir`, and delete the former.'''\n    for dirpath, dirnames, filenames in os.walk(src_dir):\n        subdir = os.path.relpath(dirpath, src_dir)\n        for f in filenames:\n            src = os.path.join(dirpath, f)\n            dst = os.path.join(dst_dir, subdir, f)\n            os.renames(src, dst)\n        for n, d in reversed(list(enumerate(dirnames))):\n            src = os.path.join(dirpath, d)\n            dst = os.path.join(dst_dir, subdir, d)\n            if not os.path.exists(dst):\n                # Directory does not exist in destination,\n                # rename it and prune it from os.walk list.\n                os.renames(src, dst)\n                del dirnames[n]\n    # Cleanup.\n    for dirpath, dirnames, filenames in os.walk(src_dir, topdown=True):\n        assert not filenames\n        os.rmdir(dirpath)\n\n\nclass Wheel:\n\n    def __init__(self, filename):\n        match = WHEEL_NAME(os.path.basename(filename))\n        if match is None:\n            raise ValueError('invalid wheel name: %r' % filename)\n        self.filename = filename\n        for k, v in match.groupdict().items():\n            setattr(self, k, v)\n\n    def tags(self):\n        '''List tags (py_version, abi, platform) supported by this wheel.'''\n        return itertools.product(\n            self.py_version.split('.'),\n            self.abi.split('.'),\n            self.platform.split('.'),\n        )\n\n    def is_compatible(self):\n        '''Is the wheel is compatible with the current platform?'''\n        supported_tags = set(\n            (t.interpreter, t.abi, t.platform) for t in sys_tags())\n        return next((True for t in self.tags() if t in supported_tags), False)\n\n    def egg_name(self):\n        return pkg_resources.Distribution(\n            project_name=self.project_name, version=self.version,\n            platform=(None if self.platform == 'any' else get_platform()),\n        ).egg_name() + '.egg'\n\n    def get_dist_info(self, zf):\n        # find the correct name of the .dist-info dir in the wheel file\n        for member in zf.namelist():\n            dirname = posixpath.dirname(member)\n            if (dirname.endswith('.dist-info') and\n                    canonicalize_name(dirname).startswith(\n                        canonicalize_name(self.project_name))):\n                return dirname\n        raise ValueError(\"unsupported wheel format. .dist-info not found\")\n\n    def install_as_egg(self, destination_eggdir):\n        '''Install wheel as an egg directory.'''\n        with zipfile.ZipFile(self.filename) as zf:\n            self._install_as_egg(destination_eggdir, zf)\n\n    def _install_as_egg(self, destination_eggdir, zf):\n        dist_basename = '%s-%s' % (self.project_name, self.version)\n        dist_info = self.get_dist_info(zf)\n        dist_data = '%s.data' % dist_basename\n        egg_info = os.path.join(destination_eggdir, 'EGG-INFO')\n\n        self._convert_metadata(zf, destination_eggdir, dist_info, egg_info)\n        self._move_data_entries(destination_eggdir, dist_data)\n        self._fix_namespace_packages(egg_info, destination_eggdir)\n\n    @staticmethod\n    def _convert_metadata(zf, destination_eggdir, dist_info, egg_info):\n        def get_metadata(name):\n            with zf.open(posixpath.join(dist_info, name)) as fp:\n                value = fp.read().decode('utf-8') if PY3 else fp.read()\n                return email.parser.Parser().parsestr(value)\n\n        wheel_metadata = get_metadata('WHEEL')\n        # Check wheel format version is supported.\n        wheel_version = parse_version(wheel_metadata.get('Wheel-Version'))\n        wheel_v1 = (\n            parse_version('1.0') <= wheel_version < parse_version('2.0dev0')\n        )\n        if not wheel_v1:\n            raise ValueError(\n                'unsupported wheel format version: %s' % wheel_version)\n        # Extract to target directory.\n        os.mkdir(destination_eggdir)\n        zf.extractall(destination_eggdir)\n        # Convert metadata.\n        dist_info = os.path.join(destination_eggdir, dist_info)\n        dist = pkg_resources.Distribution.from_location(\n            destination_eggdir, dist_info,\n            metadata=pkg_resources.PathMetadata(destination_eggdir, dist_info),\n        )\n\n        # Note: Evaluate and strip markers now,\n        # as it's difficult to convert back from the syntax:\n        # foobar; \"linux\" in sys_platform and extra == 'test'\n        def raw_req(req):\n            req.marker = None\n            return str(req)\n        install_requires = list(sorted(map(raw_req, dist.requires())))\n        extras_require = {\n            extra: sorted(\n                req\n                for req in map(raw_req, dist.requires((extra,)))\n                if req not in install_requires\n            )\n            for extra in dist.extras\n        }\n        os.rename(dist_info, egg_info)\n        os.rename(\n            os.path.join(egg_info, 'METADATA'),\n            os.path.join(egg_info, 'PKG-INFO'),\n        )\n        setup_dist = setuptools.Distribution(\n            attrs=dict(\n                install_requires=install_requires,\n                extras_require=extras_require,\n            ),\n        )\n        # Temporarily disable info traces.\n        log_threshold = log._global_log.threshold\n        log.set_threshold(log.WARN)\n        try:\n            write_requirements(\n                setup_dist.get_command_obj('egg_info'),\n                None,\n                os.path.join(egg_info, 'requires.txt'),\n            )\n        finally:\n            log.set_threshold(log_threshold)\n\n    @staticmethod\n    def _move_data_entries(destination_eggdir, dist_data):\n        \"\"\"Move data entries to their correct location.\"\"\"\n        dist_data = os.path.join(destination_eggdir, dist_data)\n        dist_data_scripts = os.path.join(dist_data, 'scripts')\n        if os.path.exists(dist_data_scripts):\n            egg_info_scripts = os.path.join(\n                destination_eggdir, 'EGG-INFO', 'scripts')\n            os.mkdir(egg_info_scripts)\n            for entry in os.listdir(dist_data_scripts):\n                # Remove bytecode, as it's not properly handled\n                # during easy_install scripts install phase.\n                if entry.endswith('.pyc'):\n                    os.unlink(os.path.join(dist_data_scripts, entry))\n                else:\n                    os.rename(\n                        os.path.join(dist_data_scripts, entry),\n                        os.path.join(egg_info_scripts, entry),\n                    )\n            os.rmdir(dist_data_scripts)\n        for subdir in filter(os.path.exists, (\n            os.path.join(dist_data, d)\n            for d in ('data', 'headers', 'purelib', 'platlib')\n        )):\n            unpack(subdir, destination_eggdir)\n        if os.path.exists(dist_data):\n            os.rmdir(dist_data)\n\n    @staticmethod\n    def _fix_namespace_packages(egg_info, destination_eggdir):\n        namespace_packages = os.path.join(\n            egg_info, 'namespace_packages.txt')\n        if os.path.exists(namespace_packages):\n            with open(namespace_packages) as fp:\n                namespace_packages = fp.read().split()\n            for mod in namespace_packages:\n                mod_dir = os.path.join(destination_eggdir, *mod.split('.'))\n                mod_init = os.path.join(mod_dir, '__init__.py')\n                if not os.path.exists(mod_dir):\n                    os.mkdir(mod_dir)\n                if not os.path.exists(mod_init):\n                    with open(mod_init, 'w') as fp:\n                        fp.write(NAMESPACE_PACKAGE_INIT)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/wheel.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/wheel.py	(date 1602088701601)
@@ -14,13 +14,9 @@
 from pkg_resources import parse_version
 from setuptools.extern.packaging.tags import sys_tags
 from setuptools.extern.packaging.utils import canonicalize_name
-from setuptools.extern.six import PY3
 from setuptools.command.egg_info import write_requirements
 
 
-__metaclass__ = type
-
-
 WHEEL_NAME = re.compile(
     r"""^(?P<project_name>.+?)-(?P<version>\d.*?)
     ((-(?P<build>\d.*?))?-(?P<py_version>.+?)-(?P<abi>.+?)-(?P<platform>.+?)
@@ -112,7 +108,7 @@
     def _convert_metadata(zf, destination_eggdir, dist_info, egg_info):
         def get_metadata(name):
             with zf.open(posixpath.join(dist_info, name)) as fp:
-                value = fp.read().decode('utf-8') if PY3 else fp.read()
+                value = fp.read().decode('utf-8')
                 return email.parser.Parser().parsestr(value)
 
         wheel_metadata = get_metadata('WHEEL')
Index: env/lib/python3.8/site-packages/setuptools/_vendor/packaging/utils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import absolute_import, division, print_function\n\nimport re\n\nfrom .version import InvalidVersion, Version\n\n\n_canonicalize_regex = re.compile(r\"[-_.]+\")\n\n\ndef canonicalize_name(name):\n    # This is taken from PEP 503.\n    return _canonicalize_regex.sub(\"-\", name).lower()\n\n\ndef canonicalize_version(version):\n    \"\"\"\n    This is very similar to Version.__str__, but has one subtle differences\n    with the way it handles the release segment.\n    \"\"\"\n\n    try:\n        version = Version(version)\n    except InvalidVersion:\n        # Legacy versions cannot be normalized\n        return version\n\n    parts = []\n\n    # Epoch\n    if version.epoch != 0:\n        parts.append(\"{0}!\".format(version.epoch))\n\n    # Release segment\n    # NB: This strips trailing '.0's to normalize\n    parts.append(re.sub(r\"(\\.0)+$\", \"\", \".\".join(str(x) for x in version.release)))\n\n    # Pre-release\n    if version.pre is not None:\n        parts.append(\"\".join(str(x) for x in version.pre))\n\n    # Post-release\n    if version.post is not None:\n        parts.append(\".post{0}\".format(version.post))\n\n    # Development release\n    if version.dev is not None:\n        parts.append(\".dev{0}\".format(version.dev))\n\n    # Local version segment\n    if version.local is not None:\n        parts.append(\"+{0}\".format(version.local))\n\n    return \"\".join(parts)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/_vendor/packaging/utils.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/_vendor/packaging/utils.py	(date 1602088701617)
@@ -5,28 +5,36 @@
 
 import re
 
+from ._typing import TYPE_CHECKING, cast
 from .version import InvalidVersion, Version
 
+if TYPE_CHECKING:  # pragma: no cover
+    from typing import NewType, Union
+
+    NormalizedName = NewType("NormalizedName", str)
 
 _canonicalize_regex = re.compile(r"[-_.]+")
 
 
 def canonicalize_name(name):
+    # type: (str) -> NormalizedName
     # This is taken from PEP 503.
-    return _canonicalize_regex.sub("-", name).lower()
+    value = _canonicalize_regex.sub("-", name).lower()
+    return cast("NormalizedName", value)
 
 
-def canonicalize_version(version):
+def canonicalize_version(_version):
+    # type: (str) -> Union[Version, str]
     """
-    This is very similar to Version.__str__, but has one subtle differences
+    This is very similar to Version.__str__, but has one subtle difference
     with the way it handles the release segment.
     """
 
     try:
-        version = Version(version)
+        version = Version(_version)
     except InvalidVersion:
         # Legacy versions cannot be normalized
-        return version
+        return _version
 
     parts = []
 
Index: env/lib/python3.8/site-packages/setuptools/_vendor/packaging/__about__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import absolute_import, division, print_function\n\n__all__ = [\n    \"__title__\",\n    \"__summary__\",\n    \"__uri__\",\n    \"__version__\",\n    \"__author__\",\n    \"__email__\",\n    \"__license__\",\n    \"__copyright__\",\n]\n\n__title__ = \"packaging\"\n__summary__ = \"Core utilities for Python packages\"\n__uri__ = \"https://github.com/pypa/packaging\"\n\n__version__ = \"19.2\"\n\n__author__ = \"Donald Stufft and individual contributors\"\n__email__ = \"donald@stufft.io\"\n\n__license__ = \"BSD or Apache License, Version 2.0\"\n__copyright__ = \"Copyright 2014-2019 %s\" % __author__\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/_vendor/packaging/__about__.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/_vendor/packaging/__about__.py	(date 1602088701617)
@@ -18,10 +18,10 @@
 __summary__ = "Core utilities for Python packages"
 __uri__ = "https://github.com/pypa/packaging"
 
-__version__ = "19.2"
+__version__ = "20.4"
 
 __author__ = "Donald Stufft and individual contributors"
 __email__ = "donald@stufft.io"
 
-__license__ = "BSD or Apache License, Version 2.0"
+__license__ = "BSD-2-Clause or Apache-2.0"
 __copyright__ = "Copyright 2014-2019 %s" % __author__
Index: env/lib/python3.8/site-packages/setuptools/unicode_utils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import unicodedata\nimport sys\n\nfrom setuptools.extern import six\n\n\n# HFS Plus uses decomposed UTF-8\ndef decompose(path):\n    if isinstance(path, six.text_type):\n        return unicodedata.normalize('NFD', path)\n    try:\n        path = path.decode('utf-8')\n        path = unicodedata.normalize('NFD', path)\n        path = path.encode('utf-8')\n    except UnicodeError:\n        pass  # Not UTF-8\n    return path\n\n\ndef filesys_decode(path):\n    \"\"\"\n    Ensure that the given path is decoded,\n    NONE when no expected encoding works\n    \"\"\"\n\n    if isinstance(path, six.text_type):\n        return path\n\n    fs_enc = sys.getfilesystemencoding() or 'utf-8'\n    candidates = fs_enc, 'utf-8'\n\n    for enc in candidates:\n        try:\n            return path.decode(enc)\n        except UnicodeDecodeError:\n            continue\n\n\ndef try_encode(string, enc):\n    \"turn unicode encoding into a functional routine\"\n    try:\n        return string.encode(enc)\n    except UnicodeEncodeError:\n        return None\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/unicode_utils.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/unicode_utils.py	(date 1602088701601)
@@ -1,12 +1,10 @@
 import unicodedata
 import sys
 
-from setuptools.extern import six
-
 
 # HFS Plus uses decomposed UTF-8
 def decompose(path):
-    if isinstance(path, six.text_type):
+    if isinstance(path, str):
         return unicodedata.normalize('NFD', path)
     try:
         path = path.decode('utf-8')
@@ -23,7 +21,7 @@
     NONE when no expected encoding works
     """
 
-    if isinstance(path, six.text_type):
+    if isinstance(path, str):
         return path
 
     fs_enc = sys.getfilesystemencoding() or 'utf-8'
Index: env/lib/python3.8/site-packages/setuptools/_vendor/packaging/tags.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\n\nfrom __future__ import absolute_import\n\nimport distutils.util\n\ntry:\n    from importlib.machinery import EXTENSION_SUFFIXES\nexcept ImportError:  # pragma: no cover\n    import imp\n\n    EXTENSION_SUFFIXES = [x[0] for x in imp.get_suffixes()]\n    del imp\nimport platform\nimport re\nimport sys\nimport sysconfig\nimport warnings\n\n\nINTERPRETER_SHORT_NAMES = {\n    \"python\": \"py\",  # Generic.\n    \"cpython\": \"cp\",\n    \"pypy\": \"pp\",\n    \"ironpython\": \"ip\",\n    \"jython\": \"jy\",\n}\n\n\n_32_BIT_INTERPRETER = sys.maxsize <= 2 ** 32\n\n\nclass Tag(object):\n\n    __slots__ = [\"_interpreter\", \"_abi\", \"_platform\"]\n\n    def __init__(self, interpreter, abi, platform):\n        self._interpreter = interpreter.lower()\n        self._abi = abi.lower()\n        self._platform = platform.lower()\n\n    @property\n    def interpreter(self):\n        return self._interpreter\n\n    @property\n    def abi(self):\n        return self._abi\n\n    @property\n    def platform(self):\n        return self._platform\n\n    def __eq__(self, other):\n        return (\n            (self.platform == other.platform)\n            and (self.abi == other.abi)\n            and (self.interpreter == other.interpreter)\n        )\n\n    def __hash__(self):\n        return hash((self._interpreter, self._abi, self._platform))\n\n    def __str__(self):\n        return \"{}-{}-{}\".format(self._interpreter, self._abi, self._platform)\n\n    def __repr__(self):\n        return \"<{self} @ {self_id}>\".format(self=self, self_id=id(self))\n\n\ndef parse_tag(tag):\n    tags = set()\n    interpreters, abis, platforms = tag.split(\"-\")\n    for interpreter in interpreters.split(\".\"):\n        for abi in abis.split(\".\"):\n            for platform_ in platforms.split(\".\"):\n                tags.add(Tag(interpreter, abi, platform_))\n    return frozenset(tags)\n\n\ndef _normalize_string(string):\n    return string.replace(\".\", \"_\").replace(\"-\", \"_\")\n\n\ndef _cpython_interpreter(py_version):\n    # TODO: Is using py_version_nodot for interpreter version critical?\n    return \"cp{major}{minor}\".format(major=py_version[0], minor=py_version[1])\n\n\ndef _cpython_abis(py_version):\n    abis = []\n    version = \"{}{}\".format(*py_version[:2])\n    debug = pymalloc = ucs4 = \"\"\n    with_debug = sysconfig.get_config_var(\"Py_DEBUG\")\n    has_refcount = hasattr(sys, \"gettotalrefcount\")\n    # Windows doesn't set Py_DEBUG, so checking for support of debug-compiled\n    # extension modules is the best option.\n    # https://github.com/pypa/pip/issues/3383#issuecomment-173267692\n    has_ext = \"_d.pyd\" in EXTENSION_SUFFIXES\n    if with_debug or (with_debug is None and (has_refcount or has_ext)):\n        debug = \"d\"\n    if py_version < (3, 8):\n        with_pymalloc = sysconfig.get_config_var(\"WITH_PYMALLOC\")\n        if with_pymalloc or with_pymalloc is None:\n            pymalloc = \"m\"\n        if py_version < (3, 3):\n            unicode_size = sysconfig.get_config_var(\"Py_UNICODE_SIZE\")\n            if unicode_size == 4 or (\n                unicode_size is None and sys.maxunicode == 0x10FFFF\n            ):\n                ucs4 = \"u\"\n    elif debug:\n        # Debug builds can also load \"normal\" extension modules.\n        # We can also assume no UCS-4 or pymalloc requirement.\n        abis.append(\"cp{version}\".format(version=version))\n    abis.insert(\n        0,\n        \"cp{version}{debug}{pymalloc}{ucs4}\".format(\n            version=version, debug=debug, pymalloc=pymalloc, ucs4=ucs4\n        ),\n    )\n    return abis\n\n\ndef _cpython_tags(py_version, interpreter, abis, platforms):\n    for abi in abis:\n        for platform_ in platforms:\n            yield Tag(interpreter, abi, platform_)\n    for tag in (Tag(interpreter, \"abi3\", platform_) for platform_ in platforms):\n        yield tag\n    for tag in (Tag(interpreter, \"none\", platform_) for platform_ in platforms):\n        yield tag\n    # PEP 384 was first implemented in Python 3.2.\n    for minor_version in range(py_version[1] - 1, 1, -1):\n        for platform_ in platforms:\n            interpreter = \"cp{major}{minor}\".format(\n                major=py_version[0], minor=minor_version\n            )\n            yield Tag(interpreter, \"abi3\", platform_)\n\n\ndef _pypy_interpreter():\n    return \"pp{py_major}{pypy_major}{pypy_minor}\".format(\n        py_major=sys.version_info[0],\n        pypy_major=sys.pypy_version_info.major,\n        pypy_minor=sys.pypy_version_info.minor,\n    )\n\n\ndef _generic_abi():\n    abi = sysconfig.get_config_var(\"SOABI\")\n    if abi:\n        return _normalize_string(abi)\n    else:\n        return \"none\"\n\n\ndef _pypy_tags(py_version, interpreter, abi, platforms):\n    for tag in (Tag(interpreter, abi, platform) for platform in platforms):\n        yield tag\n    for tag in (Tag(interpreter, \"none\", platform) for platform in platforms):\n        yield tag\n\n\ndef _generic_tags(interpreter, py_version, abi, platforms):\n    for tag in (Tag(interpreter, abi, platform) for platform in platforms):\n        yield tag\n    if abi != \"none\":\n        tags = (Tag(interpreter, \"none\", platform_) for platform_ in platforms)\n        for tag in tags:\n            yield tag\n\n\ndef _py_interpreter_range(py_version):\n    \"\"\"\n    Yield Python versions in descending order.\n\n    After the latest version, the major-only version will be yielded, and then\n    all following versions up to 'end'.\n    \"\"\"\n    yield \"py{major}{minor}\".format(major=py_version[0], minor=py_version[1])\n    yield \"py{major}\".format(major=py_version[0])\n    for minor in range(py_version[1] - 1, -1, -1):\n        yield \"py{major}{minor}\".format(major=py_version[0], minor=minor)\n\n\ndef _independent_tags(interpreter, py_version, platforms):\n    \"\"\"\n    Return the sequence of tags that are consistent across implementations.\n\n    The tags consist of:\n    - py*-none-<platform>\n    - <interpreter>-none-any\n    - py*-none-any\n    \"\"\"\n    for version in _py_interpreter_range(py_version):\n        for platform_ in platforms:\n            yield Tag(version, \"none\", platform_)\n    yield Tag(interpreter, \"none\", \"any\")\n    for version in _py_interpreter_range(py_version):\n        yield Tag(version, \"none\", \"any\")\n\n\ndef _mac_arch(arch, is_32bit=_32_BIT_INTERPRETER):\n    if not is_32bit:\n        return arch\n\n    if arch.startswith(\"ppc\"):\n        return \"ppc\"\n\n    return \"i386\"\n\n\ndef _mac_binary_formats(version, cpu_arch):\n    formats = [cpu_arch]\n    if cpu_arch == \"x86_64\":\n        if version < (10, 4):\n            return []\n        formats.extend([\"intel\", \"fat64\", \"fat32\"])\n\n    elif cpu_arch == \"i386\":\n        if version < (10, 4):\n            return []\n        formats.extend([\"intel\", \"fat32\", \"fat\"])\n\n    elif cpu_arch == \"ppc64\":\n        # TODO: Need to care about 32-bit PPC for ppc64 through 10.2?\n        if version > (10, 5) or version < (10, 4):\n            return []\n        formats.append(\"fat64\")\n\n    elif cpu_arch == \"ppc\":\n        if version > (10, 6):\n            return []\n        formats.extend([\"fat32\", \"fat\"])\n\n    formats.append(\"universal\")\n    return formats\n\n\ndef _mac_platforms(version=None, arch=None):\n    version_str, _, cpu_arch = platform.mac_ver()\n    if version is None:\n        version = tuple(map(int, version_str.split(\".\")[:2]))\n    if arch is None:\n        arch = _mac_arch(cpu_arch)\n    platforms = []\n    for minor_version in range(version[1], -1, -1):\n        compat_version = version[0], minor_version\n        binary_formats = _mac_binary_formats(compat_version, arch)\n        for binary_format in binary_formats:\n            platforms.append(\n                \"macosx_{major}_{minor}_{binary_format}\".format(\n                    major=compat_version[0],\n                    minor=compat_version[1],\n                    binary_format=binary_format,\n                )\n            )\n    return platforms\n\n\n# From PEP 513.\ndef _is_manylinux_compatible(name, glibc_version):\n    # Check for presence of _manylinux module.\n    try:\n        import _manylinux\n\n        return bool(getattr(_manylinux, name + \"_compatible\"))\n    except (ImportError, AttributeError):\n        # Fall through to heuristic check below.\n        pass\n\n    return _have_compatible_glibc(*glibc_version)\n\n\ndef _glibc_version_string():\n    # Returns glibc version string, or None if not using glibc.\n    import ctypes\n\n    # ctypes.CDLL(None) internally calls dlopen(NULL), and as the dlopen\n    # manpage says, \"If filename is NULL, then the returned handle is for the\n    # main program\". This way we can let the linker do the work to figure out\n    # which libc our process is actually using.\n    process_namespace = ctypes.CDLL(None)\n    try:\n        gnu_get_libc_version = process_namespace.gnu_get_libc_version\n    except AttributeError:\n        # Symbol doesn't exist -> therefore, we are not linked to\n        # glibc.\n        return None\n\n    # Call gnu_get_libc_version, which returns a string like \"2.5\"\n    gnu_get_libc_version.restype = ctypes.c_char_p\n    version_str = gnu_get_libc_version()\n    # py2 / py3 compatibility:\n    if not isinstance(version_str, str):\n        version_str = version_str.decode(\"ascii\")\n\n    return version_str\n\n\n# Separated out from have_compatible_glibc for easier unit testing.\ndef _check_glibc_version(version_str, required_major, minimum_minor):\n    # Parse string and check against requested version.\n    #\n    # We use a regexp instead of str.split because we want to discard any\n    # random junk that might come after the minor version -- this might happen\n    # in patched/forked versions of glibc (e.g. Linaro's version of glibc\n    # uses version strings like \"2.20-2014.11\"). See gh-3588.\n    m = re.match(r\"(?P<major>[0-9]+)\\.(?P<minor>[0-9]+)\", version_str)\n    if not m:\n        warnings.warn(\n            \"Expected glibc version with 2 components major.minor,\"\n            \" got: %s\" % version_str,\n            RuntimeWarning,\n        )\n        return False\n    return (\n        int(m.group(\"major\")) == required_major\n        and int(m.group(\"minor\")) >= minimum_minor\n    )\n\n\ndef _have_compatible_glibc(required_major, minimum_minor):\n    version_str = _glibc_version_string()\n    if version_str is None:\n        return False\n    return _check_glibc_version(version_str, required_major, minimum_minor)\n\n\ndef _linux_platforms(is_32bit=_32_BIT_INTERPRETER):\n    linux = _normalize_string(distutils.util.get_platform())\n    if linux == \"linux_x86_64\" and is_32bit:\n        linux = \"linux_i686\"\n    manylinux_support = (\n        (\"manylinux2014\", (2, 17)),  # CentOS 7 w/ glibc 2.17 (PEP 599)\n        (\"manylinux2010\", (2, 12)),  # CentOS 6 w/ glibc 2.12 (PEP 571)\n        (\"manylinux1\", (2, 5)),  # CentOS 5 w/ glibc 2.5 (PEP 513)\n    )\n    manylinux_support_iter = iter(manylinux_support)\n    for name, glibc_version in manylinux_support_iter:\n        if _is_manylinux_compatible(name, glibc_version):\n            platforms = [linux.replace(\"linux\", name)]\n            break\n    else:\n        platforms = []\n    # Support for a later manylinux implies support for an earlier version.\n    platforms += [linux.replace(\"linux\", name) for name, _ in manylinux_support_iter]\n    platforms.append(linux)\n    return platforms\n\n\ndef _generic_platforms():\n    platform = _normalize_string(distutils.util.get_platform())\n    return [platform]\n\n\ndef _interpreter_name():\n    name = platform.python_implementation().lower()\n    return INTERPRETER_SHORT_NAMES.get(name) or name\n\n\ndef _generic_interpreter(name, py_version):\n    version = sysconfig.get_config_var(\"py_version_nodot\")\n    if not version:\n        version = \"\".join(map(str, py_version[:2]))\n    return \"{name}{version}\".format(name=name, version=version)\n\n\ndef sys_tags():\n    \"\"\"\n    Returns the sequence of tag triples for the running interpreter.\n\n    The order of the sequence corresponds to priority order for the\n    interpreter, from most to least important.\n    \"\"\"\n    py_version = sys.version_info[:2]\n    interpreter_name = _interpreter_name()\n    if platform.system() == \"Darwin\":\n        platforms = _mac_platforms()\n    elif platform.system() == \"Linux\":\n        platforms = _linux_platforms()\n    else:\n        platforms = _generic_platforms()\n\n    if interpreter_name == \"cp\":\n        interpreter = _cpython_interpreter(py_version)\n        abis = _cpython_abis(py_version)\n        for tag in _cpython_tags(py_version, interpreter, abis, platforms):\n            yield tag\n    elif interpreter_name == \"pp\":\n        interpreter = _pypy_interpreter()\n        abi = _generic_abi()\n        for tag in _pypy_tags(py_version, interpreter, abi, platforms):\n            yield tag\n    else:\n        interpreter = _generic_interpreter(interpreter_name, py_version)\n        abi = _generic_abi()\n        for tag in _generic_tags(interpreter, py_version, abi, platforms):\n            yield tag\n    for tag in _independent_tags(interpreter, py_version, platforms):\n        yield tag\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/_vendor/packaging/tags.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/_vendor/packaging/tags.py	(date 1602088701617)
@@ -13,12 +13,37 @@
 
     EXTENSION_SUFFIXES = [x[0] for x in imp.get_suffixes()]
     del imp
+import logging
+import os
 import platform
 import re
+import struct
 import sys
 import sysconfig
 import warnings
 
+from ._typing import TYPE_CHECKING, cast
+
+if TYPE_CHECKING:  # pragma: no cover
+    from typing import (
+        Dict,
+        FrozenSet,
+        IO,
+        Iterable,
+        Iterator,
+        List,
+        Optional,
+        Sequence,
+        Tuple,
+        Union,
+    )
+
+    PythonVersion = Sequence[int]
+    MacVersion = Tuple[int, int]
+    GlibcVersion = Tuple[int, int]
+
+
+logger = logging.getLogger(__name__)
 
 INTERPRETER_SHORT_NAMES = {
     "python": "py",  # Generic.
@@ -26,34 +51,48 @@
     "pypy": "pp",
     "ironpython": "ip",
     "jython": "jy",
-}
+}  # type: Dict[str, str]
 
 
 _32_BIT_INTERPRETER = sys.maxsize <= 2 ** 32
 
 
 class Tag(object):
+    """
+    A representation of the tag triple for a wheel.
+
+    Instances are considered immutable and thus are hashable. Equality checking
+    is also supported.
+    """
 
     __slots__ = ["_interpreter", "_abi", "_platform"]
 
     def __init__(self, interpreter, abi, platform):
+        # type: (str, str, str) -> None
         self._interpreter = interpreter.lower()
         self._abi = abi.lower()
         self._platform = platform.lower()
 
     @property
     def interpreter(self):
+        # type: () -> str
         return self._interpreter
 
     @property
     def abi(self):
+        # type: () -> str
         return self._abi
 
     @property
     def platform(self):
+        # type: () -> str
         return self._platform
 
     def __eq__(self, other):
+        # type: (object) -> bool
+        if not isinstance(other, Tag):
+            return NotImplemented
+
         return (
             (self.platform == other.platform)
             and (self.abi == other.abi)
@@ -61,16 +100,26 @@
         )
 
     def __hash__(self):
+        # type: () -> int
         return hash((self._interpreter, self._abi, self._platform))
 
     def __str__(self):
+        # type: () -> str
         return "{}-{}-{}".format(self._interpreter, self._abi, self._platform)
 
     def __repr__(self):
+        # type: () -> str
         return "<{self} @ {self_id}>".format(self=self, self_id=id(self))
 
 
 def parse_tag(tag):
+    # type: (str) -> FrozenSet[Tag]
+    """
+    Parses the provided tag (e.g. `py3-none-any`) into a frozenset of Tag instances.
+
+    Returning a set is required due to the possibility that the tag is a
+    compressed tag set.
+    """
     tags = set()
     interpreters, abis, platforms = tag.split("-")
     for interpreter in interpreters.split("."):
@@ -80,20 +129,54 @@
     return frozenset(tags)
 
 
+def _warn_keyword_parameter(func_name, kwargs):
+    # type: (str, Dict[str, bool]) -> bool
+    """
+    Backwards-compatibility with Python 2.7 to allow treating 'warn' as keyword-only.
+    """
+    if not kwargs:
+        return False
+    elif len(kwargs) > 1 or "warn" not in kwargs:
+        kwargs.pop("warn", None)
+        arg = next(iter(kwargs.keys()))
+        raise TypeError(
+            "{}() got an unexpected keyword argument {!r}".format(func_name, arg)
+        )
+    return kwargs["warn"]
+
+
+def _get_config_var(name, warn=False):
+    # type: (str, bool) -> Union[int, str, None]
+    value = sysconfig.get_config_var(name)
+    if value is None and warn:
+        logger.debug(
+            "Config variable '%s' is unset, Python ABI tag may be incorrect", name
+        )
+    return value
+
+
 def _normalize_string(string):
+    # type: (str) -> str
     return string.replace(".", "_").replace("-", "_")
 
 
-def _cpython_interpreter(py_version):
-    # TODO: Is using py_version_nodot for interpreter version critical?
-    return "cp{major}{minor}".format(major=py_version[0], minor=py_version[1])
+def _abi3_applies(python_version):
+    # type: (PythonVersion) -> bool
+    """
+    Determine if the Python version supports abi3.
 
+    PEP 384 was first implemented in Python 3.2.
+    """
+    return len(python_version) > 1 and tuple(python_version) >= (3, 2)
 
-def _cpython_abis(py_version):
+
+def _cpython_abis(py_version, warn=False):
+    # type: (PythonVersion, bool) -> List[str]
+    py_version = tuple(py_version)  # To allow for version comparison.
     abis = []
-    version = "{}{}".format(*py_version[:2])
+    version = _version_nodot(py_version[:2])
     debug = pymalloc = ucs4 = ""
-    with_debug = sysconfig.get_config_var("Py_DEBUG")
+    with_debug = _get_config_var("Py_DEBUG", warn)
     has_refcount = hasattr(sys, "gettotalrefcount")
     # Windows doesn't set Py_DEBUG, so checking for support of debug-compiled
     # extension modules is the best option.
@@ -102,11 +185,11 @@
     if with_debug or (with_debug is None and (has_refcount or has_ext)):
         debug = "d"
     if py_version < (3, 8):
-        with_pymalloc = sysconfig.get_config_var("WITH_PYMALLOC")
+        with_pymalloc = _get_config_var("WITH_PYMALLOC", warn)
         if with_pymalloc or with_pymalloc is None:
             pymalloc = "m"
         if py_version < (3, 3):
-            unicode_size = sysconfig.get_config_var("Py_UNICODE_SIZE")
+            unicode_size = _get_config_var("Py_UNICODE_SIZE", warn)
             if unicode_size == 4 or (
                 unicode_size is None and sys.maxunicode == 0x10FFFF
             ):
@@ -124,86 +207,148 @@
     return abis
 
 
-def _cpython_tags(py_version, interpreter, abis, platforms):
+def cpython_tags(
+    python_version=None,  # type: Optional[PythonVersion]
+    abis=None,  # type: Optional[Iterable[str]]
+    platforms=None,  # type: Optional[Iterable[str]]
+    **kwargs  # type: bool
+):
+    # type: (...) -> Iterator[Tag]
+    """
+    Yields the tags for a CPython interpreter.
+
+    The tags consist of:
+    - cp<python_version>-<abi>-<platform>
+    - cp<python_version>-abi3-<platform>
+    - cp<python_version>-none-<platform>
+    - cp<less than python_version>-abi3-<platform>  # Older Python versions down to 3.2.
+
+    If python_version only specifies a major version then user-provided ABIs and
+    the 'none' ABItag will be used.
+
+    If 'abi3' or 'none' are specified in 'abis' then they will be yielded at
+    their normal position and not at the beginning.
+    """
+    warn = _warn_keyword_parameter("cpython_tags", kwargs)
+    if not python_version:
+        python_version = sys.version_info[:2]
+
+    interpreter = "cp{}".format(_version_nodot(python_version[:2]))
+
+    if abis is None:
+        if len(python_version) > 1:
+            abis = _cpython_abis(python_version, warn)
+        else:
+            abis = []
+    abis = list(abis)
+    # 'abi3' and 'none' are explicitly handled later.
+    for explicit_abi in ("abi3", "none"):
+        try:
+            abis.remove(explicit_abi)
+        except ValueError:
+            pass
+
+    platforms = list(platforms or _platform_tags())
     for abi in abis:
         for platform_ in platforms:
             yield Tag(interpreter, abi, platform_)
-    for tag in (Tag(interpreter, "abi3", platform_) for platform_ in platforms):
-        yield tag
+    if _abi3_applies(python_version):
+        for tag in (Tag(interpreter, "abi3", platform_) for platform_ in platforms):
+            yield tag
     for tag in (Tag(interpreter, "none", platform_) for platform_ in platforms):
         yield tag
-    # PEP 384 was first implemented in Python 3.2.
-    for minor_version in range(py_version[1] - 1, 1, -1):
-        for platform_ in platforms:
-            interpreter = "cp{major}{minor}".format(
-                major=py_version[0], minor=minor_version
-            )
-            yield Tag(interpreter, "abi3", platform_)
+
+    if _abi3_applies(python_version):
+        for minor_version in range(python_version[1] - 1, 1, -1):
+            for platform_ in platforms:
+                interpreter = "cp{version}".format(
+                    version=_version_nodot((python_version[0], minor_version))
+                )
+                yield Tag(interpreter, "abi3", platform_)
 
 
-def _pypy_interpreter():
-    return "pp{py_major}{pypy_major}{pypy_minor}".format(
-        py_major=sys.version_info[0],
-        pypy_major=sys.pypy_version_info.major,
-        pypy_minor=sys.pypy_version_info.minor,
-    )
-
-
 def _generic_abi():
+    # type: () -> Iterator[str]
     abi = sysconfig.get_config_var("SOABI")
     if abi:
-        return _normalize_string(abi)
-    else:
-        return "none"
+        yield _normalize_string(abi)
 
 
-def _pypy_tags(py_version, interpreter, abi, platforms):
-    for tag in (Tag(interpreter, abi, platform) for platform in platforms):
-        yield tag
-    for tag in (Tag(interpreter, "none", platform) for platform in platforms):
-        yield tag
+def generic_tags(
+    interpreter=None,  # type: Optional[str]
+    abis=None,  # type: Optional[Iterable[str]]
+    platforms=None,  # type: Optional[Iterable[str]]
+    **kwargs  # type: bool
+):
+    # type: (...) -> Iterator[Tag]
+    """
+    Yields the tags for a generic interpreter.
 
+    The tags consist of:
+    - <interpreter>-<abi>-<platform>
 
-def _generic_tags(interpreter, py_version, abi, platforms):
-    for tag in (Tag(interpreter, abi, platform) for platform in platforms):
-        yield tag
-    if abi != "none":
-        tags = (Tag(interpreter, "none", platform_) for platform_ in platforms)
-        for tag in tags:
-            yield tag
+    The "none" ABI will be added if it was not explicitly provided.
+    """
+    warn = _warn_keyword_parameter("generic_tags", kwargs)
+    if not interpreter:
+        interp_name = interpreter_name()
+        interp_version = interpreter_version(warn=warn)
+        interpreter = "".join([interp_name, interp_version])
+    if abis is None:
+        abis = _generic_abi()
+    platforms = list(platforms or _platform_tags())
+    abis = list(abis)
+    if "none" not in abis:
+        abis.append("none")
+    for abi in abis:
+        for platform_ in platforms:
+            yield Tag(interpreter, abi, platform_)
 
 
 def _py_interpreter_range(py_version):
+    # type: (PythonVersion) -> Iterator[str]
     """
-    Yield Python versions in descending order.
+    Yields Python versions in descending order.
 
     After the latest version, the major-only version will be yielded, and then
-    all following versions up to 'end'.
+    all previous versions of that major version.
     """
-    yield "py{major}{minor}".format(major=py_version[0], minor=py_version[1])
+    if len(py_version) > 1:
+        yield "py{version}".format(version=_version_nodot(py_version[:2]))
     yield "py{major}".format(major=py_version[0])
-    for minor in range(py_version[1] - 1, -1, -1):
-        yield "py{major}{minor}".format(major=py_version[0], minor=minor)
+    if len(py_version) > 1:
+        for minor in range(py_version[1] - 1, -1, -1):
+            yield "py{version}".format(version=_version_nodot((py_version[0], minor)))
 
 
-def _independent_tags(interpreter, py_version, platforms):
+def compatible_tags(
+    python_version=None,  # type: Optional[PythonVersion]
+    interpreter=None,  # type: Optional[str]
+    platforms=None,  # type: Optional[Iterable[str]]
+):
+    # type: (...) -> Iterator[Tag]
     """
-    Return the sequence of tags that are consistent across implementations.
+    Yields the sequence of tags that are compatible with a specific version of Python.
 
     The tags consist of:
     - py*-none-<platform>
-    - <interpreter>-none-any
+    - <interpreter>-none-any  # ... if `interpreter` is provided.
     - py*-none-any
     """
-    for version in _py_interpreter_range(py_version):
+    if not python_version:
+        python_version = sys.version_info[:2]
+    platforms = list(platforms or _platform_tags())
+    for version in _py_interpreter_range(python_version):
         for platform_ in platforms:
             yield Tag(version, "none", platform_)
-    yield Tag(interpreter, "none", "any")
-    for version in _py_interpreter_range(py_version):
+    if interpreter:
+        yield Tag(interpreter, "none", "any")
+    for version in _py_interpreter_range(python_version):
         yield Tag(version, "none", "any")
 
 
 def _mac_arch(arch, is_32bit=_32_BIT_INTERPRETER):
+    # type: (str, bool) -> str
     if not is_32bit:
         return arch
 
@@ -214,6 +359,7 @@
 
 
 def _mac_binary_formats(version, cpu_arch):
+    # type: (MacVersion, str) -> List[str]
     formats = [cpu_arch]
     if cpu_arch == "x86_64":
         if version < (10, 4):
@@ -240,32 +386,42 @@
     return formats
 
 
-def _mac_platforms(version=None, arch=None):
-    version_str, _, cpu_arch = platform.mac_ver()
+def mac_platforms(version=None, arch=None):
+    # type: (Optional[MacVersion], Optional[str]) -> Iterator[str]
+    """
+    Yields the platform tags for a macOS system.
+
+    The `version` parameter is a two-item tuple specifying the macOS version to
+    generate platform tags for. The `arch` parameter is the CPU architecture to
+    generate platform tags for. Both parameters default to the appropriate value
+    for the current system.
+    """
+    version_str, _, cpu_arch = platform.mac_ver()  # type: ignore
     if version is None:
-        version = tuple(map(int, version_str.split(".")[:2]))
+        version = cast("MacVersion", tuple(map(int, version_str.split(".")[:2])))
+    else:
+        version = version
     if arch is None:
         arch = _mac_arch(cpu_arch)
-    platforms = []
+    else:
+        arch = arch
     for minor_version in range(version[1], -1, -1):
         compat_version = version[0], minor_version
         binary_formats = _mac_binary_formats(compat_version, arch)
         for binary_format in binary_formats:
-            platforms.append(
-                "macosx_{major}_{minor}_{binary_format}".format(
-                    major=compat_version[0],
-                    minor=compat_version[1],
-                    binary_format=binary_format,
-                )
+            yield "macosx_{major}_{minor}_{binary_format}".format(
+                major=compat_version[0],
+                minor=compat_version[1],
+                binary_format=binary_format,
             )
-    return platforms
 
 
 # From PEP 513.
 def _is_manylinux_compatible(name, glibc_version):
+    # type: (str, GlibcVersion) -> bool
     # Check for presence of _manylinux module.
     try:
-        import _manylinux
+        import _manylinux  # noqa
 
         return bool(getattr(_manylinux, name + "_compatible"))
     except (ImportError, AttributeError):
@@ -276,14 +432,50 @@
 
 
 def _glibc_version_string():
+    # type: () -> Optional[str]
     # Returns glibc version string, or None if not using glibc.
-    import ctypes
+    return _glibc_version_string_confstr() or _glibc_version_string_ctypes()
+
+
+def _glibc_version_string_confstr():
+    # type: () -> Optional[str]
+    """
+    Primary implementation of glibc_version_string using os.confstr.
+    """
+    # os.confstr is quite a bit faster than ctypes.DLL. It's also less likely
+    # to be broken or missing. This strategy is used in the standard library
+    # platform module.
+    # https://github.com/python/cpython/blob/fcf1d003bf4f0100c9d0921ff3d70e1127ca1b71/Lib/platform.py#L175-L183
+    try:
+        # os.confstr("CS_GNU_LIBC_VERSION") returns a string like "glibc 2.17".
+        version_string = os.confstr(  # type: ignore[attr-defined] # noqa: F821
+            "CS_GNU_LIBC_VERSION"
+        )
+        assert version_string is not None
+        _, version = version_string.split()  # type: Tuple[str, str]
+    except (AssertionError, AttributeError, OSError, ValueError):
+        # os.confstr() or CS_GNU_LIBC_VERSION not available (or a bad value)...
+        return None
+    return version
+
+
+def _glibc_version_string_ctypes():
+    # type: () -> Optional[str]
+    """
+    Fallback implementation of glibc_version_string using ctypes.
+    """
+    try:
+        import ctypes
+    except ImportError:
+        return None
 
     # ctypes.CDLL(None) internally calls dlopen(NULL), and as the dlopen
     # manpage says, "If filename is NULL, then the returned handle is for the
     # main program". This way we can let the linker do the work to figure out
     # which libc our process is actually using.
-    process_namespace = ctypes.CDLL(None)
+    #
+    # Note: typeshed is wrong here so we are ignoring this line.
+    process_namespace = ctypes.CDLL(None)  # type: ignore
     try:
         gnu_get_libc_version = process_namespace.gnu_get_libc_version
     except AttributeError:
@@ -293,7 +485,7 @@
 
     # Call gnu_get_libc_version, which returns a string like "2.5"
     gnu_get_libc_version.restype = ctypes.c_char_p
-    version_str = gnu_get_libc_version()
+    version_str = gnu_get_libc_version()  # type: str
     # py2 / py3 compatibility:
     if not isinstance(version_str, str):
         version_str = version_str.decode("ascii")
@@ -303,6 +495,7 @@
 
 # Separated out from have_compatible_glibc for easier unit testing.
 def _check_glibc_version(version_str, required_major, minimum_minor):
+    # type: (str, int, int) -> bool
     # Parse string and check against requested version.
     #
     # We use a regexp instead of str.split because we want to discard any
@@ -324,81 +517,235 @@
 
 
 def _have_compatible_glibc(required_major, minimum_minor):
+    # type: (int, int) -> bool
     version_str = _glibc_version_string()
     if version_str is None:
         return False
     return _check_glibc_version(version_str, required_major, minimum_minor)
 
 
+# Python does not provide platform information at sufficient granularity to
+# identify the architecture of the running executable in some cases, so we
+# determine it dynamically by reading the information from the running
+# process. This only applies on Linux, which uses the ELF format.
+class _ELFFileHeader(object):
+    # https://en.wikipedia.org/wiki/Executable_and_Linkable_Format#File_header
+    class _InvalidELFFileHeader(ValueError):
+        """
+        An invalid ELF file header was found.
+        """
+
+    ELF_MAGIC_NUMBER = 0x7F454C46
+    ELFCLASS32 = 1
+    ELFCLASS64 = 2
+    ELFDATA2LSB = 1
+    ELFDATA2MSB = 2
+    EM_386 = 3
+    EM_S390 = 22
+    EM_ARM = 40
+    EM_X86_64 = 62
+    EF_ARM_ABIMASK = 0xFF000000
+    EF_ARM_ABI_VER5 = 0x05000000
+    EF_ARM_ABI_FLOAT_HARD = 0x00000400
+
+    def __init__(self, file):
+        # type: (IO[bytes]) -> None
+        def unpack(fmt):
+            # type: (str) -> int
+            try:
+                (result,) = struct.unpack(
+                    fmt, file.read(struct.calcsize(fmt))
+                )  # type: (int, )
+            except struct.error:
+                raise _ELFFileHeader._InvalidELFFileHeader()
+            return result
+
+        self.e_ident_magic = unpack(">I")
+        if self.e_ident_magic != self.ELF_MAGIC_NUMBER:
+            raise _ELFFileHeader._InvalidELFFileHeader()
+        self.e_ident_class = unpack("B")
+        if self.e_ident_class not in {self.ELFCLASS32, self.ELFCLASS64}:
+            raise _ELFFileHeader._InvalidELFFileHeader()
+        self.e_ident_data = unpack("B")
+        if self.e_ident_data not in {self.ELFDATA2LSB, self.ELFDATA2MSB}:
+            raise _ELFFileHeader._InvalidELFFileHeader()
+        self.e_ident_version = unpack("B")
+        self.e_ident_osabi = unpack("B")
+        self.e_ident_abiversion = unpack("B")
+        self.e_ident_pad = file.read(7)
+        format_h = "<H" if self.e_ident_data == self.ELFDATA2LSB else ">H"
+        format_i = "<I" if self.e_ident_data == self.ELFDATA2LSB else ">I"
+        format_q = "<Q" if self.e_ident_data == self.ELFDATA2LSB else ">Q"
+        format_p = format_i if self.e_ident_class == self.ELFCLASS32 else format_q
+        self.e_type = unpack(format_h)
+        self.e_machine = unpack(format_h)
+        self.e_version = unpack(format_i)
+        self.e_entry = unpack(format_p)
+        self.e_phoff = unpack(format_p)
+        self.e_shoff = unpack(format_p)
+        self.e_flags = unpack(format_i)
+        self.e_ehsize = unpack(format_h)
+        self.e_phentsize = unpack(format_h)
+        self.e_phnum = unpack(format_h)
+        self.e_shentsize = unpack(format_h)
+        self.e_shnum = unpack(format_h)
+        self.e_shstrndx = unpack(format_h)
+
+
+def _get_elf_header():
+    # type: () -> Optional[_ELFFileHeader]
+    try:
+        with open(sys.executable, "rb") as f:
+            elf_header = _ELFFileHeader(f)
+    except (IOError, OSError, TypeError, _ELFFileHeader._InvalidELFFileHeader):
+        return None
+    return elf_header
+
+
+def _is_linux_armhf():
+    # type: () -> bool
+    # hard-float ABI can be detected from the ELF header of the running
+    # process
+    # https://static.docs.arm.com/ihi0044/g/aaelf32.pdf
+    elf_header = _get_elf_header()
+    if elf_header is None:
+        return False
+    result = elf_header.e_ident_class == elf_header.ELFCLASS32
+    result &= elf_header.e_ident_data == elf_header.ELFDATA2LSB
+    result &= elf_header.e_machine == elf_header.EM_ARM
+    result &= (
+        elf_header.e_flags & elf_header.EF_ARM_ABIMASK
+    ) == elf_header.EF_ARM_ABI_VER5
+    result &= (
+        elf_header.e_flags & elf_header.EF_ARM_ABI_FLOAT_HARD
+    ) == elf_header.EF_ARM_ABI_FLOAT_HARD
+    return result
+
+
+def _is_linux_i686():
+    # type: () -> bool
+    elf_header = _get_elf_header()
+    if elf_header is None:
+        return False
+    result = elf_header.e_ident_class == elf_header.ELFCLASS32
+    result &= elf_header.e_ident_data == elf_header.ELFDATA2LSB
+    result &= elf_header.e_machine == elf_header.EM_386
+    return result
+
+
+def _have_compatible_manylinux_abi(arch):
+    # type: (str) -> bool
+    if arch == "armv7l":
+        return _is_linux_armhf()
+    if arch == "i686":
+        return _is_linux_i686()
+    return True
+
+
 def _linux_platforms(is_32bit=_32_BIT_INTERPRETER):
+    # type: (bool) -> Iterator[str]
     linux = _normalize_string(distutils.util.get_platform())
-    if linux == "linux_x86_64" and is_32bit:
-        linux = "linux_i686"
-    manylinux_support = (
-        ("manylinux2014", (2, 17)),  # CentOS 7 w/ glibc 2.17 (PEP 599)
-        ("manylinux2010", (2, 12)),  # CentOS 6 w/ glibc 2.12 (PEP 571)
-        ("manylinux1", (2, 5)),  # CentOS 5 w/ glibc 2.5 (PEP 513)
-    )
+    if is_32bit:
+        if linux == "linux_x86_64":
+            linux = "linux_i686"
+        elif linux == "linux_aarch64":
+            linux = "linux_armv7l"
+    manylinux_support = []
+    _, arch = linux.split("_", 1)
+    if _have_compatible_manylinux_abi(arch):
+        if arch in {"x86_64", "i686", "aarch64", "armv7l", "ppc64", "ppc64le", "s390x"}:
+            manylinux_support.append(
+                ("manylinux2014", (2, 17))
+            )  # CentOS 7 w/ glibc 2.17 (PEP 599)
+        if arch in {"x86_64", "i686"}:
+            manylinux_support.append(
+                ("manylinux2010", (2, 12))
+            )  # CentOS 6 w/ glibc 2.12 (PEP 571)
+            manylinux_support.append(
+                ("manylinux1", (2, 5))
+            )  # CentOS 5 w/ glibc 2.5 (PEP 513)
     manylinux_support_iter = iter(manylinux_support)
     for name, glibc_version in manylinux_support_iter:
         if _is_manylinux_compatible(name, glibc_version):
-            platforms = [linux.replace("linux", name)]
+            yield linux.replace("linux", name)
             break
-    else:
-        platforms = []
     # Support for a later manylinux implies support for an earlier version.
-    platforms += [linux.replace("linux", name) for name, _ in manylinux_support_iter]
-    platforms.append(linux)
-    return platforms
+    for name, _ in manylinux_support_iter:
+        yield linux.replace("linux", name)
+    yield linux
 
 
 def _generic_platforms():
-    platform = _normalize_string(distutils.util.get_platform())
-    return [platform]
+    # type: () -> Iterator[str]
+    yield _normalize_string(distutils.util.get_platform())
+
 
+def _platform_tags():
+    # type: () -> Iterator[str]
+    """
+    Provides the platform tags for this installation.
+    """
+    if platform.system() == "Darwin":
+        return mac_platforms()
+    elif platform.system() == "Linux":
+        return _linux_platforms()
+    else:
+        return _generic_platforms()
 
-def _interpreter_name():
-    name = platform.python_implementation().lower()
+
+def interpreter_name():
+    # type: () -> str
+    """
+    Returns the name of the running interpreter.
+    """
+    try:
+        name = sys.implementation.name  # type: ignore
+    except AttributeError:  # pragma: no cover
+        # Python 2.7 compatibility.
+        name = platform.python_implementation().lower()
     return INTERPRETER_SHORT_NAMES.get(name) or name
 
 
-def _generic_interpreter(name, py_version):
-    version = sysconfig.get_config_var("py_version_nodot")
-    if not version:
-        version = "".join(map(str, py_version[:2]))
-    return "{name}{version}".format(name=name, version=version)
+def interpreter_version(**kwargs):
+    # type: (bool) -> str
+    """
+    Returns the version of the running interpreter.
+    """
+    warn = _warn_keyword_parameter("interpreter_version", kwargs)
+    version = _get_config_var("py_version_nodot", warn=warn)
+    if version:
+        version = str(version)
+    else:
+        version = _version_nodot(sys.version_info[:2])
+    return version
+
 
+def _version_nodot(version):
+    # type: (PythonVersion) -> str
+    if any(v >= 10 for v in version):
+        sep = "_"
+    else:
+        sep = ""
+    return sep.join(map(str, version))
 
-def sys_tags():
+
+def sys_tags(**kwargs):
+    # type: (bool) -> Iterator[Tag]
     """
     Returns the sequence of tag triples for the running interpreter.
 
     The order of the sequence corresponds to priority order for the
     interpreter, from most to least important.
     """
-    py_version = sys.version_info[:2]
-    interpreter_name = _interpreter_name()
-    if platform.system() == "Darwin":
-        platforms = _mac_platforms()
-    elif platform.system() == "Linux":
-        platforms = _linux_platforms()
-    else:
-        platforms = _generic_platforms()
+    warn = _warn_keyword_parameter("sys_tags", kwargs)
 
-    if interpreter_name == "cp":
-        interpreter = _cpython_interpreter(py_version)
-        abis = _cpython_abis(py_version)
-        for tag in _cpython_tags(py_version, interpreter, abis, platforms):
-            yield tag
-    elif interpreter_name == "pp":
-        interpreter = _pypy_interpreter()
-        abi = _generic_abi()
-        for tag in _pypy_tags(py_version, interpreter, abi, platforms):
+    interp_name = interpreter_name()
+    if interp_name == "cp":
+        for tag in cpython_tags(warn=warn):
             yield tag
     else:
-        interpreter = _generic_interpreter(interpreter_name, py_version)
-        abi = _generic_abi()
-        for tag in _generic_tags(interpreter, py_version, abi, platforms):
+        for tag in generic_tags():
             yield tag
-    for tag in _independent_tags(interpreter, py_version, platforms):
+
+    for tag in compatible_tags():
         yield tag
Index: env/lib/python3.8/site-packages/setuptools/sandbox.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport sys\nimport tempfile\nimport operator\nimport functools\nimport itertools\nimport re\nimport contextlib\nimport pickle\nimport textwrap\n\nfrom setuptools.extern import six\nfrom setuptools.extern.six.moves import builtins, map\n\nimport pkg_resources\nfrom distutils.errors import DistutilsError\nfrom pkg_resources import working_set\n\nif sys.platform.startswith('java'):\n    import org.python.modules.posix.PosixModule as _os\nelse:\n    _os = sys.modules[os.name]\ntry:\n    _file = file\nexcept NameError:\n    _file = None\n_open = open\n\n\n__all__ = [\n    \"AbstractSandbox\", \"DirectorySandbox\", \"SandboxViolation\", \"run_setup\",\n]\n\n\ndef _execfile(filename, globals, locals=None):\n    \"\"\"\n    Python 3 implementation of execfile.\n    \"\"\"\n    mode = 'rb'\n    with open(filename, mode) as stream:\n        script = stream.read()\n    if locals is None:\n        locals = globals\n    code = compile(script, filename, 'exec')\n    exec(code, globals, locals)\n\n\n@contextlib.contextmanager\ndef save_argv(repl=None):\n    saved = sys.argv[:]\n    if repl is not None:\n        sys.argv[:] = repl\n    try:\n        yield saved\n    finally:\n        sys.argv[:] = saved\n\n\n@contextlib.contextmanager\ndef save_path():\n    saved = sys.path[:]\n    try:\n        yield saved\n    finally:\n        sys.path[:] = saved\n\n\n@contextlib.contextmanager\ndef override_temp(replacement):\n    \"\"\"\n    Monkey-patch tempfile.tempdir with replacement, ensuring it exists\n    \"\"\"\n    os.makedirs(replacement, exist_ok=True)\n\n    saved = tempfile.tempdir\n\n    tempfile.tempdir = replacement\n\n    try:\n        yield\n    finally:\n        tempfile.tempdir = saved\n\n\n@contextlib.contextmanager\ndef pushd(target):\n    saved = os.getcwd()\n    os.chdir(target)\n    try:\n        yield saved\n    finally:\n        os.chdir(saved)\n\n\nclass UnpickleableException(Exception):\n    \"\"\"\n    An exception representing another Exception that could not be pickled.\n    \"\"\"\n\n    @staticmethod\n    def dump(type, exc):\n        \"\"\"\n        Always return a dumped (pickled) type and exc. If exc can't be pickled,\n        wrap it in UnpickleableException first.\n        \"\"\"\n        try:\n            return pickle.dumps(type), pickle.dumps(exc)\n        except Exception:\n            # get UnpickleableException inside the sandbox\n            from setuptools.sandbox import UnpickleableException as cls\n            return cls.dump(cls, cls(repr(exc)))\n\n\nclass ExceptionSaver:\n    \"\"\"\n    A Context Manager that will save an exception, serialized, and restore it\n    later.\n    \"\"\"\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, exc, tb):\n        if not exc:\n            return\n\n        # dump the exception\n        self._saved = UnpickleableException.dump(type, exc)\n        self._tb = tb\n\n        # suppress the exception\n        return True\n\n    def resume(self):\n        \"restore and re-raise any exception\"\n\n        if '_saved' not in vars(self):\n            return\n\n        type, exc = map(pickle.loads, self._saved)\n        six.reraise(type, exc, self._tb)\n\n\n@contextlib.contextmanager\ndef save_modules():\n    \"\"\"\n    Context in which imported modules are saved.\n\n    Translates exceptions internal to the context into the equivalent exception\n    outside the context.\n    \"\"\"\n    saved = sys.modules.copy()\n    with ExceptionSaver() as saved_exc:\n        yield saved\n\n    sys.modules.update(saved)\n    # remove any modules imported since\n    del_modules = (\n        mod_name for mod_name in sys.modules\n        if mod_name not in saved\n        # exclude any encodings modules. See #285\n        and not mod_name.startswith('encodings.')\n    )\n    _clear_modules(del_modules)\n\n    saved_exc.resume()\n\n\ndef _clear_modules(module_names):\n    for mod_name in list(module_names):\n        del sys.modules[mod_name]\n\n\n@contextlib.contextmanager\ndef save_pkg_resources_state():\n    saved = pkg_resources.__getstate__()\n    try:\n        yield saved\n    finally:\n        pkg_resources.__setstate__(saved)\n\n\n@contextlib.contextmanager\ndef setup_context(setup_dir):\n    temp_dir = os.path.join(setup_dir, 'temp')\n    with save_pkg_resources_state():\n        with save_modules():\n            hide_setuptools()\n            with save_path():\n                with save_argv():\n                    with override_temp(temp_dir):\n                        with pushd(setup_dir):\n                            # ensure setuptools commands are available\n                            __import__('setuptools')\n                            yield\n\n\ndef _needs_hiding(mod_name):\n    \"\"\"\n    >>> _needs_hiding('setuptools')\n    True\n    >>> _needs_hiding('pkg_resources')\n    True\n    >>> _needs_hiding('setuptools_plugin')\n    False\n    >>> _needs_hiding('setuptools.__init__')\n    True\n    >>> _needs_hiding('distutils')\n    True\n    >>> _needs_hiding('os')\n    False\n    >>> _needs_hiding('Cython')\n    True\n    \"\"\"\n    pattern = re.compile(r'(setuptools|pkg_resources|distutils|Cython)(\\.|$)')\n    return bool(pattern.match(mod_name))\n\n\ndef hide_setuptools():\n    \"\"\"\n    Remove references to setuptools' modules from sys.modules to allow the\n    invocation to import the most appropriate setuptools. This technique is\n    necessary to avoid issues such as #315 where setuptools upgrading itself\n    would fail to find a function declared in the metadata.\n    \"\"\"\n    modules = filter(_needs_hiding, sys.modules)\n    _clear_modules(modules)\n\n\ndef run_setup(setup_script, args):\n    \"\"\"Run a distutils setup script, sandboxed in its directory\"\"\"\n    setup_dir = os.path.abspath(os.path.dirname(setup_script))\n    with setup_context(setup_dir):\n        try:\n            sys.argv[:] = [setup_script] + list(args)\n            sys.path.insert(0, setup_dir)\n            # reset to include setup dir, w/clean callback list\n            working_set.__init__()\n            working_set.callbacks.append(lambda dist: dist.activate())\n\n            # __file__ should be a byte string on Python 2 (#712)\n            dunder_file = (\n                setup_script\n                if isinstance(setup_script, str) else\n                setup_script.encode(sys.getfilesystemencoding())\n            )\n\n            with DirectorySandbox(setup_dir):\n                ns = dict(__file__=dunder_file, __name__='__main__')\n                _execfile(setup_script, ns)\n        except SystemExit as v:\n            if v.args and v.args[0]:\n                raise\n            # Normal exit, just return\n\n\nclass AbstractSandbox:\n    \"\"\"Wrap 'os' module and 'open()' builtin for virtualizing setup scripts\"\"\"\n\n    _active = False\n\n    def __init__(self):\n        self._attrs = [\n            name for name in dir(_os)\n            if not name.startswith('_') and hasattr(self, name)\n        ]\n\n    def _copy(self, source):\n        for name in self._attrs:\n            setattr(os, name, getattr(source, name))\n\n    def __enter__(self):\n        self._copy(self)\n        if _file:\n            builtins.file = self._file\n        builtins.open = self._open\n        self._active = True\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._active = False\n        if _file:\n            builtins.file = _file\n        builtins.open = _open\n        self._copy(_os)\n\n    def run(self, func):\n        \"\"\"Run 'func' under os sandboxing\"\"\"\n        with self:\n            return func()\n\n    def _mk_dual_path_wrapper(name):\n        original = getattr(_os, name)\n\n        def wrap(self, src, dst, *args, **kw):\n            if self._active:\n                src, dst = self._remap_pair(name, src, dst, *args, **kw)\n            return original(src, dst, *args, **kw)\n\n        return wrap\n\n    for name in [\"rename\", \"link\", \"symlink\"]:\n        if hasattr(_os, name):\n            locals()[name] = _mk_dual_path_wrapper(name)\n\n    def _mk_single_path_wrapper(name, original=None):\n        original = original or getattr(_os, name)\n\n        def wrap(self, path, *args, **kw):\n            if self._active:\n                path = self._remap_input(name, path, *args, **kw)\n            return original(path, *args, **kw)\n\n        return wrap\n\n    if _file:\n        _file = _mk_single_path_wrapper('file', _file)\n    _open = _mk_single_path_wrapper('open', _open)\n    for name in [\n        \"stat\", \"listdir\", \"chdir\", \"open\", \"chmod\", \"chown\", \"mkdir\",\n        \"remove\", \"unlink\", \"rmdir\", \"utime\", \"lchown\", \"chroot\", \"lstat\",\n        \"startfile\", \"mkfifo\", \"mknod\", \"pathconf\", \"access\"\n    ]:\n        if hasattr(_os, name):\n            locals()[name] = _mk_single_path_wrapper(name)\n\n    def _mk_single_with_return(name):\n        original = getattr(_os, name)\n\n        def wrap(self, path, *args, **kw):\n            if self._active:\n                path = self._remap_input(name, path, *args, **kw)\n                return self._remap_output(name, original(path, *args, **kw))\n            return original(path, *args, **kw)\n\n        return wrap\n\n    for name in ['readlink', 'tempnam']:\n        if hasattr(_os, name):\n            locals()[name] = _mk_single_with_return(name)\n\n    def _mk_query(name):\n        original = getattr(_os, name)\n\n        def wrap(self, *args, **kw):\n            retval = original(*args, **kw)\n            if self._active:\n                return self._remap_output(name, retval)\n            return retval\n\n        return wrap\n\n    for name in ['getcwd', 'tmpnam']:\n        if hasattr(_os, name):\n            locals()[name] = _mk_query(name)\n\n    def _validate_path(self, path):\n        \"\"\"Called to remap or validate any path, whether input or output\"\"\"\n        return path\n\n    def _remap_input(self, operation, path, *args, **kw):\n        \"\"\"Called for path inputs\"\"\"\n        return self._validate_path(path)\n\n    def _remap_output(self, operation, path):\n        \"\"\"Called for path outputs\"\"\"\n        return self._validate_path(path)\n\n    def _remap_pair(self, operation, src, dst, *args, **kw):\n        \"\"\"Called for path pairs like rename, link, and symlink operations\"\"\"\n        return (\n            self._remap_input(operation + '-from', src, *args, **kw),\n            self._remap_input(operation + '-to', dst, *args, **kw)\n        )\n\n\nif hasattr(os, 'devnull'):\n    _EXCEPTIONS = [os.devnull]\nelse:\n    _EXCEPTIONS = []\n\n\nclass DirectorySandbox(AbstractSandbox):\n    \"\"\"Restrict operations to a single subdirectory - pseudo-chroot\"\"\"\n\n    write_ops = dict.fromkeys([\n        \"open\", \"chmod\", \"chown\", \"mkdir\", \"remove\", \"unlink\", \"rmdir\",\n        \"utime\", \"lchown\", \"chroot\", \"mkfifo\", \"mknod\", \"tempnam\",\n    ])\n\n    _exception_patterns = [\n        # Allow lib2to3 to attempt to save a pickled grammar object (#121)\n        r'.*lib2to3.*\\.pickle$',\n    ]\n    \"exempt writing to paths that match the pattern\"\n\n    def __init__(self, sandbox, exceptions=_EXCEPTIONS):\n        self._sandbox = os.path.normcase(os.path.realpath(sandbox))\n        self._prefix = os.path.join(self._sandbox, '')\n        self._exceptions = [\n            os.path.normcase(os.path.realpath(path))\n            for path in exceptions\n        ]\n        AbstractSandbox.__init__(self)\n\n    def _violation(self, operation, *args, **kw):\n        from setuptools.sandbox import SandboxViolation\n        raise SandboxViolation(operation, args, kw)\n\n    if _file:\n\n        def _file(self, path, mode='r', *args, **kw):\n            if mode not in ('r', 'rt', 'rb', 'rU', 'U') and not self._ok(path):\n                self._violation(\"file\", path, mode, *args, **kw)\n            return _file(path, mode, *args, **kw)\n\n    def _open(self, path, mode='r', *args, **kw):\n        if mode not in ('r', 'rt', 'rb', 'rU', 'U') and not self._ok(path):\n            self._violation(\"open\", path, mode, *args, **kw)\n        return _open(path, mode, *args, **kw)\n\n    def tmpnam(self):\n        self._violation(\"tmpnam\")\n\n    def _ok(self, path):\n        active = self._active\n        try:\n            self._active = False\n            realpath = os.path.normcase(os.path.realpath(path))\n            return (\n                self._exempted(realpath)\n                or realpath == self._sandbox\n                or realpath.startswith(self._prefix)\n            )\n        finally:\n            self._active = active\n\n    def _exempted(self, filepath):\n        start_matches = (\n            filepath.startswith(exception)\n            for exception in self._exceptions\n        )\n        pattern_matches = (\n            re.match(pattern, filepath)\n            for pattern in self._exception_patterns\n        )\n        candidates = itertools.chain(start_matches, pattern_matches)\n        return any(candidates)\n\n    def _remap_input(self, operation, path, *args, **kw):\n        \"\"\"Called for path inputs\"\"\"\n        if operation in self.write_ops and not self._ok(path):\n            self._violation(operation, os.path.realpath(path), *args, **kw)\n        return path\n\n    def _remap_pair(self, operation, src, dst, *args, **kw):\n        \"\"\"Called for path pairs like rename, link, and symlink operations\"\"\"\n        if not self._ok(src) or not self._ok(dst):\n            self._violation(operation, src, dst, *args, **kw)\n        return (src, dst)\n\n    def open(self, file, flags, mode=0o777, *args, **kw):\n        \"\"\"Called for low-level os.open()\"\"\"\n        if flags & WRITE_FLAGS and not self._ok(file):\n            self._violation(\"os.open\", file, flags, mode, *args, **kw)\n        return _os.open(file, flags, mode, *args, **kw)\n\n\nWRITE_FLAGS = functools.reduce(\n    operator.or_, [\n        getattr(_os, a, 0) for a in\n        \"O_WRONLY O_RDWR O_APPEND O_CREAT O_TRUNC O_TEMPORARY\".split()]\n)\n\n\nclass SandboxViolation(DistutilsError):\n    \"\"\"A setup script attempted to modify the filesystem outside the sandbox\"\"\"\n\n    tmpl = textwrap.dedent(\"\"\"\n        SandboxViolation: {cmd}{args!r} {kwargs}\n\n        The package setup script has attempted to modify files on your system\n        that are not within the EasyInstall build area, and has been aborted.\n\n        This package cannot be safely installed by EasyInstall, and may not\n        support alternate installation locations even if you run its setup\n        script by hand.  Please inform the package's author and the EasyInstall\n        maintainers to find out if a fix or workaround is available.\n        \"\"\").lstrip()\n\n    def __str__(self):\n        cmd, args, kwargs = self.args\n        return self.tmpl.format(**locals())\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/sandbox.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/sandbox.py	(date 1602088701601)
@@ -8,9 +8,7 @@
 import contextlib
 import pickle
 import textwrap
-
-from setuptools.extern import six
-from setuptools.extern.six.moves import builtins, map
+import builtins
 
 import pkg_resources
 from distutils.errors import DistutilsError
@@ -138,7 +136,7 @@
             return
 
         type, exc = map(pickle.loads, self._saved)
-        six.reraise(type, exc, self._tb)
+        raise exc.with_traceback(self._tb)
 
 
 @contextlib.contextmanager
@@ -185,8 +183,8 @@
     temp_dir = os.path.join(setup_dir, 'temp')
     with save_pkg_resources_state():
         with save_modules():
-            hide_setuptools()
             with save_path():
+                hide_setuptools()
                 with save_argv():
                     with override_temp(temp_dir):
                         with pushd(setup_dir):
@@ -195,6 +193,15 @@
                             yield
 
 
+_MODULES_TO_HIDE = {
+    'setuptools',
+    'distutils',
+    'pkg_resources',
+    'Cython',
+    '_distutils_hack',
+}
+
+
 def _needs_hiding(mod_name):
     """
     >>> _needs_hiding('setuptools')
@@ -212,8 +219,8 @@
     >>> _needs_hiding('Cython')
     True
     """
-    pattern = re.compile(r'(setuptools|pkg_resources|distutils|Cython)(\.|$)')
-    return bool(pattern.match(mod_name))
+    base_module = mod_name.split('.', 1)[0]
+    return base_module in _MODULES_TO_HIDE
 
 
 def hide_setuptools():
@@ -223,6 +230,10 @@
     necessary to avoid issues such as #315 where setuptools upgrading itself
     would fail to find a function declared in the metadata.
     """
+    _distutils_hack = sys.modules.get('_distutils_hack', None)
+    if _distutils_hack is not None:
+        _distutils_hack.remove_shim()
+
     modules = filter(_needs_hiding, sys.modules)
     _clear_modules(modules)
 
@@ -238,15 +249,8 @@
             working_set.__init__()
             working_set.callbacks.append(lambda dist: dist.activate())
 
-            # __file__ should be a byte string on Python 2 (#712)
-            dunder_file = (
-                setup_script
-                if isinstance(setup_script, str) else
-                setup_script.encode(sys.getfilesystemencoding())
-            )
-
             with DirectorySandbox(setup_dir):
-                ns = dict(__file__=dunder_file, __name__='__main__')
+                ns = dict(__file__=setup_script, __name__='__main__')
                 _execfile(setup_script, ns)
         except SystemExit as v:
             if v.args and v.args[0]:
Index: env/lib/python3.8/site-packages/setuptools/_vendor/packaging/_structures.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import absolute_import, division, print_function\n\n\nclass Infinity(object):\n    def __repr__(self):\n        return \"Infinity\"\n\n    def __hash__(self):\n        return hash(repr(self))\n\n    def __lt__(self, other):\n        return False\n\n    def __le__(self, other):\n        return False\n\n    def __eq__(self, other):\n        return isinstance(other, self.__class__)\n\n    def __ne__(self, other):\n        return not isinstance(other, self.__class__)\n\n    def __gt__(self, other):\n        return True\n\n    def __ge__(self, other):\n        return True\n\n    def __neg__(self):\n        return NegativeInfinity\n\n\nInfinity = Infinity()\n\n\nclass NegativeInfinity(object):\n    def __repr__(self):\n        return \"-Infinity\"\n\n    def __hash__(self):\n        return hash(repr(self))\n\n    def __lt__(self, other):\n        return True\n\n    def __le__(self, other):\n        return True\n\n    def __eq__(self, other):\n        return isinstance(other, self.__class__)\n\n    def __ne__(self, other):\n        return not isinstance(other, self.__class__)\n\n    def __gt__(self, other):\n        return False\n\n    def __ge__(self, other):\n        return False\n\n    def __neg__(self):\n        return Infinity\n\n\nNegativeInfinity = NegativeInfinity()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/_vendor/packaging/_structures.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/_vendor/packaging/_structures.py	(date 1602088701617)
@@ -4,65 +4,83 @@
 from __future__ import absolute_import, division, print_function
 
 
-class Infinity(object):
+class InfinityType(object):
     def __repr__(self):
+        # type: () -> str
         return "Infinity"
 
     def __hash__(self):
+        # type: () -> int
         return hash(repr(self))
 
     def __lt__(self, other):
+        # type: (object) -> bool
         return False
 
     def __le__(self, other):
+        # type: (object) -> bool
         return False
 
     def __eq__(self, other):
+        # type: (object) -> bool
         return isinstance(other, self.__class__)
 
     def __ne__(self, other):
+        # type: (object) -> bool
         return not isinstance(other, self.__class__)
 
     def __gt__(self, other):
+        # type: (object) -> bool
         return True
 
     def __ge__(self, other):
+        # type: (object) -> bool
         return True
 
     def __neg__(self):
+        # type: (object) -> NegativeInfinityType
         return NegativeInfinity
 
 
-Infinity = Infinity()
+Infinity = InfinityType()
 
 
-class NegativeInfinity(object):
+class NegativeInfinityType(object):
     def __repr__(self):
+        # type: () -> str
         return "-Infinity"
 
     def __hash__(self):
+        # type: () -> int
         return hash(repr(self))
 
     def __lt__(self, other):
+        # type: (object) -> bool
         return True
 
     def __le__(self, other):
+        # type: (object) -> bool
         return True
 
     def __eq__(self, other):
+        # type: (object) -> bool
         return isinstance(other, self.__class__)
 
     def __ne__(self, other):
+        # type: (object) -> bool
         return not isinstance(other, self.__class__)
 
     def __gt__(self, other):
+        # type: (object) -> bool
         return False
 
     def __ge__(self, other):
+        # type: (object) -> bool
         return False
 
     def __neg__(self):
+        # type: (object) -> InfinityType
         return Infinity
 
 
-NegativeInfinity = NegativeInfinity()
+NegativeInfinity = NegativeInfinityType()
Index: env/lib/python3.8/site-packages/setuptools/depends.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import sys\nimport marshal\nimport contextlib\nfrom distutils.version import StrictVersion\n\nfrom .py33compat import Bytecode\n\nfrom .py27compat import find_module, PY_COMPILED, PY_FROZEN, PY_SOURCE\nfrom . import py27compat\n\n\n__all__ = [\n    'Require', 'find_module', 'get_module_constant', 'extract_constant'\n]\n\n\nclass Require:\n    \"\"\"A prerequisite to building or installing a distribution\"\"\"\n\n    def __init__(\n            self, name, requested_version, module, homepage='',\n            attribute=None, format=None):\n\n        if format is None and requested_version is not None:\n            format = StrictVersion\n\n        if format is not None:\n            requested_version = format(requested_version)\n            if attribute is None:\n                attribute = '__version__'\n\n        self.__dict__.update(locals())\n        del self.self\n\n    def full_name(self):\n        \"\"\"Return full package/distribution name, w/version\"\"\"\n        if self.requested_version is not None:\n            return '%s-%s' % (self.name, self.requested_version)\n        return self.name\n\n    def version_ok(self, version):\n        \"\"\"Is 'version' sufficiently up-to-date?\"\"\"\n        return self.attribute is None or self.format is None or \\\n            str(version) != \"unknown\" and version >= self.requested_version\n\n    def get_version(self, paths=None, default=\"unknown\"):\n        \"\"\"Get version number of installed module, 'None', or 'default'\n\n        Search 'paths' for module.  If not found, return 'None'.  If found,\n        return the extracted version attribute, or 'default' if no version\n        attribute was specified, or the value cannot be determined without\n        importing the module.  The version is formatted according to the\n        requirement's version format (if any), unless it is 'None' or the\n        supplied 'default'.\n        \"\"\"\n\n        if self.attribute is None:\n            try:\n                f, p, i = find_module(self.module, paths)\n                if f:\n                    f.close()\n                return default\n            except ImportError:\n                return None\n\n        v = get_module_constant(self.module, self.attribute, default, paths)\n\n        if v is not None and v is not default and self.format is not None:\n            return self.format(v)\n\n        return v\n\n    def is_present(self, paths=None):\n        \"\"\"Return true if dependency is present on 'paths'\"\"\"\n        return self.get_version(paths) is not None\n\n    def is_current(self, paths=None):\n        \"\"\"Return true if dependency is present and up-to-date on 'paths'\"\"\"\n        version = self.get_version(paths)\n        if version is None:\n            return False\n        return self.version_ok(version)\n\n\ndef maybe_close(f):\n    @contextlib.contextmanager\n    def empty():\n        yield\n        return\n    if not f:\n        return empty()\n\n    return contextlib.closing(f)\n\n\ndef get_module_constant(module, symbol, default=-1, paths=None):\n    \"\"\"Find 'module' by searching 'paths', and extract 'symbol'\n\n    Return 'None' if 'module' does not exist on 'paths', or it does not define\n    'symbol'.  If the module defines 'symbol' as a constant, return the\n    constant.  Otherwise, return 'default'.\"\"\"\n\n    try:\n        f, path, (suffix, mode, kind) = info = find_module(module, paths)\n    except ImportError:\n        # Module doesn't exist\n        return None\n\n    with maybe_close(f):\n        if kind == PY_COMPILED:\n            f.read(8)  # skip magic & date\n            code = marshal.load(f)\n        elif kind == PY_FROZEN:\n            code = py27compat.get_frozen_object(module, paths)\n        elif kind == PY_SOURCE:\n            code = compile(f.read(), path, 'exec')\n        else:\n            # Not something we can parse; we'll have to import it.  :(\n            imported = py27compat.get_module(module, paths, info)\n            return getattr(imported, symbol, None)\n\n    return extract_constant(code, symbol, default)\n\n\ndef extract_constant(code, symbol, default=-1):\n    \"\"\"Extract the constant value of 'symbol' from 'code'\n\n    If the name 'symbol' is bound to a constant value by the Python code\n    object 'code', return that value.  If 'symbol' is bound to an expression,\n    return 'default'.  Otherwise, return 'None'.\n\n    Return value is based on the first assignment to 'symbol'.  'symbol' must\n    be a global, or at least a non-\"fast\" local in the code block.  That is,\n    only 'STORE_NAME' and 'STORE_GLOBAL' opcodes are checked, and 'symbol'\n    must be present in 'code.co_names'.\n    \"\"\"\n    if symbol not in code.co_names:\n        # name's not there, can't possibly be an assignment\n        return None\n\n    name_idx = list(code.co_names).index(symbol)\n\n    STORE_NAME = 90\n    STORE_GLOBAL = 97\n    LOAD_CONST = 100\n\n    const = default\n\n    for byte_code in Bytecode(code):\n        op = byte_code.opcode\n        arg = byte_code.arg\n\n        if op == LOAD_CONST:\n            const = code.co_consts[arg]\n        elif arg == name_idx and (op == STORE_NAME or op == STORE_GLOBAL):\n            return const\n        else:\n            const = default\n\n\ndef _update_globals():\n    \"\"\"\n    Patch the globals to remove the objects not available on some platforms.\n\n    XXX it'd be better to test assertions about bytecode instead.\n    \"\"\"\n\n    if not sys.platform.startswith('java') and sys.platform != 'cli':\n        return\n    incompatible = 'extract_constant', 'get_module_constant'\n    for name in incompatible:\n        del globals()[name]\n        __all__.remove(name)\n\n\n_update_globals()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/depends.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/depends.py	(date 1602088701597)
@@ -1,12 +1,11 @@
 import sys
 import marshal
 import contextlib
+import dis
 from distutils.version import StrictVersion
 
-from .py33compat import Bytecode
-
-from .py27compat import find_module, PY_COMPILED, PY_FROZEN, PY_SOURCE
-from . import py27compat
+from ._imp import find_module, PY_COMPILED, PY_FROZEN, PY_SOURCE
+from . import _imp
 
 
 __all__ = [
@@ -111,12 +110,12 @@
             f.read(8)  # skip magic & date
             code = marshal.load(f)
         elif kind == PY_FROZEN:
-            code = py27compat.get_frozen_object(module, paths)
+            code = _imp.get_frozen_object(module, paths)
         elif kind == PY_SOURCE:
             code = compile(f.read(), path, 'exec')
         else:
             # Not something we can parse; we'll have to import it.  :(
-            imported = py27compat.get_module(module, paths, info)
+            imported = _imp.get_module(module, paths, info)
             return getattr(imported, symbol, None)
 
     return extract_constant(code, symbol, default)
@@ -146,7 +145,7 @@
 
     const = default
 
-    for byte_code in Bytecode(code):
+    for byte_code in dis.Bytecode(code):
         op = byte_code.opcode
         arg = byte_code.arg
 
Index: env/lib/python3.8/site-packages/setuptools/_vendor/packaging/specifiers.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import absolute_import, division, print_function\n\nimport abc\nimport functools\nimport itertools\nimport re\n\nfrom ._compat import string_types, with_metaclass\nfrom .version import Version, LegacyVersion, parse\n\n\nclass InvalidSpecifier(ValueError):\n    \"\"\"\n    An invalid specifier was found, users should refer to PEP 440.\n    \"\"\"\n\n\nclass BaseSpecifier(with_metaclass(abc.ABCMeta, object)):\n    @abc.abstractmethod\n    def __str__(self):\n        \"\"\"\n        Returns the str representation of this Specifier like object. This\n        should be representative of the Specifier itself.\n        \"\"\"\n\n    @abc.abstractmethod\n    def __hash__(self):\n        \"\"\"\n        Returns a hash value for this Specifier like object.\n        \"\"\"\n\n    @abc.abstractmethod\n    def __eq__(self, other):\n        \"\"\"\n        Returns a boolean representing whether or not the two Specifier like\n        objects are equal.\n        \"\"\"\n\n    @abc.abstractmethod\n    def __ne__(self, other):\n        \"\"\"\n        Returns a boolean representing whether or not the two Specifier like\n        objects are not equal.\n        \"\"\"\n\n    @abc.abstractproperty\n    def prereleases(self):\n        \"\"\"\n        Returns whether or not pre-releases as a whole are allowed by this\n        specifier.\n        \"\"\"\n\n    @prereleases.setter\n    def prereleases(self, value):\n        \"\"\"\n        Sets whether or not pre-releases as a whole are allowed by this\n        specifier.\n        \"\"\"\n\n    @abc.abstractmethod\n    def contains(self, item, prereleases=None):\n        \"\"\"\n        Determines if the given item is contained within this specifier.\n        \"\"\"\n\n    @abc.abstractmethod\n    def filter(self, iterable, prereleases=None):\n        \"\"\"\n        Takes an iterable of items and filters them so that only items which\n        are contained within this specifier are allowed in it.\n        \"\"\"\n\n\nclass _IndividualSpecifier(BaseSpecifier):\n\n    _operators = {}\n\n    def __init__(self, spec=\"\", prereleases=None):\n        match = self._regex.search(spec)\n        if not match:\n            raise InvalidSpecifier(\"Invalid specifier: '{0}'\".format(spec))\n\n        self._spec = (match.group(\"operator\").strip(), match.group(\"version\").strip())\n\n        # Store whether or not this Specifier should accept prereleases\n        self._prereleases = prereleases\n\n    def __repr__(self):\n        pre = (\n            \", prereleases={0!r}\".format(self.prereleases)\n            if self._prereleases is not None\n            else \"\"\n        )\n\n        return \"<{0}({1!r}{2})>\".format(self.__class__.__name__, str(self), pre)\n\n    def __str__(self):\n        return \"{0}{1}\".format(*self._spec)\n\n    def __hash__(self):\n        return hash(self._spec)\n\n    def __eq__(self, other):\n        if isinstance(other, string_types):\n            try:\n                other = self.__class__(other)\n            except InvalidSpecifier:\n                return NotImplemented\n        elif not isinstance(other, self.__class__):\n            return NotImplemented\n\n        return self._spec == other._spec\n\n    def __ne__(self, other):\n        if isinstance(other, string_types):\n            try:\n                other = self.__class__(other)\n            except InvalidSpecifier:\n                return NotImplemented\n        elif not isinstance(other, self.__class__):\n            return NotImplemented\n\n        return self._spec != other._spec\n\n    def _get_operator(self, op):\n        return getattr(self, \"_compare_{0}\".format(self._operators[op]))\n\n    def _coerce_version(self, version):\n        if not isinstance(version, (LegacyVersion, Version)):\n            version = parse(version)\n        return version\n\n    @property\n    def operator(self):\n        return self._spec[0]\n\n    @property\n    def version(self):\n        return self._spec[1]\n\n    @property\n    def prereleases(self):\n        return self._prereleases\n\n    @prereleases.setter\n    def prereleases(self, value):\n        self._prereleases = value\n\n    def __contains__(self, item):\n        return self.contains(item)\n\n    def contains(self, item, prereleases=None):\n        # Determine if prereleases are to be allowed or not.\n        if prereleases is None:\n            prereleases = self.prereleases\n\n        # Normalize item to a Version or LegacyVersion, this allows us to have\n        # a shortcut for ``\"2.0\" in Specifier(\">=2\")\n        item = self._coerce_version(item)\n\n        # Determine if we should be supporting prereleases in this specifier\n        # or not, if we do not support prereleases than we can short circuit\n        # logic if this version is a prereleases.\n        if item.is_prerelease and not prereleases:\n            return False\n\n        # Actually do the comparison to determine if this item is contained\n        # within this Specifier or not.\n        return self._get_operator(self.operator)(item, self.version)\n\n    def filter(self, iterable, prereleases=None):\n        yielded = False\n        found_prereleases = []\n\n        kw = {\"prereleases\": prereleases if prereleases is not None else True}\n\n        # Attempt to iterate over all the values in the iterable and if any of\n        # them match, yield them.\n        for version in iterable:\n            parsed_version = self._coerce_version(version)\n\n            if self.contains(parsed_version, **kw):\n                # If our version is a prerelease, and we were not set to allow\n                # prereleases, then we'll store it for later incase nothing\n                # else matches this specifier.\n                if parsed_version.is_prerelease and not (\n                    prereleases or self.prereleases\n                ):\n                    found_prereleases.append(version)\n                # Either this is not a prerelease, or we should have been\n                # accepting prereleases from the beginning.\n                else:\n                    yielded = True\n                    yield version\n\n        # Now that we've iterated over everything, determine if we've yielded\n        # any values, and if we have not and we have any prereleases stored up\n        # then we will go ahead and yield the prereleases.\n        if not yielded and found_prereleases:\n            for version in found_prereleases:\n                yield version\n\n\nclass LegacySpecifier(_IndividualSpecifier):\n\n    _regex_str = r\"\"\"\n        (?P<operator>(==|!=|<=|>=|<|>))\n        \\s*\n        (?P<version>\n            [^,;\\s)]* # Since this is a \"legacy\" specifier, and the version\n                      # string can be just about anything, we match everything\n                      # except for whitespace, a semi-colon for marker support,\n                      # a closing paren since versions can be enclosed in\n                      # them, and a comma since it's a version separator.\n        )\n        \"\"\"\n\n    _regex = re.compile(r\"^\\s*\" + _regex_str + r\"\\s*$\", re.VERBOSE | re.IGNORECASE)\n\n    _operators = {\n        \"==\": \"equal\",\n        \"!=\": \"not_equal\",\n        \"<=\": \"less_than_equal\",\n        \">=\": \"greater_than_equal\",\n        \"<\": \"less_than\",\n        \">\": \"greater_than\",\n    }\n\n    def _coerce_version(self, version):\n        if not isinstance(version, LegacyVersion):\n            version = LegacyVersion(str(version))\n        return version\n\n    def _compare_equal(self, prospective, spec):\n        return prospective == self._coerce_version(spec)\n\n    def _compare_not_equal(self, prospective, spec):\n        return prospective != self._coerce_version(spec)\n\n    def _compare_less_than_equal(self, prospective, spec):\n        return prospective <= self._coerce_version(spec)\n\n    def _compare_greater_than_equal(self, prospective, spec):\n        return prospective >= self._coerce_version(spec)\n\n    def _compare_less_than(self, prospective, spec):\n        return prospective < self._coerce_version(spec)\n\n    def _compare_greater_than(self, prospective, spec):\n        return prospective > self._coerce_version(spec)\n\n\ndef _require_version_compare(fn):\n    @functools.wraps(fn)\n    def wrapped(self, prospective, spec):\n        if not isinstance(prospective, Version):\n            return False\n        return fn(self, prospective, spec)\n\n    return wrapped\n\n\nclass Specifier(_IndividualSpecifier):\n\n    _regex_str = r\"\"\"\n        (?P<operator>(~=|==|!=|<=|>=|<|>|===))\n        (?P<version>\n            (?:\n                # The identity operators allow for an escape hatch that will\n                # do an exact string match of the version you wish to install.\n                # This will not be parsed by PEP 440 and we cannot determine\n                # any semantic meaning from it. This operator is discouraged\n                # but included entirely as an escape hatch.\n                (?<====)  # Only match for the identity operator\n                \\s*\n                [^\\s]*    # We just match everything, except for whitespace\n                          # since we are only testing for strict identity.\n            )\n            |\n            (?:\n                # The (non)equality operators allow for wild card and local\n                # versions to be specified so we have to define these two\n                # operators separately to enable that.\n                (?<===|!=)            # Only match for equals and not equals\n\n                \\s*\n                v?\n                (?:[0-9]+!)?          # epoch\n                [0-9]+(?:\\.[0-9]+)*   # release\n                (?:                   # pre release\n                    [-_\\.]?\n                    (a|b|c|rc|alpha|beta|pre|preview)\n                    [-_\\.]?\n                    [0-9]*\n                )?\n                (?:                   # post release\n                    (?:-[0-9]+)|(?:[-_\\.]?(post|rev|r)[-_\\.]?[0-9]*)\n                )?\n\n                # You cannot use a wild card and a dev or local version\n                # together so group them with a | and make them optional.\n                (?:\n                    (?:[-_\\.]?dev[-_\\.]?[0-9]*)?         # dev release\n                    (?:\\+[a-z0-9]+(?:[-_\\.][a-z0-9]+)*)? # local\n                    |\n                    \\.\\*  # Wild card syntax of .*\n                )?\n            )\n            |\n            (?:\n                # The compatible operator requires at least two digits in the\n                # release segment.\n                (?<=~=)               # Only match for the compatible operator\n\n                \\s*\n                v?\n                (?:[0-9]+!)?          # epoch\n                [0-9]+(?:\\.[0-9]+)+   # release  (We have a + instead of a *)\n                (?:                   # pre release\n                    [-_\\.]?\n                    (a|b|c|rc|alpha|beta|pre|preview)\n                    [-_\\.]?\n                    [0-9]*\n                )?\n                (?:                                   # post release\n                    (?:-[0-9]+)|(?:[-_\\.]?(post|rev|r)[-_\\.]?[0-9]*)\n                )?\n                (?:[-_\\.]?dev[-_\\.]?[0-9]*)?          # dev release\n            )\n            |\n            (?:\n                # All other operators only allow a sub set of what the\n                # (non)equality operators do. Specifically they do not allow\n                # local versions to be specified nor do they allow the prefix\n                # matching wild cards.\n                (?<!==|!=|~=)         # We have special cases for these\n                                      # operators so we want to make sure they\n                                      # don't match here.\n\n                \\s*\n                v?\n                (?:[0-9]+!)?          # epoch\n                [0-9]+(?:\\.[0-9]+)*   # release\n                (?:                   # pre release\n                    [-_\\.]?\n                    (a|b|c|rc|alpha|beta|pre|preview)\n                    [-_\\.]?\n                    [0-9]*\n                )?\n                (?:                                   # post release\n                    (?:-[0-9]+)|(?:[-_\\.]?(post|rev|r)[-_\\.]?[0-9]*)\n                )?\n                (?:[-_\\.]?dev[-_\\.]?[0-9]*)?          # dev release\n            )\n        )\n        \"\"\"\n\n    _regex = re.compile(r\"^\\s*\" + _regex_str + r\"\\s*$\", re.VERBOSE | re.IGNORECASE)\n\n    _operators = {\n        \"~=\": \"compatible\",\n        \"==\": \"equal\",\n        \"!=\": \"not_equal\",\n        \"<=\": \"less_than_equal\",\n        \">=\": \"greater_than_equal\",\n        \"<\": \"less_than\",\n        \">\": \"greater_than\",\n        \"===\": \"arbitrary\",\n    }\n\n    @_require_version_compare\n    def _compare_compatible(self, prospective, spec):\n        # Compatible releases have an equivalent combination of >= and ==. That\n        # is that ~=2.2 is equivalent to >=2.2,==2.*. This allows us to\n        # implement this in terms of the other specifiers instead of\n        # implementing it ourselves. The only thing we need to do is construct\n        # the other specifiers.\n\n        # We want everything but the last item in the version, but we want to\n        # ignore post and dev releases and we want to treat the pre-release as\n        # it's own separate segment.\n        prefix = \".\".join(\n            list(\n                itertools.takewhile(\n                    lambda x: (not x.startswith(\"post\") and not x.startswith(\"dev\")),\n                    _version_split(spec),\n                )\n            )[:-1]\n        )\n\n        # Add the prefix notation to the end of our string\n        prefix += \".*\"\n\n        return self._get_operator(\">=\")(prospective, spec) and self._get_operator(\"==\")(\n            prospective, prefix\n        )\n\n    @_require_version_compare\n    def _compare_equal(self, prospective, spec):\n        # We need special logic to handle prefix matching\n        if spec.endswith(\".*\"):\n            # In the case of prefix matching we want to ignore local segment.\n            prospective = Version(prospective.public)\n            # Split the spec out by dots, and pretend that there is an implicit\n            # dot in between a release segment and a pre-release segment.\n            spec = _version_split(spec[:-2])  # Remove the trailing .*\n\n            # Split the prospective version out by dots, and pretend that there\n            # is an implicit dot in between a release segment and a pre-release\n            # segment.\n            prospective = _version_split(str(prospective))\n\n            # Shorten the prospective version to be the same length as the spec\n            # so that we can determine if the specifier is a prefix of the\n            # prospective version or not.\n            prospective = prospective[: len(spec)]\n\n            # Pad out our two sides with zeros so that they both equal the same\n            # length.\n            spec, prospective = _pad_version(spec, prospective)\n        else:\n            # Convert our spec string into a Version\n            spec = Version(spec)\n\n            # If the specifier does not have a local segment, then we want to\n            # act as if the prospective version also does not have a local\n            # segment.\n            if not spec.local:\n                prospective = Version(prospective.public)\n\n        return prospective == spec\n\n    @_require_version_compare\n    def _compare_not_equal(self, prospective, spec):\n        return not self._compare_equal(prospective, spec)\n\n    @_require_version_compare\n    def _compare_less_than_equal(self, prospective, spec):\n        return prospective <= Version(spec)\n\n    @_require_version_compare\n    def _compare_greater_than_equal(self, prospective, spec):\n        return prospective >= Version(spec)\n\n    @_require_version_compare\n    def _compare_less_than(self, prospective, spec):\n        # Convert our spec to a Version instance, since we'll want to work with\n        # it as a version.\n        spec = Version(spec)\n\n        # Check to see if the prospective version is less than the spec\n        # version. If it's not we can short circuit and just return False now\n        # instead of doing extra unneeded work.\n        if not prospective < spec:\n            return False\n\n        # This special case is here so that, unless the specifier itself\n        # includes is a pre-release version, that we do not accept pre-release\n        # versions for the version mentioned in the specifier (e.g. <3.1 should\n        # not match 3.1.dev0, but should match 3.0.dev0).\n        if not spec.is_prerelease and prospective.is_prerelease:\n            if Version(prospective.base_version) == Version(spec.base_version):\n                return False\n\n        # If we've gotten to here, it means that prospective version is both\n        # less than the spec version *and* it's not a pre-release of the same\n        # version in the spec.\n        return True\n\n    @_require_version_compare\n    def _compare_greater_than(self, prospective, spec):\n        # Convert our spec to a Version instance, since we'll want to work with\n        # it as a version.\n        spec = Version(spec)\n\n        # Check to see if the prospective version is greater than the spec\n        # version. If it's not we can short circuit and just return False now\n        # instead of doing extra unneeded work.\n        if not prospective > spec:\n            return False\n\n        # This special case is here so that, unless the specifier itself\n        # includes is a post-release version, that we do not accept\n        # post-release versions for the version mentioned in the specifier\n        # (e.g. >3.1 should not match 3.0.post0, but should match 3.2.post0).\n        if not spec.is_postrelease and prospective.is_postrelease:\n            if Version(prospective.base_version) == Version(spec.base_version):\n                return False\n\n        # Ensure that we do not allow a local version of the version mentioned\n        # in the specifier, which is technically greater than, to match.\n        if prospective.local is not None:\n            if Version(prospective.base_version) == Version(spec.base_version):\n                return False\n\n        # If we've gotten to here, it means that prospective version is both\n        # greater than the spec version *and* it's not a pre-release of the\n        # same version in the spec.\n        return True\n\n    def _compare_arbitrary(self, prospective, spec):\n        return str(prospective).lower() == str(spec).lower()\n\n    @property\n    def prereleases(self):\n        # If there is an explicit prereleases set for this, then we'll just\n        # blindly use that.\n        if self._prereleases is not None:\n            return self._prereleases\n\n        # Look at all of our specifiers and determine if they are inclusive\n        # operators, and if they are if they are including an explicit\n        # prerelease.\n        operator, version = self._spec\n        if operator in [\"==\", \">=\", \"<=\", \"~=\", \"===\"]:\n            # The == specifier can include a trailing .*, if it does we\n            # want to remove before parsing.\n            if operator == \"==\" and version.endswith(\".*\"):\n                version = version[:-2]\n\n            # Parse the version, and if it is a pre-release than this\n            # specifier allows pre-releases.\n            if parse(version).is_prerelease:\n                return True\n\n        return False\n\n    @prereleases.setter\n    def prereleases(self, value):\n        self._prereleases = value\n\n\n_prefix_regex = re.compile(r\"^([0-9]+)((?:a|b|c|rc)[0-9]+)$\")\n\n\ndef _version_split(version):\n    result = []\n    for item in version.split(\".\"):\n        match = _prefix_regex.search(item)\n        if match:\n            result.extend(match.groups())\n        else:\n            result.append(item)\n    return result\n\n\ndef _pad_version(left, right):\n    left_split, right_split = [], []\n\n    # Get the release segment of our versions\n    left_split.append(list(itertools.takewhile(lambda x: x.isdigit(), left)))\n    right_split.append(list(itertools.takewhile(lambda x: x.isdigit(), right)))\n\n    # Get the rest of our versions\n    left_split.append(left[len(left_split[0]) :])\n    right_split.append(right[len(right_split[0]) :])\n\n    # Insert our padding\n    left_split.insert(1, [\"0\"] * max(0, len(right_split[0]) - len(left_split[0])))\n    right_split.insert(1, [\"0\"] * max(0, len(left_split[0]) - len(right_split[0])))\n\n    return (list(itertools.chain(*left_split)), list(itertools.chain(*right_split)))\n\n\nclass SpecifierSet(BaseSpecifier):\n    def __init__(self, specifiers=\"\", prereleases=None):\n        # Split on , to break each indidivual specifier into it's own item, and\n        # strip each item to remove leading/trailing whitespace.\n        specifiers = [s.strip() for s in specifiers.split(\",\") if s.strip()]\n\n        # Parsed each individual specifier, attempting first to make it a\n        # Specifier and falling back to a LegacySpecifier.\n        parsed = set()\n        for specifier in specifiers:\n            try:\n                parsed.add(Specifier(specifier))\n            except InvalidSpecifier:\n                parsed.add(LegacySpecifier(specifier))\n\n        # Turn our parsed specifiers into a frozen set and save them for later.\n        self._specs = frozenset(parsed)\n\n        # Store our prereleases value so we can use it later to determine if\n        # we accept prereleases or not.\n        self._prereleases = prereleases\n\n    def __repr__(self):\n        pre = (\n            \", prereleases={0!r}\".format(self.prereleases)\n            if self._prereleases is not None\n            else \"\"\n        )\n\n        return \"<SpecifierSet({0!r}{1})>\".format(str(self), pre)\n\n    def __str__(self):\n        return \",\".join(sorted(str(s) for s in self._specs))\n\n    def __hash__(self):\n        return hash(self._specs)\n\n    def __and__(self, other):\n        if isinstance(other, string_types):\n            other = SpecifierSet(other)\n        elif not isinstance(other, SpecifierSet):\n            return NotImplemented\n\n        specifier = SpecifierSet()\n        specifier._specs = frozenset(self._specs | other._specs)\n\n        if self._prereleases is None and other._prereleases is not None:\n            specifier._prereleases = other._prereleases\n        elif self._prereleases is not None and other._prereleases is None:\n            specifier._prereleases = self._prereleases\n        elif self._prereleases == other._prereleases:\n            specifier._prereleases = self._prereleases\n        else:\n            raise ValueError(\n                \"Cannot combine SpecifierSets with True and False prerelease \"\n                \"overrides.\"\n            )\n\n        return specifier\n\n    def __eq__(self, other):\n        if isinstance(other, string_types):\n            other = SpecifierSet(other)\n        elif isinstance(other, _IndividualSpecifier):\n            other = SpecifierSet(str(other))\n        elif not isinstance(other, SpecifierSet):\n            return NotImplemented\n\n        return self._specs == other._specs\n\n    def __ne__(self, other):\n        if isinstance(other, string_types):\n            other = SpecifierSet(other)\n        elif isinstance(other, _IndividualSpecifier):\n            other = SpecifierSet(str(other))\n        elif not isinstance(other, SpecifierSet):\n            return NotImplemented\n\n        return self._specs != other._specs\n\n    def __len__(self):\n        return len(self._specs)\n\n    def __iter__(self):\n        return iter(self._specs)\n\n    @property\n    def prereleases(self):\n        # If we have been given an explicit prerelease modifier, then we'll\n        # pass that through here.\n        if self._prereleases is not None:\n            return self._prereleases\n\n        # If we don't have any specifiers, and we don't have a forced value,\n        # then we'll just return None since we don't know if this should have\n        # pre-releases or not.\n        if not self._specs:\n            return None\n\n        # Otherwise we'll see if any of the given specifiers accept\n        # prereleases, if any of them do we'll return True, otherwise False.\n        return any(s.prereleases for s in self._specs)\n\n    @prereleases.setter\n    def prereleases(self, value):\n        self._prereleases = value\n\n    def __contains__(self, item):\n        return self.contains(item)\n\n    def contains(self, item, prereleases=None):\n        # Ensure that our item is a Version or LegacyVersion instance.\n        if not isinstance(item, (LegacyVersion, Version)):\n            item = parse(item)\n\n        # Determine if we're forcing a prerelease or not, if we're not forcing\n        # one for this particular filter call, then we'll use whatever the\n        # SpecifierSet thinks for whether or not we should support prereleases.\n        if prereleases is None:\n            prereleases = self.prereleases\n\n        # We can determine if we're going to allow pre-releases by looking to\n        # see if any of the underlying items supports them. If none of them do\n        # and this item is a pre-release then we do not allow it and we can\n        # short circuit that here.\n        # Note: This means that 1.0.dev1 would not be contained in something\n        #       like >=1.0.devabc however it would be in >=1.0.debabc,>0.0.dev0\n        if not prereleases and item.is_prerelease:\n            return False\n\n        # We simply dispatch to the underlying specs here to make sure that the\n        # given version is contained within all of them.\n        # Note: This use of all() here means that an empty set of specifiers\n        #       will always return True, this is an explicit design decision.\n        return all(s.contains(item, prereleases=prereleases) for s in self._specs)\n\n    def filter(self, iterable, prereleases=None):\n        # Determine if we're forcing a prerelease or not, if we're not forcing\n        # one for this particular filter call, then we'll use whatever the\n        # SpecifierSet thinks for whether or not we should support prereleases.\n        if prereleases is None:\n            prereleases = self.prereleases\n\n        # If we have any specifiers, then we want to wrap our iterable in the\n        # filter method for each one, this will act as a logical AND amongst\n        # each specifier.\n        if self._specs:\n            for spec in self._specs:\n                iterable = spec.filter(iterable, prereleases=bool(prereleases))\n            return iterable\n        # If we do not have any specifiers, then we need to have a rough filter\n        # which will filter out any pre-releases, unless there are no final\n        # releases, and which will filter out LegacyVersion in general.\n        else:\n            filtered = []\n            found_prereleases = []\n\n            for item in iterable:\n                # Ensure that we some kind of Version class for this item.\n                if not isinstance(item, (LegacyVersion, Version)):\n                    parsed_version = parse(item)\n                else:\n                    parsed_version = item\n\n                # Filter out any item which is parsed as a LegacyVersion\n                if isinstance(parsed_version, LegacyVersion):\n                    continue\n\n                # Store any item which is a pre-release for later unless we've\n                # already found a final version or we are accepting prereleases\n                if parsed_version.is_prerelease and not prereleases:\n                    if not filtered:\n                        found_prereleases.append(item)\n                else:\n                    filtered.append(item)\n\n            # If we've found no items except for pre-releases, then we'll go\n            # ahead and use the pre-releases\n            if not filtered and found_prereleases and prereleases is None:\n                return found_prereleases\n\n            return filtered\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/_vendor/packaging/specifiers.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/_vendor/packaging/specifiers.py	(date 1602088701617)
@@ -9,8 +9,27 @@
 import re
 
 from ._compat import string_types, with_metaclass
+from ._typing import TYPE_CHECKING
+from .utils import canonicalize_version
 from .version import Version, LegacyVersion, parse
 
+if TYPE_CHECKING:  # pragma: no cover
+    from typing import (
+        List,
+        Dict,
+        Union,
+        Iterable,
+        Iterator,
+        Optional,
+        Callable,
+        Tuple,
+        FrozenSet,
+    )
+
+    ParsedVersion = Union[Version, LegacyVersion]
+    UnparsedVersion = Union[Version, LegacyVersion, str]
+    CallableOperator = Callable[[ParsedVersion, str], bool]
+
 
 class InvalidSpecifier(ValueError):
     """
@@ -18,9 +37,10 @@
     """
 
 
-class BaseSpecifier(with_metaclass(abc.ABCMeta, object)):
+class BaseSpecifier(with_metaclass(abc.ABCMeta, object)):  # type: ignore
     @abc.abstractmethod
     def __str__(self):
+        # type: () -> str
         """
         Returns the str representation of this Specifier like object. This
         should be representative of the Specifier itself.
@@ -28,12 +48,14 @@
 
     @abc.abstractmethod
     def __hash__(self):
+        # type: () -> int
         """
         Returns a hash value for this Specifier like object.
         """
 
     @abc.abstractmethod
     def __eq__(self, other):
+        # type: (object) -> bool
         """
         Returns a boolean representing whether or not the two Specifier like
         objects are equal.
@@ -41,6 +63,7 @@
 
     @abc.abstractmethod
     def __ne__(self, other):
+        # type: (object) -> bool
         """
         Returns a boolean representing whether or not the two Specifier like
         objects are not equal.
@@ -48,6 +71,7 @@
 
     @abc.abstractproperty
     def prereleases(self):
+        # type: () -> Optional[bool]
         """
         Returns whether or not pre-releases as a whole are allowed by this
         specifier.
@@ -55,6 +79,7 @@
 
     @prereleases.setter
     def prereleases(self, value):
+        # type: (bool) -> None
         """
         Sets whether or not pre-releases as a whole are allowed by this
         specifier.
@@ -62,12 +87,14 @@
 
     @abc.abstractmethod
     def contains(self, item, prereleases=None):
+        # type: (str, Optional[bool]) -> bool
         """
         Determines if the given item is contained within this specifier.
         """
 
     @abc.abstractmethod
     def filter(self, iterable, prereleases=None):
+        # type: (Iterable[UnparsedVersion], Optional[bool]) -> Iterable[UnparsedVersion]
         """
         Takes an iterable of items and filters them so that only items which
         are contained within this specifier are allowed in it.
@@ -76,19 +103,24 @@
 
 class _IndividualSpecifier(BaseSpecifier):
 
-    _operators = {}
+    _operators = {}  # type: Dict[str, str]
 
     def __init__(self, spec="", prereleases=None):
+        # type: (str, Optional[bool]) -> None
         match = self._regex.search(spec)
         if not match:
             raise InvalidSpecifier("Invalid specifier: '{0}'".format(spec))
 
-        self._spec = (match.group("operator").strip(), match.group("version").strip())
+        self._spec = (
+            match.group("operator").strip(),
+            match.group("version").strip(),
+        )  # type: Tuple[str, str]
 
         # Store whether or not this Specifier should accept prereleases
         self._prereleases = prereleases
 
     def __repr__(self):
+        # type: () -> str
         pre = (
             ", prereleases={0!r}".format(self.prereleases)
             if self._prereleases is not None
@@ -98,26 +130,35 @@
         return "<{0}({1!r}{2})>".format(self.__class__.__name__, str(self), pre)
 
     def __str__(self):
+        # type: () -> str
         return "{0}{1}".format(*self._spec)
 
+    @property
+    def _canonical_spec(self):
+        # type: () -> Tuple[str, Union[Version, str]]
+        return self._spec[0], canonicalize_version(self._spec[1])
+
     def __hash__(self):
-        return hash(self._spec)
+        # type: () -> int
+        return hash(self._canonical_spec)
 
     def __eq__(self, other):
+        # type: (object) -> bool
         if isinstance(other, string_types):
             try:
-                other = self.__class__(other)
+                other = self.__class__(str(other))
             except InvalidSpecifier:
                 return NotImplemented
         elif not isinstance(other, self.__class__):
             return NotImplemented
 
-        return self._spec == other._spec
+        return self._canonical_spec == other._canonical_spec
 
     def __ne__(self, other):
+        # type: (object) -> bool
         if isinstance(other, string_types):
             try:
-                other = self.__class__(other)
+                other = self.__class__(str(other))
             except InvalidSpecifier:
                 return NotImplemented
         elif not isinstance(other, self.__class__):
@@ -126,52 +167,67 @@
         return self._spec != other._spec
 
     def _get_operator(self, op):
-        return getattr(self, "_compare_{0}".format(self._operators[op]))
+        # type: (str) -> CallableOperator
+        operator_callable = getattr(
+            self, "_compare_{0}".format(self._operators[op])
+        )  # type: CallableOperator
+        return operator_callable
 
     def _coerce_version(self, version):
+        # type: (UnparsedVersion) -> ParsedVersion
         if not isinstance(version, (LegacyVersion, Version)):
             version = parse(version)
         return version
 
     @property
     def operator(self):
+        # type: () -> str
         return self._spec[0]
 
     @property
     def version(self):
+        # type: () -> str
         return self._spec[1]
 
     @property
     def prereleases(self):
+        # type: () -> Optional[bool]
         return self._prereleases
 
     @prereleases.setter
     def prereleases(self, value):
+        # type: (bool) -> None
         self._prereleases = value
 
     def __contains__(self, item):
+        # type: (str) -> bool
         return self.contains(item)
 
     def contains(self, item, prereleases=None):
+        # type: (UnparsedVersion, Optional[bool]) -> bool
+
         # Determine if prereleases are to be allowed or not.
         if prereleases is None:
             prereleases = self.prereleases
 
         # Normalize item to a Version or LegacyVersion, this allows us to have
         # a shortcut for ``"2.0" in Specifier(">=2")
-        item = self._coerce_version(item)
+        normalized_item = self._coerce_version(item)
 
         # Determine if we should be supporting prereleases in this specifier
         # or not, if we do not support prereleases than we can short circuit
         # logic if this version is a prereleases.
-        if item.is_prerelease and not prereleases:
+        if normalized_item.is_prerelease and not prereleases:
             return False
 
         # Actually do the comparison to determine if this item is contained
         # within this Specifier or not.
-        return self._get_operator(self.operator)(item, self.version)
+        operator_callable = self._get_operator(self.operator)  # type: CallableOperator
+        return operator_callable(normalized_item, self.version)
 
     def filter(self, iterable, prereleases=None):
+        # type: (Iterable[UnparsedVersion], Optional[bool]) -> Iterable[UnparsedVersion]
+
         yielded = False
         found_prereleases = []
 
@@ -230,32 +286,43 @@
     }
 
     def _coerce_version(self, version):
+        # type: (Union[ParsedVersion, str]) -> LegacyVersion
         if not isinstance(version, LegacyVersion):
             version = LegacyVersion(str(version))
         return version
 
     def _compare_equal(self, prospective, spec):
+        # type: (LegacyVersion, str) -> bool
         return prospective == self._coerce_version(spec)
 
     def _compare_not_equal(self, prospective, spec):
+        # type: (LegacyVersion, str) -> bool
         return prospective != self._coerce_version(spec)
 
     def _compare_less_than_equal(self, prospective, spec):
+        # type: (LegacyVersion, str) -> bool
         return prospective <= self._coerce_version(spec)
 
     def _compare_greater_than_equal(self, prospective, spec):
+        # type: (LegacyVersion, str) -> bool
         return prospective >= self._coerce_version(spec)
 
     def _compare_less_than(self, prospective, spec):
+        # type: (LegacyVersion, str) -> bool
         return prospective < self._coerce_version(spec)
 
     def _compare_greater_than(self, prospective, spec):
+        # type: (LegacyVersion, str) -> bool
         return prospective > self._coerce_version(spec)
 
 
-def _require_version_compare(fn):
+def _require_version_compare(
+    fn  # type: (Callable[[Specifier, ParsedVersion, str], bool])
+):
+    # type: (...) -> Callable[[Specifier, ParsedVersion, str], bool]
     @functools.wraps(fn)
     def wrapped(self, prospective, spec):
+        # type: (Specifier, ParsedVersion, str) -> bool
         if not isinstance(prospective, Version):
             return False
         return fn(self, prospective, spec)
@@ -373,6 +440,8 @@
 
     @_require_version_compare
     def _compare_compatible(self, prospective, spec):
+        # type: (ParsedVersion, str) -> bool
+
         # Compatible releases have an equivalent combination of >= and ==. That
         # is that ~=2.2 is equivalent to >=2.2,==2.*. This allows us to
         # implement this in terms of the other specifiers instead of
@@ -400,56 +469,75 @@
 
     @_require_version_compare
     def _compare_equal(self, prospective, spec):
+        # type: (ParsedVersion, str) -> bool
+
         # We need special logic to handle prefix matching
         if spec.endswith(".*"):
             # In the case of prefix matching we want to ignore local segment.
             prospective = Version(prospective.public)
             # Split the spec out by dots, and pretend that there is an implicit
             # dot in between a release segment and a pre-release segment.
-            spec = _version_split(spec[:-2])  # Remove the trailing .*
+            split_spec = _version_split(spec[:-2])  # Remove the trailing .*
 
             # Split the prospective version out by dots, and pretend that there
             # is an implicit dot in between a release segment and a pre-release
             # segment.
-            prospective = _version_split(str(prospective))
+            split_prospective = _version_split(str(prospective))
 
             # Shorten the prospective version to be the same length as the spec
             # so that we can determine if the specifier is a prefix of the
             # prospective version or not.
-            prospective = prospective[: len(spec)]
+            shortened_prospective = split_prospective[: len(split_spec)]
 
             # Pad out our two sides with zeros so that they both equal the same
             # length.
-            spec, prospective = _pad_version(spec, prospective)
+            padded_spec, padded_prospective = _pad_version(
+                split_spec, shortened_prospective
+            )
+
+            return padded_prospective == padded_spec
         else:
             # Convert our spec string into a Version
-            spec = Version(spec)
+            spec_version = Version(spec)
 
             # If the specifier does not have a local segment, then we want to
             # act as if the prospective version also does not have a local
             # segment.
-            if not spec.local:
+            if not spec_version.local:
                 prospective = Version(prospective.public)
 
-        return prospective == spec
+            return prospective == spec_version
 
     @_require_version_compare
     def _compare_not_equal(self, prospective, spec):
+        # type: (ParsedVersion, str) -> bool
         return not self._compare_equal(prospective, spec)
 
     @_require_version_compare
     def _compare_less_than_equal(self, prospective, spec):
-        return prospective <= Version(spec)
+        # type: (ParsedVersion, str) -> bool
+
+        # NB: Local version identifiers are NOT permitted in the version
+        # specifier, so local version labels can be universally removed from
+        # the prospective version.
+        return Version(prospective.public) <= Version(spec)
 
     @_require_version_compare
     def _compare_greater_than_equal(self, prospective, spec):
-        return prospective >= Version(spec)
+        # type: (ParsedVersion, str) -> bool
+
+        # NB: Local version identifiers are NOT permitted in the version
+        # specifier, so local version labels can be universally removed from
+        # the prospective version.
+        return Version(prospective.public) >= Version(spec)
 
     @_require_version_compare
-    def _compare_less_than(self, prospective, spec):
+    def _compare_less_than(self, prospective, spec_str):
+        # type: (ParsedVersion, str) -> bool
+
         # Convert our spec to a Version instance, since we'll want to work with
         # it as a version.
-        spec = Version(spec)
+        spec = Version(spec_str)
 
         # Check to see if the prospective version is less than the spec
         # version. If it's not we can short circuit and just return False now
@@ -471,10 +559,12 @@
         return True
 
     @_require_version_compare
-    def _compare_greater_than(self, prospective, spec):
+    def _compare_greater_than(self, prospective, spec_str):
+        # type: (ParsedVersion, str) -> bool
+
         # Convert our spec to a Version instance, since we'll want to work with
         # it as a version.
-        spec = Version(spec)
+        spec = Version(spec_str)
 
         # Check to see if the prospective version is greater than the spec
         # version. If it's not we can short circuit and just return False now
@@ -502,10 +592,13 @@
         return True
 
     def _compare_arbitrary(self, prospective, spec):
+        # type: (Version, str) -> bool
         return str(prospective).lower() == str(spec).lower()
 
     @property
     def prereleases(self):
+        # type: () -> bool
+
         # If there is an explicit prereleases set for this, then we'll just
         # blindly use that.
         if self._prereleases is not None:
@@ -530,6 +623,7 @@
 
     @prereleases.setter
     def prereleases(self, value):
+        # type: (bool) -> None
         self._prereleases = value
 
 
@@ -537,7 +631,8 @@
 
 
 def _version_split(version):
-    result = []
+    # type: (str) -> List[str]
+    result = []  # type: List[str]
     for item in version.split("."):
         match = _prefix_regex.search(item)
         if match:
@@ -548,6 +643,7 @@
 
 
 def _pad_version(left, right):
+    # type: (List[str], List[str]) -> Tuple[List[str], List[str]]
     left_split, right_split = [], []
 
     # Get the release segment of our versions
@@ -567,14 +663,16 @@
 
 class SpecifierSet(BaseSpecifier):
     def __init__(self, specifiers="", prereleases=None):
-        # Split on , to break each indidivual specifier into it's own item, and
+        # type: (str, Optional[bool]) -> None
+
+        # Split on , to break each individual specifier into it's own item, and
         # strip each item to remove leading/trailing whitespace.
-        specifiers = [s.strip() for s in specifiers.split(",") if s.strip()]
+        split_specifiers = [s.strip() for s in specifiers.split(",") if s.strip()]
 
         # Parsed each individual specifier, attempting first to make it a
         # Specifier and falling back to a LegacySpecifier.
         parsed = set()
-        for specifier in specifiers:
+        for specifier in split_specifiers:
             try:
                 parsed.add(Specifier(specifier))
             except InvalidSpecifier:
@@ -588,6 +686,7 @@
         self._prereleases = prereleases
 
     def __repr__(self):
+        # type: () -> str
         pre = (
             ", prereleases={0!r}".format(self.prereleases)
             if self._prereleases is not None
@@ -597,12 +696,15 @@
         return "<SpecifierSet({0!r}{1})>".format(str(self), pre)
 
     def __str__(self):
+        # type: () -> str
         return ",".join(sorted(str(s) for s in self._specs))
 
     def __hash__(self):
+        # type: () -> int
         return hash(self._specs)
 
     def __and__(self, other):
+        # type: (Union[SpecifierSet, str]) -> SpecifierSet
         if isinstance(other, string_types):
             other = SpecifierSet(other)
         elif not isinstance(other, SpecifierSet):
@@ -626,9 +728,8 @@
         return specifier
 
     def __eq__(self, other):
-        if isinstance(other, string_types):
-            other = SpecifierSet(other)
-        elif isinstance(other, _IndividualSpecifier):
+        # type: (object) -> bool
+        if isinstance(other, (string_types, _IndividualSpecifier)):
             other = SpecifierSet(str(other))
         elif not isinstance(other, SpecifierSet):
             return NotImplemented
@@ -636,9 +737,8 @@
         return self._specs == other._specs
 
     def __ne__(self, other):
-        if isinstance(other, string_types):
-            other = SpecifierSet(other)
-        elif isinstance(other, _IndividualSpecifier):
+        # type: (object) -> bool
+        if isinstance(other, (string_types, _IndividualSpecifier)):
             other = SpecifierSet(str(other))
         elif not isinstance(other, SpecifierSet):
             return NotImplemented
@@ -646,13 +746,17 @@
         return self._specs != other._specs
 
     def __len__(self):
+        # type: () -> int
         return len(self._specs)
 
     def __iter__(self):
+        # type: () -> Iterator[FrozenSet[_IndividualSpecifier]]
         return iter(self._specs)
 
     @property
     def prereleases(self):
+        # type: () -> Optional[bool]
+
         # If we have been given an explicit prerelease modifier, then we'll
         # pass that through here.
         if self._prereleases is not None:
@@ -670,12 +774,16 @@
 
     @prereleases.setter
     def prereleases(self, value):
+        # type: (bool) -> None
         self._prereleases = value
 
     def __contains__(self, item):
+        # type: (Union[ParsedVersion, str]) -> bool
         return self.contains(item)
 
     def contains(self, item, prereleases=None):
+        # type: (Union[ParsedVersion, str], Optional[bool]) -> bool
+
         # Ensure that our item is a Version or LegacyVersion instance.
         if not isinstance(item, (LegacyVersion, Version)):
             item = parse(item)
@@ -701,7 +809,13 @@
         #       will always return True, this is an explicit design decision.
         return all(s.contains(item, prereleases=prereleases) for s in self._specs)
 
-    def filter(self, iterable, prereleases=None):
+    def filter(
+        self,
+        iterable,  # type: Iterable[Union[ParsedVersion, str]]
+        prereleases=None,  # type: Optional[bool]
+    ):
+        # type: (...) -> Iterable[Union[ParsedVersion, str]]
+
         # Determine if we're forcing a prerelease or not, if we're not forcing
         # one for this particular filter call, then we'll use whatever the
         # SpecifierSet thinks for whether or not we should support prereleases.
@@ -719,8 +833,8 @@
         # which will filter out any pre-releases, unless there are no final
         # releases, and which will filter out LegacyVersion in general.
         else:
-            filtered = []
-            found_prereleases = []
+            filtered = []  # type: List[Union[ParsedVersion, str]]
+            found_prereleases = []  # type: List[Union[ParsedVersion, str]]
 
             for item in iterable:
                 # Ensure that we some kind of Version class for this item.
Index: env/lib/python3.8/site-packages/pip/_internal/cli/base_command.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"Base Command class, and related routines\"\"\"\n\nfrom __future__ import absolute_import, print_function\n\nimport logging\nimport logging.config\nimport optparse\nimport os\nimport platform\nimport sys\nimport traceback\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.command_context import CommandContextMixIn\nfrom pip._internal.cli.parser import (\n    ConfigOptionParser,\n    UpdatingDefaultsHelpFormatter,\n)\nfrom pip._internal.cli.status_codes import (\n    ERROR,\n    PREVIOUS_BUILD_DIR_ERROR,\n    UNKNOWN_ERROR,\n    VIRTUALENV_NOT_FOUND,\n)\nfrom pip._internal.exceptions import (\n    BadCommand,\n    CommandError,\n    InstallationError,\n    NetworkConnectionError,\n    PreviousBuildDirError,\n    SubProcessError,\n    UninstallationError,\n)\nfrom pip._internal.utils.deprecation import deprecated\nfrom pip._internal.utils.filesystem import check_path_owner\nfrom pip._internal.utils.logging import BrokenStdoutLoggingError, setup_logging\nfrom pip._internal.utils.misc import get_prog, normalize_path\nfrom pip._internal.utils.temp_dir import (\n    global_tempdir_manager,\n    tempdir_registry,\n)\nfrom pip._internal.utils.typing import MYPY_CHECK_RUNNING\nfrom pip._internal.utils.virtualenv import running_under_virtualenv\n\nif MYPY_CHECK_RUNNING:\n    from typing import List, Optional, Tuple, Any\n    from optparse import Values\n\n    from pip._internal.utils.temp_dir import (\n        TempDirectoryTypeRegistry as TempDirRegistry\n    )\n\n__all__ = ['Command']\n\nlogger = logging.getLogger(__name__)\n\n\nclass Command(CommandContextMixIn):\n    usage = None  # type: str\n    ignore_require_venv = False  # type: bool\n\n    def __init__(self, name, summary, isolated=False):\n        # type: (str, str, bool) -> None\n        super(Command, self).__init__()\n        parser_kw = {\n            'usage': self.usage,\n            'prog': '{} {}'.format(get_prog(), name),\n            'formatter': UpdatingDefaultsHelpFormatter(),\n            'add_help_option': False,\n            'name': name,\n            'description': self.__doc__,\n            'isolated': isolated,\n        }\n\n        self.name = name\n        self.summary = summary\n        self.parser = ConfigOptionParser(**parser_kw)\n\n        self.tempdir_registry = None  # type: Optional[TempDirRegistry]\n\n        # Commands should add options to this option group\n        optgroup_name = '{} Options'.format(self.name.capitalize())\n        self.cmd_opts = optparse.OptionGroup(self.parser, optgroup_name)\n\n        # Add the general options\n        gen_opts = cmdoptions.make_option_group(\n            cmdoptions.general_group,\n            self.parser,\n        )\n        self.parser.add_option_group(gen_opts)\n\n        self.add_options()\n\n    def add_options(self):\n        # type: () -> None\n        pass\n\n    def handle_pip_version_check(self, options):\n        # type: (Values) -> None\n        \"\"\"\n        This is a no-op so that commands by default do not do the pip version\n        check.\n        \"\"\"\n        # Make sure we do the pip version check if the index_group options\n        # are present.\n        assert not hasattr(options, 'no_index')\n\n    def run(self, options, args):\n        # type: (Values, List[Any]) -> int\n        raise NotImplementedError\n\n    def parse_args(self, args):\n        # type: (List[str]) -> Tuple[Any, Any]\n        # factored out for testability\n        return self.parser.parse_args(args)\n\n    def main(self, args):\n        # type: (List[str]) -> int\n        try:\n            with self.main_context():\n                return self._main(args)\n        finally:\n            logging.shutdown()\n\n    def _main(self, args):\n        # type: (List[str]) -> int\n        # We must initialize this before the tempdir manager, otherwise the\n        # configuration would not be accessible by the time we clean up the\n        # tempdir manager.\n        self.tempdir_registry = self.enter_context(tempdir_registry())\n        # Intentionally set as early as possible so globally-managed temporary\n        # directories are available to the rest of the code.\n        self.enter_context(global_tempdir_manager())\n\n        options, args = self.parse_args(args)\n\n        # Set verbosity so that it can be used elsewhere.\n        self.verbosity = options.verbose - options.quiet\n\n        level_number = setup_logging(\n            verbosity=self.verbosity,\n            no_color=options.no_color,\n            user_log_file=options.log,\n        )\n\n        if (\n            sys.version_info[:2] == (2, 7) and\n            not options.no_python_version_warning\n        ):\n            message = (\n                \"pip 21.0 will drop support for Python 2.7 in January 2021. \"\n                \"More details about Python 2 support in pip can be found at \"\n                \"https://pip.pypa.io/en/latest/development/release-process/#python-2-support\"  # noqa\n            )\n            if platform.python_implementation() == \"CPython\":\n                message = (\n                    \"Python 2.7 reached the end of its life on January \"\n                    \"1st, 2020. Please upgrade your Python as Python 2.7 \"\n                    \"is no longer maintained. \"\n                ) + message\n            deprecated(message, replacement=None, gone_in=None)\n\n        # TODO: Try to get these passing down from the command?\n        #       without resorting to os.environ to hold these.\n        #       This also affects isolated builds and it should.\n\n        if options.no_input:\n            os.environ['PIP_NO_INPUT'] = '1'\n\n        if options.exists_action:\n            os.environ['PIP_EXISTS_ACTION'] = ' '.join(options.exists_action)\n\n        if options.require_venv and not self.ignore_require_venv:\n            # If a venv is required check if it can really be found\n            if not running_under_virtualenv():\n                logger.critical(\n                    'Could not find an activated virtualenv (required).'\n                )\n                sys.exit(VIRTUALENV_NOT_FOUND)\n\n        if options.cache_dir:\n            options.cache_dir = normalize_path(options.cache_dir)\n            if not check_path_owner(options.cache_dir):\n                logger.warning(\n                    \"The directory '%s' or its parent directory is not owned \"\n                    \"or is not writable by the current user. The cache \"\n                    \"has been disabled. Check the permissions and owner of \"\n                    \"that directory. If executing pip with sudo, you may want \"\n                    \"sudo's -H flag.\",\n                    options.cache_dir,\n                )\n                options.cache_dir = None\n\n        if getattr(options, \"build_dir\", None):\n            deprecated(\n                reason=(\n                    \"The -b/--build/--build-dir/--build-directory \"\n                    \"option is deprecated.\"\n                ),\n                replacement=(\n                    \"use the TMPDIR/TEMP/TMP environment variable, \"\n                    \"possibly combined with --no-clean\"\n                ),\n                gone_in=\"20.3\",\n                issue=8333,\n            )\n\n        if 'resolver' in options.unstable_features:\n            logger.critical(\n                \"--unstable-feature=resolver is no longer supported, and \"\n                \"has been replaced with --use-feature=2020-resolver instead.\"\n            )\n            sys.exit(ERROR)\n\n        try:\n            status = self.run(options, args)\n            assert isinstance(status, int)\n            return status\n        except PreviousBuildDirError as exc:\n            logger.critical(str(exc))\n            logger.debug('Exception information:', exc_info=True)\n\n            return PREVIOUS_BUILD_DIR_ERROR\n        except (InstallationError, UninstallationError, BadCommand,\n                SubProcessError, NetworkConnectionError) as exc:\n            logger.critical(str(exc))\n            logger.debug('Exception information:', exc_info=True)\n\n            return ERROR\n        except CommandError as exc:\n            logger.critical('%s', exc)\n            logger.debug('Exception information:', exc_info=True)\n\n            return ERROR\n        except BrokenStdoutLoggingError:\n            # Bypass our logger and write any remaining messages to stderr\n            # because stdout no longer works.\n            print('ERROR: Pipe to stdout was broken', file=sys.stderr)\n            if level_number <= logging.DEBUG:\n                traceback.print_exc(file=sys.stderr)\n\n            return ERROR\n        except KeyboardInterrupt:\n            logger.critical('Operation cancelled by user')\n            logger.debug('Exception information:', exc_info=True)\n\n            return ERROR\n        except BaseException:\n            logger.critical('Exception:', exc_info=True)\n\n            return UNKNOWN_ERROR\n        finally:\n            self.handle_pip_version_check(options)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pip/_internal/cli/base_command.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pip/_internal/cli/base_command.py	(date 1602088700393)
@@ -158,7 +158,19 @@
                     "1st, 2020. Please upgrade your Python as Python 2.7 "
                     "is no longer maintained. "
                 ) + message
-            deprecated(message, replacement=None, gone_in=None)
+            deprecated(message, replacement=None, gone_in="21.0")
+
+        if (
+            sys.version_info[:2] == (3, 5) and
+            not options.no_python_version_warning
+        ):
+            message = (
+                "Python 3.5 reached the end of its life on September "
+                "13th, 2020. Please upgrade your Python as Python 3.5 "
+                "is no longer maintained. pip 21.0 will drop support "
+                "for Python 3.5 in January 2021."
+            )
+            deprecated(message, replacement=None, gone_in="21.0")
 
         # TODO: Try to get these passing down from the command?
         #       without resorting to os.environ to hold these.
Index: env/lib/python3.8/site-packages/setuptools/installer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import glob\nimport os\nimport subprocess\nimport sys\nfrom distutils import log\nfrom distutils.errors import DistutilsError\n\nimport pkg_resources\nfrom setuptools.command.easy_install import easy_install\nfrom setuptools.extern import six\nfrom setuptools.wheel import Wheel\n\nfrom .py31compat import TemporaryDirectory\n\n\ndef _fixup_find_links(find_links):\n    \"\"\"Ensure find-links option end-up being a list of strings.\"\"\"\n    if isinstance(find_links, six.string_types):\n        return find_links.split()\n    assert isinstance(find_links, (tuple, list))\n    return find_links\n\n\ndef _legacy_fetch_build_egg(dist, req):\n    \"\"\"Fetch an egg needed for building.\n\n    Legacy path using EasyInstall.\n    \"\"\"\n    tmp_dist = dist.__class__({'script_args': ['easy_install']})\n    opts = tmp_dist.get_option_dict('easy_install')\n    opts.clear()\n    opts.update(\n        (k, v)\n        for k, v in dist.get_option_dict('easy_install').items()\n        if k in (\n            # don't use any other settings\n            'find_links', 'site_dirs', 'index_url',\n            'optimize', 'site_dirs', 'allow_hosts',\n        ))\n    if dist.dependency_links:\n        links = dist.dependency_links[:]\n        if 'find_links' in opts:\n            links = _fixup_find_links(opts['find_links'][1]) + links\n        opts['find_links'] = ('setup', links)\n    install_dir = dist.get_egg_cache_dir()\n    cmd = easy_install(\n        tmp_dist, args=[\"x\"], install_dir=install_dir,\n        exclude_scripts=True,\n        always_copy=False, build_directory=None, editable=False,\n        upgrade=False, multi_version=True, no_report=True, user=False\n    )\n    cmd.ensure_finalized()\n    return cmd.easy_install(req)\n\n\ndef fetch_build_egg(dist, req):\n    \"\"\"Fetch an egg needed for building.\n\n    Use pip/wheel to fetch/build a wheel.\"\"\"\n    # Check pip is available.\n    try:\n        pkg_resources.get_distribution('pip')\n    except pkg_resources.DistributionNotFound:\n        dist.announce(\n            'WARNING: The pip package is not available, falling back '\n            'to EasyInstall for handling setup_requires/test_requires; '\n            'this is deprecated and will be removed in a future version.',\n            log.WARN\n        )\n        return _legacy_fetch_build_egg(dist, req)\n    # Warn if wheel is not.\n    try:\n        pkg_resources.get_distribution('wheel')\n    except pkg_resources.DistributionNotFound:\n        dist.announce('WARNING: The wheel package is not available.', log.WARN)\n    # Ignore environment markers; if supplied, it is required.\n    req = strip_marker(req)\n    # Take easy_install options into account, but do not override relevant\n    # pip environment variables (like PIP_INDEX_URL or PIP_QUIET); they'll\n    # take precedence.\n    opts = dist.get_option_dict('easy_install')\n    if 'allow_hosts' in opts:\n        raise DistutilsError('the `allow-hosts` option is not supported '\n                             'when using pip to install requirements.')\n    if 'PIP_QUIET' in os.environ or 'PIP_VERBOSE' in os.environ:\n        quiet = False\n    else:\n        quiet = True\n    if 'PIP_INDEX_URL' in os.environ:\n        index_url = None\n    elif 'index_url' in opts:\n        index_url = opts['index_url'][1]\n    else:\n        index_url = None\n    if 'find_links' in opts:\n        find_links = _fixup_find_links(opts['find_links'][1])[:]\n    else:\n        find_links = []\n    if dist.dependency_links:\n        find_links.extend(dist.dependency_links)\n    eggs_dir = os.path.realpath(dist.get_egg_cache_dir())\n    environment = pkg_resources.Environment()\n    for egg_dist in pkg_resources.find_distributions(eggs_dir):\n        if egg_dist in req and environment.can_add(egg_dist):\n            return egg_dist\n    with TemporaryDirectory() as tmpdir:\n        cmd = [\n            sys.executable, '-m', 'pip',\n            '--disable-pip-version-check',\n            'wheel', '--no-deps',\n            '-w', tmpdir,\n        ]\n        if quiet:\n            cmd.append('--quiet')\n        if index_url is not None:\n            cmd.extend(('--index-url', index_url))\n        if find_links is not None:\n            for link in find_links:\n                cmd.extend(('--find-links', link))\n        # If requirement is a PEP 508 direct URL, directly pass\n        # the URL to pip, as `req @ url` does not work on the\n        # command line.\n        if req.url:\n            cmd.append(req.url)\n        else:\n            cmd.append(str(req))\n        try:\n            subprocess.check_call(cmd)\n        except subprocess.CalledProcessError as e:\n            raise DistutilsError(str(e)) from e\n        wheel = Wheel(glob.glob(os.path.join(tmpdir, '*.whl'))[0])\n        dist_location = os.path.join(eggs_dir, wheel.egg_name())\n        wheel.install_as_egg(dist_location)\n        dist_metadata = pkg_resources.PathMetadata(\n            dist_location, os.path.join(dist_location, 'EGG-INFO'))\n        dist = pkg_resources.Distribution.from_filename(\n            dist_location, metadata=dist_metadata)\n        return dist\n\n\ndef strip_marker(req):\n    \"\"\"\n    Return a new requirement without the environment marker to avoid\n    calling pip with something like `babel; extra == \"i18n\"`, which\n    would always be ignored.\n    \"\"\"\n    # create a copy to avoid mutating the input\n    req = pkg_resources.Requirement.parse(str(req))\n    req.marker = None\n    return req\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/installer.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/installer.py	(date 1602088701601)
@@ -2,20 +2,18 @@
 import os
 import subprocess
 import sys
+import tempfile
 from distutils import log
 from distutils.errors import DistutilsError
 
 import pkg_resources
 from setuptools.command.easy_install import easy_install
-from setuptools.extern import six
 from setuptools.wheel import Wheel
 
-from .py31compat import TemporaryDirectory
-
 
 def _fixup_find_links(find_links):
     """Ensure find-links option end-up being a list of strings."""
-    if isinstance(find_links, six.string_types):
+    if isinstance(find_links, str):
         return find_links.split()
     assert isinstance(find_links, (tuple, list))
     return find_links
@@ -103,7 +101,7 @@
     for egg_dist in pkg_resources.find_distributions(eggs_dir):
         if egg_dist in req and environment.can_add(egg_dist):
             return egg_dist
-    with TemporaryDirectory() as tmpdir:
+    with tempfile.TemporaryDirectory() as tmpdir:
         cmd = [
             sys.executable, '-m', 'pip',
             '--disable-pip-version-check',
Index: env/lib/python3.8/site-packages/pip/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from pip._internal.utils.typing import MYPY_CHECK_RUNNING\n\nif MYPY_CHECK_RUNNING:\n    from typing import List, Optional\n\n\n__version__ = \"20.2.1\"\n\n\ndef main(args=None):\n    # type: (Optional[List[str]]) -> int\n    \"\"\"This is an internal API only meant for use by pip's own console scripts.\n\n    For additional details, see https://github.com/pypa/pip/issues/7498.\n    \"\"\"\n    from pip._internal.utils.entrypoints import _wrapper\n\n    return _wrapper(args)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pip/__init__.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pip/__init__.py	(date 1602088700389)
@@ -4,7 +4,7 @@
     from typing import List, Optional
 
 
-__version__ = "20.2.1"
+__version__ = "20.2.3"
 
 
 def main(args=None):
Index: env/lib/python3.8/site-packages/setuptools/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"Extensions to the 'distutils' for large or complex distributions\"\"\"\n\nimport os\nimport functools\n\n# Disabled for now due to: #2228, #2230\nimport setuptools.distutils_patch  # noqa: F401\n\nimport distutils.core\nimport distutils.filelist\nimport re\nfrom distutils.errors import DistutilsOptionError\nfrom distutils.util import convert_path\nfrom fnmatch import fnmatchcase\n\nfrom ._deprecation_warning import SetuptoolsDeprecationWarning\n\nfrom setuptools.extern.six import PY3, string_types\nfrom setuptools.extern.six.moves import filter, map\n\nimport setuptools.version\nfrom setuptools.extension import Extension\nfrom setuptools.dist import Distribution\nfrom setuptools.depends import Require\nfrom . import monkey\n\n__metaclass__ = type\n\n\n__all__ = [\n    'setup', 'Distribution', 'Command', 'Extension', 'Require',\n    'SetuptoolsDeprecationWarning',\n    'find_packages'\n]\n\nif PY3:\n    __all__.append('find_namespace_packages')\n\n__version__ = setuptools.version.__version__\n\nbootstrap_install_from = None\n\n# If we run 2to3 on .py files, should we also convert docstrings?\n# Default: yes; assume that we can detect doctests reliably\nrun_2to3_on_doctests = True\n# Standard package names for fixer packages\nlib2to3_fixer_packages = ['lib2to3.fixes']\n\n\nclass PackageFinder:\n    \"\"\"\n    Generate a list of all Python packages found within a directory\n    \"\"\"\n\n    @classmethod\n    def find(cls, where='.', exclude=(), include=('*',)):\n        \"\"\"Return a list all Python packages found within directory 'where'\n\n        'where' is the root directory which will be searched for packages.  It\n        should be supplied as a \"cross-platform\" (i.e. URL-style) path; it will\n        be converted to the appropriate local path syntax.\n\n        'exclude' is a sequence of package names to exclude; '*' can be used\n        as a wildcard in the names, such that 'foo.*' will exclude all\n        subpackages of 'foo' (but not 'foo' itself).\n\n        'include' is a sequence of package names to include.  If it's\n        specified, only the named packages will be included.  If it's not\n        specified, all found packages will be included.  'include' can contain\n        shell style wildcard patterns just like 'exclude'.\n        \"\"\"\n\n        return list(cls._find_packages_iter(\n            convert_path(where),\n            cls._build_filter('ez_setup', '*__pycache__', *exclude),\n            cls._build_filter(*include)))\n\n    @classmethod\n    def _find_packages_iter(cls, where, exclude, include):\n        \"\"\"\n        All the packages found in 'where' that pass the 'include' filter, but\n        not the 'exclude' filter.\n        \"\"\"\n        for root, dirs, files in os.walk(where, followlinks=True):\n            # Copy dirs to iterate over it, then empty dirs.\n            all_dirs = dirs[:]\n            dirs[:] = []\n\n            for dir in all_dirs:\n                full_path = os.path.join(root, dir)\n                rel_path = os.path.relpath(full_path, where)\n                package = rel_path.replace(os.path.sep, '.')\n\n                # Skip directory trees that are not valid packages\n                if ('.' in dir or not cls._looks_like_package(full_path)):\n                    continue\n\n                # Should this package be included?\n                if include(package) and not exclude(package):\n                    yield package\n\n                # Keep searching subdirectories, as there may be more packages\n                # down there, even if the parent was excluded.\n                dirs.append(dir)\n\n    @staticmethod\n    def _looks_like_package(path):\n        \"\"\"Does a directory look like a package?\"\"\"\n        return os.path.isfile(os.path.join(path, '__init__.py'))\n\n    @staticmethod\n    def _build_filter(*patterns):\n        \"\"\"\n        Given a list of patterns, return a callable that will be true only if\n        the input matches at least one of the patterns.\n        \"\"\"\n        return lambda name: any(fnmatchcase(name, pat=pat) for pat in patterns)\n\n\nclass PEP420PackageFinder(PackageFinder):\n    @staticmethod\n    def _looks_like_package(path):\n        return True\n\n\nfind_packages = PackageFinder.find\n\nif PY3:\n    find_namespace_packages = PEP420PackageFinder.find\n\n\ndef _install_setup_requires(attrs):\n    # Note: do not use `setuptools.Distribution` directly, as\n    # our PEP 517 backend patch `distutils.core.Distribution`.\n    class MinimalDistribution(distutils.core.Distribution):\n        \"\"\"\n        A minimal version of a distribution for supporting the\n        fetch_build_eggs interface.\n        \"\"\"\n        def __init__(self, attrs):\n            _incl = 'dependency_links', 'setup_requires'\n            filtered = {\n                k: attrs[k]\n                for k in set(_incl) & set(attrs)\n            }\n            distutils.core.Distribution.__init__(self, filtered)\n\n        def finalize_options(self):\n            \"\"\"\n            Disable finalize_options to avoid building the working set.\n            Ref #2158.\n            \"\"\"\n\n    dist = MinimalDistribution(attrs)\n\n    # Honor setup.cfg's options.\n    dist.parse_config_files(ignore_option_errors=True)\n    if dist.setup_requires:\n        dist.fetch_build_eggs(dist.setup_requires)\n\n\ndef setup(**attrs):\n    # Make sure we have any requirements needed to interpret 'attrs'.\n    _install_setup_requires(attrs)\n    return distutils.core.setup(**attrs)\n\n\nsetup.__doc__ = distutils.core.setup.__doc__\n\n\n_Command = monkey.get_unpatched(distutils.core.Command)\n\n\nclass Command(_Command):\n    __doc__ = _Command.__doc__\n\n    command_consumes_arguments = False\n\n    def __init__(self, dist, **kw):\n        \"\"\"\n        Construct the command for dist, updating\n        vars(self) with any keyword parameters.\n        \"\"\"\n        _Command.__init__(self, dist)\n        vars(self).update(kw)\n\n    def _ensure_stringlike(self, option, what, default=None):\n        val = getattr(self, option)\n        if val is None:\n            setattr(self, option, default)\n            return default\n        elif not isinstance(val, string_types):\n            raise DistutilsOptionError(\"'%s' must be a %s (got `%s`)\"\n                                       % (option, what, val))\n        return val\n\n    def ensure_string_list(self, option):\n        r\"\"\"Ensure that 'option' is a list of strings.  If 'option' is\n        currently a string, we split it either on /,\\s*/ or /\\s+/, so\n        \"foo bar baz\", \"foo,bar,baz\", and \"foo,   bar baz\" all become\n        [\"foo\", \"bar\", \"baz\"].\n        \"\"\"\n        val = getattr(self, option)\n        if val is None:\n            return\n        elif isinstance(val, string_types):\n            setattr(self, option, re.split(r',\\s*|\\s+', val))\n        else:\n            if isinstance(val, list):\n                ok = all(isinstance(v, string_types) for v in val)\n            else:\n                ok = False\n            if not ok:\n                raise DistutilsOptionError(\n                    \"'%s' must be a list of strings (got %r)\"\n                    % (option, val))\n\n    def reinitialize_command(self, command, reinit_subcommands=0, **kw):\n        cmd = _Command.reinitialize_command(self, command, reinit_subcommands)\n        vars(cmd).update(kw)\n        return cmd\n\n\ndef _find_all_simple(path):\n    \"\"\"\n    Find all files under 'path'\n    \"\"\"\n    results = (\n        os.path.join(base, file)\n        for base, dirs, files in os.walk(path, followlinks=True)\n        for file in files\n    )\n    return filter(os.path.isfile, results)\n\n\ndef findall(dir=os.curdir):\n    \"\"\"\n    Find all files under 'dir' and return the list of full filenames.\n    Unless dir is '.', return full filenames with dir prepended.\n    \"\"\"\n    files = _find_all_simple(dir)\n    if dir == os.curdir:\n        make_rel = functools.partial(os.path.relpath, start=dir)\n        files = map(make_rel, files)\n    return list(files)\n\n\nclass sic(str):\n    \"\"\"Treat this string as-is (https://en.wikipedia.org/wiki/Sic)\"\"\"\n\n\n# Apply monkey patches\nmonkey.patch_all()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/__init__.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/__init__.py	(date 1602088701593)
@@ -1,40 +1,30 @@
 """Extensions to the 'distutils' for large or complex distributions"""
 
-import os
+from fnmatch import fnmatchcase
 import functools
-
-# Disabled for now due to: #2228, #2230
-import setuptools.distutils_patch  # noqa: F401
-
-import distutils.core
-import distutils.filelist
+import os
 import re
+
+import _distutils_hack.override  # noqa: F401
+
+import distutils.core
 from distutils.errors import DistutilsOptionError
 from distutils.util import convert_path
-from fnmatch import fnmatchcase
 
 from ._deprecation_warning import SetuptoolsDeprecationWarning
 
-from setuptools.extern.six import PY3, string_types
-from setuptools.extern.six.moves import filter, map
-
 import setuptools.version
 from setuptools.extension import Extension
 from setuptools.dist import Distribution
 from setuptools.depends import Require
 from . import monkey
 
-__metaclass__ = type
-
 
 __all__ = [
     'setup', 'Distribution', 'Command', 'Extension', 'Require',
     'SetuptoolsDeprecationWarning',
-    'find_packages'
+    'find_packages', 'find_namespace_packages',
 ]
-
-if PY3:
-    __all__.append('find_namespace_packages')
 
 __version__ = setuptools.version.__version__
 
@@ -124,9 +114,7 @@
 
 
 find_packages = PackageFinder.find
-
-if PY3:
-    find_namespace_packages = PEP420PackageFinder.find
+find_namespace_packages = PEP420PackageFinder.find
 
 
 def _install_setup_requires(attrs):
@@ -189,7 +177,7 @@
         if val is None:
             setattr(self, option, default)
             return default
-        elif not isinstance(val, string_types):
+        elif not isinstance(val, str):
             raise DistutilsOptionError("'%s' must be a %s (got `%s`)"
                                        % (option, what, val))
         return val
@@ -203,11 +191,11 @@
         val = getattr(self, option)
         if val is None:
             return
-        elif isinstance(val, string_types):
+        elif isinstance(val, str):
             setattr(self, option, re.split(r',\s*|\s+', val))
         else:
             if isinstance(val, list):
-                ok = all(isinstance(v, string_types) for v in val)
+                ok = all(isinstance(v, str) for v in val)
             else:
                 ok = False
             if not ok:
Index: env/lib/python3.8/site-packages/setuptools/package_index.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"PyPI and direct package downloading\"\"\"\nimport sys\nimport os\nimport re\nimport shutil\nimport socket\nimport base64\nimport hashlib\nimport itertools\nimport warnings\nfrom functools import wraps\n\nfrom setuptools.extern import six\nfrom setuptools.extern.six.moves import urllib, http_client, configparser, map\n\nimport setuptools\nfrom pkg_resources import (\n    CHECKOUT_DIST, Distribution, BINARY_DIST, normalize_path, SOURCE_DIST,\n    Environment, find_distributions, safe_name, safe_version,\n    to_filename, Requirement, DEVELOP_DIST, EGG_DIST,\n)\nfrom setuptools import ssl_support\nfrom distutils import log\nfrom distutils.errors import DistutilsError\nfrom fnmatch import translate\nfrom setuptools.py27compat import get_all_headers\nfrom setuptools.py33compat import unescape\nfrom setuptools.wheel import Wheel\n\n__metaclass__ = type\n\nEGG_FRAGMENT = re.compile(r'^egg=([-A-Za-z0-9_.+!]+)$')\nHREF = re.compile(r\"\"\"href\\s*=\\s*['\"]?([^'\"> ]+)\"\"\", re.I)\nPYPI_MD5 = re.compile(\n    r'<a href=\"([^\"#]+)\">([^<]+)</a>\\n\\s+\\(<a (?:title=\"MD5 hash\"\\n\\s+)'\n    r'href=\"[^?]+\\?:action=show_md5&amp;digest=([0-9a-f]{32})\">md5</a>\\)'\n)\nURL_SCHEME = re.compile('([-+.a-z0-9]{2,}):', re.I).match\nEXTENSIONS = \".tar.gz .tar.bz2 .tar .zip .tgz\".split()\n\n__all__ = [\n    'PackageIndex', 'distros_for_url', 'parse_bdist_wininst',\n    'interpret_distro_name',\n]\n\n_SOCKET_TIMEOUT = 15\n\n_tmpl = \"setuptools/{setuptools.__version__} Python-urllib/{py_major}\"\nuser_agent = _tmpl.format(\n    py_major='{}.{}'.format(*sys.version_info), setuptools=setuptools)\n\n\ndef parse_requirement_arg(spec):\n    try:\n        return Requirement.parse(spec)\n    except ValueError as e:\n        raise DistutilsError(\n            \"Not a URL, existing file, or requirement spec: %r\" % (spec,)\n        ) from e\n\n\ndef parse_bdist_wininst(name):\n    \"\"\"Return (base,pyversion) or (None,None) for possible .exe name\"\"\"\n\n    lower = name.lower()\n    base, py_ver, plat = None, None, None\n\n    if lower.endswith('.exe'):\n        if lower.endswith('.win32.exe'):\n            base = name[:-10]\n            plat = 'win32'\n        elif lower.startswith('.win32-py', -16):\n            py_ver = name[-7:-4]\n            base = name[:-16]\n            plat = 'win32'\n        elif lower.endswith('.win-amd64.exe'):\n            base = name[:-14]\n            plat = 'win-amd64'\n        elif lower.startswith('.win-amd64-py', -20):\n            py_ver = name[-7:-4]\n            base = name[:-20]\n            plat = 'win-amd64'\n    return base, py_ver, plat\n\n\ndef egg_info_for_url(url):\n    parts = urllib.parse.urlparse(url)\n    scheme, server, path, parameters, query, fragment = parts\n    base = urllib.parse.unquote(path.split('/')[-1])\n    if server == 'sourceforge.net' and base == 'download':  # XXX Yuck\n        base = urllib.parse.unquote(path.split('/')[-2])\n    if '#' in base:\n        base, fragment = base.split('#', 1)\n    return base, fragment\n\n\ndef distros_for_url(url, metadata=None):\n    \"\"\"Yield egg or source distribution objects that might be found at a URL\"\"\"\n    base, fragment = egg_info_for_url(url)\n    for dist in distros_for_location(url, base, metadata):\n        yield dist\n    if fragment:\n        match = EGG_FRAGMENT.match(fragment)\n        if match:\n            for dist in interpret_distro_name(\n                url, match.group(1), metadata, precedence=CHECKOUT_DIST\n            ):\n                yield dist\n\n\ndef distros_for_location(location, basename, metadata=None):\n    \"\"\"Yield egg or source distribution objects based on basename\"\"\"\n    if basename.endswith('.egg.zip'):\n        basename = basename[:-4]  # strip the .zip\n    if basename.endswith('.egg') and '-' in basename:\n        # only one, unambiguous interpretation\n        return [Distribution.from_location(location, basename, metadata)]\n    if basename.endswith('.whl') and '-' in basename:\n        wheel = Wheel(basename)\n        if not wheel.is_compatible():\n            return []\n        return [Distribution(\n            location=location,\n            project_name=wheel.project_name,\n            version=wheel.version,\n            # Increase priority over eggs.\n            precedence=EGG_DIST + 1,\n        )]\n    if basename.endswith('.exe'):\n        win_base, py_ver, platform = parse_bdist_wininst(basename)\n        if win_base is not None:\n            return interpret_distro_name(\n                location, win_base, metadata, py_ver, BINARY_DIST, platform\n            )\n    # Try source distro extensions (.zip, .tgz, etc.)\n    #\n    for ext in EXTENSIONS:\n        if basename.endswith(ext):\n            basename = basename[:-len(ext)]\n            return interpret_distro_name(location, basename, metadata)\n    return []  # no extension matched\n\n\ndef distros_for_filename(filename, metadata=None):\n    \"\"\"Yield possible egg or source distribution objects based on a filename\"\"\"\n    return distros_for_location(\n        normalize_path(filename), os.path.basename(filename), metadata\n    )\n\n\ndef interpret_distro_name(\n        location, basename, metadata, py_version=None, precedence=SOURCE_DIST,\n        platform=None\n):\n    \"\"\"Generate alternative interpretations of a source distro name\n\n    Note: if `location` is a filesystem filename, you should call\n    ``pkg_resources.normalize_path()`` on it before passing it to this\n    routine!\n    \"\"\"\n    # Generate alternative interpretations of a source distro name\n    # Because some packages are ambiguous as to name/versions split\n    # e.g. \"adns-python-1.1.0\", \"egenix-mx-commercial\", etc.\n    # So, we generate each possible interepretation (e.g. \"adns, python-1.1.0\"\n    # \"adns-python, 1.1.0\", and \"adns-python-1.1.0, no version\").  In practice,\n    # the spurious interpretations should be ignored, because in the event\n    # there's also an \"adns\" package, the spurious \"python-1.1.0\" version will\n    # compare lower than any numeric version number, and is therefore unlikely\n    # to match a request for it.  It's still a potential problem, though, and\n    # in the long run PyPI and the distutils should go for \"safe\" names and\n    # versions in distribution archive names (sdist and bdist).\n\n    parts = basename.split('-')\n    if not py_version and any(re.match(r'py\\d\\.\\d$', p) for p in parts[2:]):\n        # it is a bdist_dumb, not an sdist -- bail out\n        return\n\n    for p in range(1, len(parts) + 1):\n        yield Distribution(\n            location, metadata, '-'.join(parts[:p]), '-'.join(parts[p:]),\n            py_version=py_version, precedence=precedence,\n            platform=platform\n        )\n\n\n# From Python 2.7 docs\ndef unique_everseen(iterable, key=None):\n    \"List unique elements, preserving order. Remember all elements ever seen.\"\n    # unique_everseen('AAAABBBCCDAABBB') --> A B C D\n    # unique_everseen('ABBCcAD', str.lower) --> A B C D\n    seen = set()\n    seen_add = seen.add\n    if key is None:\n        for element in six.moves.filterfalse(seen.__contains__, iterable):\n            seen_add(element)\n            yield element\n    else:\n        for element in iterable:\n            k = key(element)\n            if k not in seen:\n                seen_add(k)\n                yield element\n\n\ndef unique_values(func):\n    \"\"\"\n    Wrap a function returning an iterable such that the resulting iterable\n    only ever yields unique items.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        return unique_everseen(func(*args, **kwargs))\n\n    return wrapper\n\n\nREL = re.compile(r\"\"\"<([^>]*\\srel\\s*=\\s*['\"]?([^'\">]+)[^>]*)>\"\"\", re.I)\n# this line is here to fix emacs' cruddy broken syntax highlighting\n\n\n@unique_values\ndef find_external_links(url, page):\n    \"\"\"Find rel=\"homepage\" and rel=\"download\" links in `page`, yielding URLs\"\"\"\n\n    for match in REL.finditer(page):\n        tag, rel = match.groups()\n        rels = set(map(str.strip, rel.lower().split(',')))\n        if 'homepage' in rels or 'download' in rels:\n            for match in HREF.finditer(tag):\n                yield urllib.parse.urljoin(url, htmldecode(match.group(1)))\n\n    for tag in (\"<th>Home Page\", \"<th>Download URL\"):\n        pos = page.find(tag)\n        if pos != -1:\n            match = HREF.search(page, pos)\n            if match:\n                yield urllib.parse.urljoin(url, htmldecode(match.group(1)))\n\n\nclass ContentChecker:\n    \"\"\"\n    A null content checker that defines the interface for checking content\n    \"\"\"\n\n    def feed(self, block):\n        \"\"\"\n        Feed a block of data to the hash.\n        \"\"\"\n        return\n\n    def is_valid(self):\n        \"\"\"\n        Check the hash. Return False if validation fails.\n        \"\"\"\n        return True\n\n    def report(self, reporter, template):\n        \"\"\"\n        Call reporter with information about the checker (hash name)\n        substituted into the template.\n        \"\"\"\n        return\n\n\nclass HashChecker(ContentChecker):\n    pattern = re.compile(\n        r'(?P<hash_name>sha1|sha224|sha384|sha256|sha512|md5)='\n        r'(?P<expected>[a-f0-9]+)'\n    )\n\n    def __init__(self, hash_name, expected):\n        self.hash_name = hash_name\n        self.hash = hashlib.new(hash_name)\n        self.expected = expected\n\n    @classmethod\n    def from_url(cls, url):\n        \"Construct a (possibly null) ContentChecker from a URL\"\n        fragment = urllib.parse.urlparse(url)[-1]\n        if not fragment:\n            return ContentChecker()\n        match = cls.pattern.search(fragment)\n        if not match:\n            return ContentChecker()\n        return cls(**match.groupdict())\n\n    def feed(self, block):\n        self.hash.update(block)\n\n    def is_valid(self):\n        return self.hash.hexdigest() == self.expected\n\n    def report(self, reporter, template):\n        msg = template % self.hash_name\n        return reporter(msg)\n\n\nclass PackageIndex(Environment):\n    \"\"\"A distribution index that scans web pages for download URLs\"\"\"\n\n    def __init__(\n            self, index_url=\"https://pypi.org/simple/\", hosts=('*',),\n            ca_bundle=None, verify_ssl=True, *args, **kw\n    ):\n        Environment.__init__(self, *args, **kw)\n        self.index_url = index_url + \"/\" [:not index_url.endswith('/')]\n        self.scanned_urls = {}\n        self.fetched_urls = {}\n        self.package_pages = {}\n        self.allows = re.compile('|'.join(map(translate, hosts))).match\n        self.to_scan = []\n        use_ssl = (\n            verify_ssl\n            and ssl_support.is_available\n            and (ca_bundle or ssl_support.find_ca_bundle())\n        )\n        if use_ssl:\n            self.opener = ssl_support.opener_for(ca_bundle)\n        else:\n            self.opener = urllib.request.urlopen\n\n    def process_url(self, url, retrieve=False):\n        \"\"\"Evaluate a URL as a possible download, and maybe retrieve it\"\"\"\n        if url in self.scanned_urls and not retrieve:\n            return\n        self.scanned_urls[url] = True\n        if not URL_SCHEME(url):\n            self.process_filename(url)\n            return\n        else:\n            dists = list(distros_for_url(url))\n            if dists:\n                if not self.url_ok(url):\n                    return\n                self.debug(\"Found link: %s\", url)\n\n        if dists or not retrieve or url in self.fetched_urls:\n            list(map(self.add, dists))\n            return  # don't need the actual page\n\n        if not self.url_ok(url):\n            self.fetched_urls[url] = True\n            return\n\n        self.info(\"Reading %s\", url)\n        self.fetched_urls[url] = True  # prevent multiple fetch attempts\n        tmpl = \"Download error on %s: %%s -- Some packages may not be found!\"\n        f = self.open_url(url, tmpl % url)\n        if f is None:\n            return\n        if isinstance(f, urllib.error.HTTPError) and f.code == 401:\n            self.info(\"Authentication error: %s\" % f.msg)\n        self.fetched_urls[f.url] = True\n        if 'html' not in f.headers.get('content-type', '').lower():\n            f.close()  # not html, we can't process it\n            return\n\n        base = f.url  # handle redirects\n        page = f.read()\n        if not isinstance(page, str):\n            # In Python 3 and got bytes but want str.\n            if isinstance(f, urllib.error.HTTPError):\n                # Errors have no charset, assume latin1:\n                charset = 'latin-1'\n            else:\n                charset = f.headers.get_param('charset') or 'latin-1'\n            page = page.decode(charset, \"ignore\")\n        f.close()\n        for match in HREF.finditer(page):\n            link = urllib.parse.urljoin(base, htmldecode(match.group(1)))\n            self.process_url(link)\n        if url.startswith(self.index_url) and getattr(f, 'code', None) != 404:\n            page = self.process_index(url, page)\n\n    def process_filename(self, fn, nested=False):\n        # process filenames or directories\n        if not os.path.exists(fn):\n            self.warn(\"Not found: %s\", fn)\n            return\n\n        if os.path.isdir(fn) and not nested:\n            path = os.path.realpath(fn)\n            for item in os.listdir(path):\n                self.process_filename(os.path.join(path, item), True)\n\n        dists = distros_for_filename(fn)\n        if dists:\n            self.debug(\"Found: %s\", fn)\n            list(map(self.add, dists))\n\n    def url_ok(self, url, fatal=False):\n        s = URL_SCHEME(url)\n        is_file = s and s.group(1).lower() == 'file'\n        if is_file or self.allows(urllib.parse.urlparse(url)[1]):\n            return True\n        msg = (\n            \"\\nNote: Bypassing %s (disallowed host; see \"\n            \"http://bit.ly/2hrImnY for details).\\n\")\n        if fatal:\n            raise DistutilsError(msg % url)\n        else:\n            self.warn(msg, url)\n\n    def scan_egg_links(self, search_path):\n        dirs = filter(os.path.isdir, search_path)\n        egg_links = (\n            (path, entry)\n            for path in dirs\n            for entry in os.listdir(path)\n            if entry.endswith('.egg-link')\n        )\n        list(itertools.starmap(self.scan_egg_link, egg_links))\n\n    def scan_egg_link(self, path, entry):\n        with open(os.path.join(path, entry)) as raw_lines:\n            # filter non-empty lines\n            lines = list(filter(None, map(str.strip, raw_lines)))\n\n        if len(lines) != 2:\n            # format is not recognized; punt\n            return\n\n        egg_path, setup_path = lines\n\n        for dist in find_distributions(os.path.join(path, egg_path)):\n            dist.location = os.path.join(path, *lines)\n            dist.precedence = SOURCE_DIST\n            self.add(dist)\n\n    def process_index(self, url, page):\n        \"\"\"Process the contents of a PyPI page\"\"\"\n\n        def scan(link):\n            # Process a URL to see if it's for a package page\n            if link.startswith(self.index_url):\n                parts = list(map(\n                    urllib.parse.unquote, link[len(self.index_url):].split('/')\n                ))\n                if len(parts) == 2 and '#' not in parts[1]:\n                    # it's a package page, sanitize and index it\n                    pkg = safe_name(parts[0])\n                    ver = safe_version(parts[1])\n                    self.package_pages.setdefault(pkg.lower(), {})[link] = True\n                    return to_filename(pkg), to_filename(ver)\n            return None, None\n\n        # process an index page into the package-page index\n        for match in HREF.finditer(page):\n            try:\n                scan(urllib.parse.urljoin(url, htmldecode(match.group(1))))\n            except ValueError:\n                pass\n\n        pkg, ver = scan(url)  # ensure this page is in the page index\n        if pkg:\n            # process individual package page\n            for new_url in find_external_links(url, page):\n                # Process the found URL\n                base, frag = egg_info_for_url(new_url)\n                if base.endswith('.py') and not frag:\n                    if ver:\n                        new_url += '#egg=%s-%s' % (pkg, ver)\n                    else:\n                        self.need_version_info(url)\n                self.scan_url(new_url)\n\n            return PYPI_MD5.sub(\n                lambda m: '<a href=\"%s#md5=%s\">%s</a>' % m.group(1, 3, 2), page\n            )\n        else:\n            return \"\"  # no sense double-scanning non-package pages\n\n    def need_version_info(self, url):\n        self.scan_all(\n            \"Page at %s links to .py file(s) without version info; an index \"\n            \"scan is required.\", url\n        )\n\n    def scan_all(self, msg=None, *args):\n        if self.index_url not in self.fetched_urls:\n            if msg:\n                self.warn(msg, *args)\n            self.info(\n                \"Scanning index of all packages (this may take a while)\"\n            )\n        self.scan_url(self.index_url)\n\n    def find_packages(self, requirement):\n        self.scan_url(self.index_url + requirement.unsafe_name + '/')\n\n        if not self.package_pages.get(requirement.key):\n            # Fall back to safe version of the name\n            self.scan_url(self.index_url + requirement.project_name + '/')\n\n        if not self.package_pages.get(requirement.key):\n            # We couldn't find the target package, so search the index page too\n            self.not_found_in_index(requirement)\n\n        for url in list(self.package_pages.get(requirement.key, ())):\n            # scan each page that might be related to the desired package\n            self.scan_url(url)\n\n    def obtain(self, requirement, installer=None):\n        self.prescan()\n        self.find_packages(requirement)\n        for dist in self[requirement.key]:\n            if dist in requirement:\n                return dist\n            self.debug(\"%s does not match %s\", requirement, dist)\n        return super(PackageIndex, self).obtain(requirement, installer)\n\n    def check_hash(self, checker, filename, tfp):\n        \"\"\"\n        checker is a ContentChecker\n        \"\"\"\n        checker.report(\n            self.debug,\n            \"Validating %%s checksum for %s\" % filename)\n        if not checker.is_valid():\n            tfp.close()\n            os.unlink(filename)\n            raise DistutilsError(\n                \"%s validation failed for %s; \"\n                \"possible download problem?\"\n                % (checker.hash.name, os.path.basename(filename))\n            )\n\n    def add_find_links(self, urls):\n        \"\"\"Add `urls` to the list that will be prescanned for searches\"\"\"\n        for url in urls:\n            if (\n                self.to_scan is None  # if we have already \"gone online\"\n                or not URL_SCHEME(url)  # or it's a local file/directory\n                or url.startswith('file:')\n                or list(distros_for_url(url))  # or a direct package link\n            ):\n                # then go ahead and process it now\n                self.scan_url(url)\n            else:\n                # otherwise, defer retrieval till later\n                self.to_scan.append(url)\n\n    def prescan(self):\n        \"\"\"Scan urls scheduled for prescanning (e.g. --find-links)\"\"\"\n        if self.to_scan:\n            list(map(self.scan_url, self.to_scan))\n        self.to_scan = None  # from now on, go ahead and process immediately\n\n    def not_found_in_index(self, requirement):\n        if self[requirement.key]:  # we've seen at least one distro\n            meth, msg = self.info, \"Couldn't retrieve index page for %r\"\n        else:  # no distros seen for this name, might be misspelled\n            meth, msg = (\n                self.warn,\n                \"Couldn't find index page for %r (maybe misspelled?)\")\n        meth(msg, requirement.unsafe_name)\n        self.scan_all()\n\n    def download(self, spec, tmpdir):\n        \"\"\"Locate and/or download `spec` to `tmpdir`, returning a local path\n\n        `spec` may be a ``Requirement`` object, or a string containing a URL,\n        an existing local filename, or a project/version requirement spec\n        (i.e. the string form of a ``Requirement`` object).  If it is the URL\n        of a .py file with an unambiguous ``#egg=name-version`` tag (i.e., one\n        that escapes ``-`` as ``_`` throughout), a trivial ``setup.py`` is\n        automatically created alongside the downloaded file.\n\n        If `spec` is a ``Requirement`` object or a string containing a\n        project/version requirement spec, this method returns the location of\n        a matching distribution (possibly after downloading it to `tmpdir`).\n        If `spec` is a locally existing file or directory name, it is simply\n        returned unchanged.  If `spec` is a URL, it is downloaded to a subpath\n        of `tmpdir`, and the local filename is returned.  Various errors may be\n        raised if a problem occurs during downloading.\n        \"\"\"\n        if not isinstance(spec, Requirement):\n            scheme = URL_SCHEME(spec)\n            if scheme:\n                # It's a url, download it to tmpdir\n                found = self._download_url(scheme.group(1), spec, tmpdir)\n                base, fragment = egg_info_for_url(spec)\n                if base.endswith('.py'):\n                    found = self.gen_setup(found, fragment, tmpdir)\n                return found\n            elif os.path.exists(spec):\n                # Existing file or directory, just return it\n                return spec\n            else:\n                spec = parse_requirement_arg(spec)\n        return getattr(self.fetch_distribution(spec, tmpdir), 'location', None)\n\n    def fetch_distribution(\n            self, requirement, tmpdir, force_scan=False, source=False,\n            develop_ok=False, local_index=None):\n        \"\"\"Obtain a distribution suitable for fulfilling `requirement`\n\n        `requirement` must be a ``pkg_resources.Requirement`` instance.\n        If necessary, or if the `force_scan` flag is set, the requirement is\n        searched for in the (online) package index as well as the locally\n        installed packages.  If a distribution matching `requirement` is found,\n        the returned distribution's ``location`` is the value you would have\n        gotten from calling the ``download()`` method with the matching\n        distribution's URL or filename.  If no matching distribution is found,\n        ``None`` is returned.\n\n        If the `source` flag is set, only source distributions and source\n        checkout links will be considered.  Unless the `develop_ok` flag is\n        set, development and system eggs (i.e., those using the ``.egg-info``\n        format) will be ignored.\n        \"\"\"\n        # process a Requirement\n        self.info(\"Searching for %s\", requirement)\n        skipped = {}\n        dist = None\n\n        def find(req, env=None):\n            if env is None:\n                env = self\n            # Find a matching distribution; may be called more than once\n\n            for dist in env[req.key]:\n\n                if dist.precedence == DEVELOP_DIST and not develop_ok:\n                    if dist not in skipped:\n                        self.warn(\n                            \"Skipping development or system egg: %s\", dist,\n                        )\n                        skipped[dist] = 1\n                    continue\n\n                test = (\n                    dist in req\n                    and (dist.precedence <= SOURCE_DIST or not source)\n                )\n                if test:\n                    loc = self.download(dist.location, tmpdir)\n                    dist.download_location = loc\n                    if os.path.exists(dist.download_location):\n                        return dist\n\n        if force_scan:\n            self.prescan()\n            self.find_packages(requirement)\n            dist = find(requirement)\n\n        if not dist and local_index is not None:\n            dist = find(requirement, local_index)\n\n        if dist is None:\n            if self.to_scan is not None:\n                self.prescan()\n            dist = find(requirement)\n\n        if dist is None and not force_scan:\n            self.find_packages(requirement)\n            dist = find(requirement)\n\n        if dist is None:\n            self.warn(\n                \"No local packages or working download links found for %s%s\",\n                (source and \"a source distribution of \" or \"\"),\n                requirement,\n            )\n        else:\n            self.info(\"Best match: %s\", dist)\n            return dist.clone(location=dist.download_location)\n\n    def fetch(self, requirement, tmpdir, force_scan=False, source=False):\n        \"\"\"Obtain a file suitable for fulfilling `requirement`\n\n        DEPRECATED; use the ``fetch_distribution()`` method now instead.  For\n        backward compatibility, this routine is identical but returns the\n        ``location`` of the downloaded distribution instead of a distribution\n        object.\n        \"\"\"\n        dist = self.fetch_distribution(requirement, tmpdir, force_scan, source)\n        if dist is not None:\n            return dist.location\n        return None\n\n    def gen_setup(self, filename, fragment, tmpdir):\n        match = EGG_FRAGMENT.match(fragment)\n        dists = match and [\n            d for d in\n            interpret_distro_name(filename, match.group(1), None) if d.version\n        ] or []\n\n        if len(dists) == 1:  # unambiguous ``#egg`` fragment\n            basename = os.path.basename(filename)\n\n            # Make sure the file has been downloaded to the temp dir.\n            if os.path.dirname(filename) != tmpdir:\n                dst = os.path.join(tmpdir, basename)\n                from setuptools.command.easy_install import samefile\n                if not samefile(filename, dst):\n                    shutil.copy2(filename, dst)\n                    filename = dst\n\n            with open(os.path.join(tmpdir, 'setup.py'), 'w') as file:\n                file.write(\n                    \"from setuptools import setup\\n\"\n                    \"setup(name=%r, version=%r, py_modules=[%r])\\n\"\n                    % (\n                        dists[0].project_name, dists[0].version,\n                        os.path.splitext(basename)[0]\n                    )\n                )\n            return filename\n\n        elif match:\n            raise DistutilsError(\n                \"Can't unambiguously interpret project/version identifier %r; \"\n                \"any dashes in the name or version should be escaped using \"\n                \"underscores. %r\" % (fragment, dists)\n            )\n        else:\n            raise DistutilsError(\n                \"Can't process plain .py files without an '#egg=name-version'\"\n                \" suffix to enable automatic setup script generation.\"\n            )\n\n    dl_blocksize = 8192\n\n    def _download_to(self, url, filename):\n        self.info(\"Downloading %s\", url)\n        # Download the file\n        fp = None\n        try:\n            checker = HashChecker.from_url(url)\n            fp = self.open_url(url)\n            if isinstance(fp, urllib.error.HTTPError):\n                raise DistutilsError(\n                    \"Can't download %s: %s %s\" % (url, fp.code, fp.msg)\n                )\n            headers = fp.info()\n            blocknum = 0\n            bs = self.dl_blocksize\n            size = -1\n            if \"content-length\" in headers:\n                # Some servers return multiple Content-Length headers :(\n                sizes = get_all_headers(headers, 'Content-Length')\n                size = max(map(int, sizes))\n                self.reporthook(url, filename, blocknum, bs, size)\n            with open(filename, 'wb') as tfp:\n                while True:\n                    block = fp.read(bs)\n                    if block:\n                        checker.feed(block)\n                        tfp.write(block)\n                        blocknum += 1\n                        self.reporthook(url, filename, blocknum, bs, size)\n                    else:\n                        break\n                self.check_hash(checker, filename, tfp)\n            return headers\n        finally:\n            if fp:\n                fp.close()\n\n    def reporthook(self, url, filename, blocknum, blksize, size):\n        pass  # no-op\n\n    def open_url(self, url, warning=None):\n        if url.startswith('file:'):\n            return local_open(url)\n        try:\n            return open_with_auth(url, self.opener)\n        except (ValueError, http_client.InvalidURL) as v:\n            msg = ' '.join([str(arg) for arg in v.args])\n            if warning:\n                self.warn(warning, msg)\n            else:\n                raise DistutilsError('%s %s' % (url, msg)) from v\n        except urllib.error.HTTPError as v:\n            return v\n        except urllib.error.URLError as v:\n            if warning:\n                self.warn(warning, v.reason)\n            else:\n                raise DistutilsError(\"Download error for %s: %s\"\n                                     % (url, v.reason)) from v\n        except http_client.BadStatusLine as v:\n            if warning:\n                self.warn(warning, v.line)\n            else:\n                raise DistutilsError(\n                    '%s returned a bad status line. The server might be '\n                    'down, %s' %\n                    (url, v.line)\n                ) from v\n        except (http_client.HTTPException, socket.error) as v:\n            if warning:\n                self.warn(warning, v)\n            else:\n                raise DistutilsError(\"Download error for %s: %s\"\n                                     % (url, v)) from v\n\n    def _download_url(self, scheme, url, tmpdir):\n        # Determine download filename\n        #\n        name, fragment = egg_info_for_url(url)\n        if name:\n            while '..' in name:\n                name = name.replace('..', '.').replace('\\\\', '_')\n        else:\n            name = \"__downloaded__\"  # default if URL has no path contents\n\n        if name.endswith('.egg.zip'):\n            name = name[:-4]  # strip the extra .zip before download\n\n        filename = os.path.join(tmpdir, name)\n\n        # Download the file\n        #\n        if scheme == 'svn' or scheme.startswith('svn+'):\n            return self._download_svn(url, filename)\n        elif scheme == 'git' or scheme.startswith('git+'):\n            return self._download_git(url, filename)\n        elif scheme.startswith('hg+'):\n            return self._download_hg(url, filename)\n        elif scheme == 'file':\n            return urllib.request.url2pathname(urllib.parse.urlparse(url)[2])\n        else:\n            self.url_ok(url, True)  # raises error if not allowed\n            return self._attempt_download(url, filename)\n\n    def scan_url(self, url):\n        self.process_url(url, True)\n\n    def _attempt_download(self, url, filename):\n        headers = self._download_to(url, filename)\n        if 'html' in headers.get('content-type', '').lower():\n            return self._download_html(url, headers, filename)\n        else:\n            return filename\n\n    def _download_html(self, url, headers, filename):\n        file = open(filename)\n        for line in file:\n            if line.strip():\n                # Check for a subversion index page\n                if re.search(r'<title>([^- ]+ - )?Revision \\d+:', line):\n                    # it's a subversion index page:\n                    file.close()\n                    os.unlink(filename)\n                    return self._download_svn(url, filename)\n                break  # not an index page\n        file.close()\n        os.unlink(filename)\n        raise DistutilsError(\"Unexpected HTML page found at \" + url)\n\n    def _download_svn(self, url, filename):\n        warnings.warn(\"SVN download support is deprecated\", UserWarning)\n        url = url.split('#', 1)[0]  # remove any fragment for svn's sake\n        creds = ''\n        if url.lower().startswith('svn:') and '@' in url:\n            scheme, netloc, path, p, q, f = urllib.parse.urlparse(url)\n            if not netloc and path.startswith('//') and '/' in path[2:]:\n                netloc, path = path[2:].split('/', 1)\n                auth, host = _splituser(netloc)\n                if auth:\n                    if ':' in auth:\n                        user, pw = auth.split(':', 1)\n                        creds = \" --username=%s --password=%s\" % (user, pw)\n                    else:\n                        creds = \" --username=\" + auth\n                    netloc = host\n                    parts = scheme, netloc, url, p, q, f\n                    url = urllib.parse.urlunparse(parts)\n        self.info(\"Doing subversion checkout from %s to %s\", url, filename)\n        os.system(\"svn checkout%s -q %s %s\" % (creds, url, filename))\n        return filename\n\n    @staticmethod\n    def _vcs_split_rev_from_url(url, pop_prefix=False):\n        scheme, netloc, path, query, frag = urllib.parse.urlsplit(url)\n\n        scheme = scheme.split('+', 1)[-1]\n\n        # Some fragment identification fails\n        path = path.split('#', 1)[0]\n\n        rev = None\n        if '@' in path:\n            path, rev = path.rsplit('@', 1)\n\n        # Also, discard fragment\n        url = urllib.parse.urlunsplit((scheme, netloc, path, query, ''))\n\n        return url, rev\n\n    def _download_git(self, url, filename):\n        filename = filename.split('#', 1)[0]\n        url, rev = self._vcs_split_rev_from_url(url, pop_prefix=True)\n\n        self.info(\"Doing git clone from %s to %s\", url, filename)\n        os.system(\"git clone --quiet %s %s\" % (url, filename))\n\n        if rev is not None:\n            self.info(\"Checking out %s\", rev)\n            os.system(\"git -C %s checkout --quiet %s\" % (\n                filename,\n                rev,\n            ))\n\n        return filename\n\n    def _download_hg(self, url, filename):\n        filename = filename.split('#', 1)[0]\n        url, rev = self._vcs_split_rev_from_url(url, pop_prefix=True)\n\n        self.info(\"Doing hg clone from %s to %s\", url, filename)\n        os.system(\"hg clone --quiet %s %s\" % (url, filename))\n\n        if rev is not None:\n            self.info(\"Updating to %s\", rev)\n            os.system(\"hg --cwd %s up -C -r %s -q\" % (\n                filename,\n                rev,\n            ))\n\n        return filename\n\n    def debug(self, msg, *args):\n        log.debug(msg, *args)\n\n    def info(self, msg, *args):\n        log.info(msg, *args)\n\n    def warn(self, msg, *args):\n        log.warn(msg, *args)\n\n\n# This pattern matches a character entity reference (a decimal numeric\n# references, a hexadecimal numeric reference, or a named reference).\nentity_sub = re.compile(r'&(#(\\d+|x[\\da-fA-F]+)|[\\w.:-]+);?').sub\n\n\ndef decode_entity(match):\n    what = match.group(0)\n    return unescape(what)\n\n\ndef htmldecode(text):\n    \"\"\"\n    Decode HTML entities in the given text.\n\n    >>> htmldecode(\n    ...     'https://../package_name-0.1.2.tar.gz'\n    ...     '?tokena=A&amp;tokenb=B\">package_name-0.1.2.tar.gz')\n    'https://../package_name-0.1.2.tar.gz?tokena=A&tokenb=B\">package_name-0.1.2.tar.gz'\n    \"\"\"\n    return entity_sub(decode_entity, text)\n\n\ndef socket_timeout(timeout=15):\n    def _socket_timeout(func):\n        def _socket_timeout(*args, **kwargs):\n            old_timeout = socket.getdefaulttimeout()\n            socket.setdefaulttimeout(timeout)\n            try:\n                return func(*args, **kwargs)\n            finally:\n                socket.setdefaulttimeout(old_timeout)\n\n        return _socket_timeout\n\n    return _socket_timeout\n\n\ndef _encode_auth(auth):\n    \"\"\"\n    A function compatible with Python 2.3-3.3 that will encode\n    auth from a URL suitable for an HTTP header.\n    >>> str(_encode_auth('username%3Apassword'))\n    'dXNlcm5hbWU6cGFzc3dvcmQ='\n\n    Long auth strings should not cause a newline to be inserted.\n    >>> long_auth = 'username:' + 'password'*10\n    >>> chr(10) in str(_encode_auth(long_auth))\n    False\n    \"\"\"\n    auth_s = urllib.parse.unquote(auth)\n    # convert to bytes\n    auth_bytes = auth_s.encode()\n    encoded_bytes = base64.b64encode(auth_bytes)\n    # convert back to a string\n    encoded = encoded_bytes.decode()\n    # strip the trailing carriage return\n    return encoded.replace('\\n', '')\n\n\nclass Credential:\n    \"\"\"\n    A username/password pair. Use like a namedtuple.\n    \"\"\"\n\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n\n    def __iter__(self):\n        yield self.username\n        yield self.password\n\n    def __str__(self):\n        return '%(username)s:%(password)s' % vars(self)\n\n\nclass PyPIConfig(configparser.RawConfigParser):\n    def __init__(self):\n        \"\"\"\n        Load from ~/.pypirc\n        \"\"\"\n        defaults = dict.fromkeys(['username', 'password', 'repository'], '')\n        configparser.RawConfigParser.__init__(self, defaults)\n\n        rc = os.path.join(os.path.expanduser('~'), '.pypirc')\n        if os.path.exists(rc):\n            self.read(rc)\n\n    @property\n    def creds_by_repository(self):\n        sections_with_repositories = [\n            section for section in self.sections()\n            if self.get(section, 'repository').strip()\n        ]\n\n        return dict(map(self._get_repo_cred, sections_with_repositories))\n\n    def _get_repo_cred(self, section):\n        repo = self.get(section, 'repository').strip()\n        return repo, Credential(\n            self.get(section, 'username').strip(),\n            self.get(section, 'password').strip(),\n        )\n\n    def find_credential(self, url):\n        \"\"\"\n        If the URL indicated appears to be a repository defined in this\n        config, return the credential for that repository.\n        \"\"\"\n        for repository, cred in self.creds_by_repository.items():\n            if url.startswith(repository):\n                return cred\n\n\ndef open_with_auth(url, opener=urllib.request.urlopen):\n    \"\"\"Open a urllib2 request, handling HTTP authentication\"\"\"\n\n    parsed = urllib.parse.urlparse(url)\n    scheme, netloc, path, params, query, frag = parsed\n\n    # Double scheme does not raise on macOS as revealed by a\n    # failing test. We would expect \"nonnumeric port\". Refs #20.\n    if netloc.endswith(':'):\n        raise http_client.InvalidURL(\"nonnumeric port: ''\")\n\n    if scheme in ('http', 'https'):\n        auth, address = _splituser(netloc)\n    else:\n        auth = None\n\n    if not auth:\n        cred = PyPIConfig().find_credential(url)\n        if cred:\n            auth = str(cred)\n            info = cred.username, url\n            log.info('Authenticating as %s for %s (from .pypirc)', *info)\n\n    if auth:\n        auth = \"Basic \" + _encode_auth(auth)\n        parts = scheme, address, path, params, query, frag\n        new_url = urllib.parse.urlunparse(parts)\n        request = urllib.request.Request(new_url)\n        request.add_header(\"Authorization\", auth)\n    else:\n        request = urllib.request.Request(url)\n\n    request.add_header('User-Agent', user_agent)\n    fp = opener(request)\n\n    if auth:\n        # Put authentication info back into request URL if same host,\n        # so that links found on the page will work\n        s2, h2, path2, param2, query2, frag2 = urllib.parse.urlparse(fp.url)\n        if s2 == scheme and h2 == address:\n            parts = s2, netloc, path2, param2, query2, frag2\n            fp.url = urllib.parse.urlunparse(parts)\n\n    return fp\n\n\n# copy of urllib.parse._splituser from Python 3.8\ndef _splituser(host):\n    \"\"\"splituser('user[:passwd]@host[:port]')\n    --> 'user[:passwd]', 'host[:port]'.\"\"\"\n    user, delim, host = host.rpartition('@')\n    return (user if delim else None), host\n\n\n# adding a timeout to avoid freezing package_index\nopen_with_auth = socket_timeout(_SOCKET_TIMEOUT)(open_with_auth)\n\n\ndef fix_sf_url(url):\n    return url  # backward compatibility\n\n\ndef local_open(url):\n    \"\"\"Read a local path, with special support for directories\"\"\"\n    scheme, server, path, param, query, frag = urllib.parse.urlparse(url)\n    filename = urllib.request.url2pathname(path)\n    if os.path.isfile(filename):\n        return urllib.request.urlopen(url)\n    elif path.endswith('/') and os.path.isdir(filename):\n        files = []\n        for f in os.listdir(filename):\n            filepath = os.path.join(filename, f)\n            if f == 'index.html':\n                with open(filepath, 'r') as fp:\n                    body = fp.read()\n                break\n            elif os.path.isdir(filepath):\n                f += '/'\n            files.append('<a href=\"{name}\">{name}</a>'.format(name=f))\n        else:\n            tmpl = (\n                \"<html><head><title>{url}</title>\"\n                \"</head><body>{files}</body></html>\")\n            body = tmpl.format(url=url, files='\\n'.join(files))\n        status, message = 200, \"OK\"\n    else:\n        status, message, body = 404, \"Path not found\", \"Not found\"\n\n    headers = {'content-type': 'text/html'}\n    body_stream = six.StringIO(body)\n    return urllib.error.HTTPError(url, status, message, headers, body_stream)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/package_index.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/package_index.py	(date 1602088701601)
@@ -2,17 +2,21 @@
 import sys
 import os
 import re
+import io
 import shutil
 import socket
 import base64
 import hashlib
 import itertools
 import warnings
+import configparser
+import html
+import http.client
+import urllib.parse
+import urllib.request
+import urllib.error
 from functools import wraps
 
-from setuptools.extern import six
-from setuptools.extern.six.moves import urllib, http_client, configparser, map
-
 import setuptools
 from pkg_resources import (
     CHECKOUT_DIST, Distribution, BINARY_DIST, normalize_path, SOURCE_DIST,
@@ -23,12 +27,8 @@
 from distutils import log
 from distutils.errors import DistutilsError
 from fnmatch import translate
-from setuptools.py27compat import get_all_headers
-from setuptools.py33compat import unescape
 from setuptools.wheel import Wheel
 
-__metaclass__ = type
-
 EGG_FRAGMENT = re.compile(r'^egg=([-A-Za-z0-9_.+!]+)$')
 HREF = re.compile(r"""href\s*=\s*['"]?([^'"> ]+)""", re.I)
 PYPI_MD5 = re.compile(
@@ -191,7 +191,7 @@
     seen = set()
     seen_add = seen.add
     if key is None:
-        for element in six.moves.filterfalse(seen.__contains__, iterable):
+        for element in itertools.filterfalse(seen.__contains__, iterable):
             seen_add(element)
             yield element
     else:
@@ -740,7 +740,7 @@
             size = -1
             if "content-length" in headers:
                 # Some servers return multiple Content-Length headers :(
-                sizes = get_all_headers(headers, 'Content-Length')
+                sizes = headers.get_all('Content-Length')
                 size = max(map(int, sizes))
                 self.reporthook(url, filename, blocknum, bs, size)
             with open(filename, 'wb') as tfp:
@@ -767,7 +767,7 @@
             return local_open(url)
         try:
             return open_with_auth(url, self.opener)
-        except (ValueError, http_client.InvalidURL) as v:
+        except (ValueError, http.client.InvalidURL) as v:
             msg = ' '.join([str(arg) for arg in v.args])
             if warning:
                 self.warn(warning, msg)
@@ -781,7 +781,7 @@
             else:
                 raise DistutilsError("Download error for %s: %s"
                                      % (url, v.reason)) from v
-        except http_client.BadStatusLine as v:
+        except http.client.BadStatusLine as v:
             if warning:
                 self.warn(warning, v.line)
             else:
@@ -790,7 +790,7 @@
                     'down, %s' %
                     (url, v.line)
                 ) from v
-        except (http_client.HTTPException, socket.error) as v:
+        except (http.client.HTTPException, socket.error) as v:
             if warning:
                 self.warn(warning, v)
             else:
@@ -940,7 +940,7 @@
 
 def decode_entity(match):
     what = match.group(0)
-    return unescape(what)
+    return html.unescape(what)
 
 
 def htmldecode(text):
@@ -972,8 +972,7 @@
 
 def _encode_auth(auth):
     """
-    A function compatible with Python 2.3-3.3 that will encode
-    auth from a URL suitable for an HTTP header.
+    Encode auth from a URL suitable for an HTTP header.
     >>> str(_encode_auth('username%3Apassword'))
     'dXNlcm5hbWU6cGFzc3dvcmQ='
 
@@ -1056,7 +1055,7 @@
     # Double scheme does not raise on macOS as revealed by a
     # failing test. We would expect "nonnumeric port". Refs #20.
     if netloc.endswith(':'):
-        raise http_client.InvalidURL("nonnumeric port: ''")
+        raise http.client.InvalidURL("nonnumeric port: ''")
 
     if scheme in ('http', 'https'):
         auth, address = _splituser(netloc)
@@ -1136,5 +1135,5 @@
         status, message, body = 404, "Path not found", "Not found"
 
     headers = {'content-type': 'text/html'}
-    body_stream = six.StringIO(body)
+    body_stream = io.StringIO(body)
     return urllib.error.HTTPError(url, status, message, headers, body_stream)
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"JavaScriptSettings\">\n    <option name=\"languageLevel\" value=\"ES6\" />\n  </component>\n  <component name=\"ProjectRootManager\" version=\"2\" project-jdk-name=\"Python 3.7 (Gigahacks)\" project-jdk-type=\"Python SDK\" />\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/misc.xml	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ .idea/misc.xml	(date 1602088789566)
@@ -3,5 +3,5 @@
   <component name="JavaScriptSettings">
     <option name="languageLevel" value="ES6" />
   </component>
-  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.7 (Gigahacks)" project-jdk-type="Python SDK" />
+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.8 (Gigahacks)" project-jdk-type="Python SDK" />
 </project>
\ No newline at end of file
Index: .idea/Gigahacks.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<module type=\"PYTHON_MODULE\" version=\"4\">\n  <component name=\"NewModuleRootManager\">\n    <content url=\"file://$MODULE_DIR$\" />\n    <orderEntry type=\"jdk\" jdkName=\"Python 3.7 (Gigahacks)\" jdkType=\"Python SDK\" />\n    <orderEntry type=\"sourceFolder\" forTests=\"false\" />\n  </component>\n</module>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/Gigahacks.iml	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ .idea/Gigahacks.iml	(date 1602088789514)
@@ -2,7 +2,7 @@
 <module type="PYTHON_MODULE" version="4">
   <component name="NewModuleRootManager">
     <content url="file://$MODULE_DIR$" />
-    <orderEntry type="jdk" jdkName="Python 3.7 (Gigahacks)" jdkType="Python SDK" />
+    <orderEntry type="jdk" jdkName="Python 3.8 (Gigahacks)" jdkType="Python SDK" />
     <orderEntry type="sourceFolder" forTests="false" />
   </component>
 </module>
\ No newline at end of file
Index: env/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/resolver.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import functools\nimport logging\n\nfrom pip._vendor import six\nfrom pip._vendor.packaging.utils import canonicalize_name\nfrom pip._vendor.resolvelib import BaseReporter, ResolutionImpossible\nfrom pip._vendor.resolvelib import Resolver as RLResolver\n\nfrom pip._internal.exceptions import InstallationError\nfrom pip._internal.req.req_install import check_invalid_constraint_type\nfrom pip._internal.req.req_set import RequirementSet\nfrom pip._internal.resolution.base import BaseResolver\nfrom pip._internal.resolution.resolvelib.provider import PipProvider\nfrom pip._internal.utils.misc import dist_is_editable\nfrom pip._internal.utils.typing import MYPY_CHECK_RUNNING\n\nfrom .factory import Factory\n\nif MYPY_CHECK_RUNNING:\n    from typing import Dict, List, Optional, Set, Tuple\n\n    from pip._vendor.packaging.specifiers import SpecifierSet\n    from pip._vendor.resolvelib.resolvers import Result\n    from pip._vendor.resolvelib.structs import Graph\n\n    from pip._internal.cache import WheelCache\n    from pip._internal.index.package_finder import PackageFinder\n    from pip._internal.operations.prepare import RequirementPreparer\n    from pip._internal.req.req_install import InstallRequirement\n    from pip._internal.resolution.base import InstallRequirementProvider\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Resolver(BaseResolver):\n    _allowed_strategies = {\"eager\", \"only-if-needed\", \"to-satisfy-only\"}\n\n    def __init__(\n        self,\n        preparer,  # type: RequirementPreparer\n        finder,  # type: PackageFinder\n        wheel_cache,  # type: Optional[WheelCache]\n        make_install_req,  # type: InstallRequirementProvider\n        use_user_site,  # type: bool\n        ignore_dependencies,  # type: bool\n        ignore_installed,  # type: bool\n        ignore_requires_python,  # type: bool\n        force_reinstall,  # type: bool\n        upgrade_strategy,  # type: str\n        py_version_info=None,  # type: Optional[Tuple[int, ...]]\n        lazy_wheel=False,  # type: bool\n    ):\n        super(Resolver, self).__init__()\n        if lazy_wheel:\n            logger.warning(\n                'pip is using lazily downloaded wheels using HTTP '\n                'range requests to obtain dependency information. '\n                'This experimental feature is enabled through '\n                '--use-feature=fast-deps and it is not ready for production.'\n            )\n\n        assert upgrade_strategy in self._allowed_strategies\n\n        self.factory = Factory(\n            finder=finder,\n            preparer=preparer,\n            make_install_req=make_install_req,\n            wheel_cache=wheel_cache,\n            use_user_site=use_user_site,\n            force_reinstall=force_reinstall,\n            ignore_installed=ignore_installed,\n            ignore_requires_python=ignore_requires_python,\n            py_version_info=py_version_info,\n            lazy_wheel=lazy_wheel,\n        )\n        self.ignore_dependencies = ignore_dependencies\n        self.upgrade_strategy = upgrade_strategy\n        self._result = None  # type: Optional[Result]\n\n    def resolve(self, root_reqs, check_supported_wheels):\n        # type: (List[InstallRequirement], bool) -> RequirementSet\n\n        constraints = {}  # type: Dict[str, SpecifierSet]\n        user_requested = set()  # type: Set[str]\n        requirements = []\n        for req in root_reqs:\n            if req.constraint:\n                # Ensure we only accept valid constraints\n                problem = check_invalid_constraint_type(req)\n                if problem:\n                    raise InstallationError(problem)\n\n                name = canonicalize_name(req.name)\n                if name in constraints:\n                    constraints[name] = constraints[name] & req.specifier\n                else:\n                    constraints[name] = req.specifier\n            else:\n                if req.user_supplied and req.name:\n                    user_requested.add(canonicalize_name(req.name))\n                r = self.factory.make_requirement_from_install_req(\n                    req, requested_extras=(),\n                )\n                if r is not None:\n                    requirements.append(r)\n\n        provider = PipProvider(\n            factory=self.factory,\n            constraints=constraints,\n            ignore_dependencies=self.ignore_dependencies,\n            upgrade_strategy=self.upgrade_strategy,\n            user_requested=user_requested,\n        )\n        reporter = BaseReporter()\n        resolver = RLResolver(provider, reporter)\n\n        try:\n            try_to_avoid_resolution_too_deep = 2000000\n            self._result = resolver.resolve(\n                requirements, max_rounds=try_to_avoid_resolution_too_deep,\n            )\n\n        except ResolutionImpossible as e:\n            error = self.factory.get_installation_error(e)\n            six.raise_from(error, e)\n\n        req_set = RequirementSet(check_supported_wheels=check_supported_wheels)\n        for candidate in self._result.mapping.values():\n            ireq = candidate.get_install_requirement()\n            if ireq is None:\n                continue\n\n            # Check if there is already an installation under the same name,\n            # and set a flag for later stages to uninstall it, if needed.\n            # * There isn't, good -- no uninstalltion needed.\n            # * The --force-reinstall flag is set. Always reinstall.\n            # * The installation is different in version or editable-ness, so\n            #   we need to uninstall it to install the new distribution.\n            # * The installed version is the same as the pending distribution.\n            #   Skip this distrubiton altogether to save work.\n            installed_dist = self.factory.get_dist_to_uninstall(candidate)\n            if installed_dist is None:\n                ireq.should_reinstall = False\n            elif self.factory.force_reinstall:\n                ireq.should_reinstall = True\n            elif installed_dist.parsed_version != candidate.version:\n                ireq.should_reinstall = True\n            elif dist_is_editable(installed_dist) != candidate.is_editable:\n                ireq.should_reinstall = True\n            else:\n                continue\n\n            link = candidate.source_link\n            if link and link.is_yanked:\n                # The reason can contain non-ASCII characters, Unicode\n                # is required for Python 2.\n                msg = (\n                    u'The candidate selected for download or install is a '\n                    u'yanked version: {name!r} candidate (version {version} '\n                    u'at {link})\\nReason for being yanked: {reason}'\n                ).format(\n                    name=candidate.name,\n                    version=candidate.version,\n                    link=link,\n                    reason=link.yanked_reason or u'<none given>',\n                )\n                logger.warning(msg)\n\n            req_set.add_named_requirement(ireq)\n\n        return req_set\n\n    def get_installation_order(self, req_set):\n        # type: (RequirementSet) -> List[InstallRequirement]\n        \"\"\"Get order for installation of requirements in RequirementSet.\n\n        The returned list contains a requirement before another that depends on\n        it. This helps ensure that the environment is kept consistent as they\n        get installed one-by-one.\n\n        The current implementation creates a topological ordering of the\n        dependency graph, while breaking any cycles in the graph at arbitrary\n        points. We make no guarantees about where the cycle would be broken,\n        other than they would be broken.\n        \"\"\"\n        assert self._result is not None, \"must call resolve() first\"\n\n        graph = self._result.graph\n        weights = get_topological_weights(graph)\n\n        sorted_items = sorted(\n            req_set.requirements.items(),\n            key=functools.partial(_req_set_item_sorter, weights=weights),\n            reverse=True,\n        )\n        return [ireq for _, ireq in sorted_items]\n\n\ndef get_topological_weights(graph):\n    # type: (Graph) -> Dict[Optional[str], int]\n    \"\"\"Assign weights to each node based on how \"deep\" they are.\n\n    This implementation may change at any point in the future without prior\n    notice.\n\n    We take the length for the longest path to any node from root, ignoring any\n    paths that contain a single node twice (i.e. cycles). This is done through\n    a depth-first search through the graph, while keeping track of the path to\n    the node.\n\n    Cycles in the graph result would result in node being revisited while also\n    being it's own path. In this case, take no action. This helps ensure we\n    don't get stuck in a cycle.\n\n    When assigning weight, the longer path (i.e. larger length) is preferred.\n    \"\"\"\n    path = set()  # type: Set[Optional[str]]\n    weights = {}  # type: Dict[Optional[str], int]\n\n    def visit(node):\n        # type: (Optional[str]) -> None\n        if node in path:\n            # We hit a cycle, so we'll break it here.\n            return\n\n        # Time to visit the children!\n        path.add(node)\n        for child in graph.iter_children(node):\n            visit(child)\n        path.remove(node)\n\n        last_known_parent_count = weights.get(node, 0)\n        weights[node] = max(last_known_parent_count, len(path))\n\n    # `None` is guaranteed to be the root node by resolvelib.\n    visit(None)\n\n    # Sanity checks\n    assert weights[None] == 0\n    assert len(weights) == len(graph)\n\n    return weights\n\n\ndef _req_set_item_sorter(\n    item,     # type: Tuple[str, InstallRequirement]\n    weights,  # type: Dict[Optional[str], int]\n):\n    # type: (...) -> Tuple[int, str]\n    \"\"\"Key function used to sort install requirements for installation.\n\n    Based on the \"weight\" mapping calculated in ``get_installation_order()``.\n    The canonical package name is returned as the second member as a tie-\n    breaker to ensure the result is predictable, which is useful in tests.\n    \"\"\"\n    name = canonicalize_name(item[0])\n    return weights[name], name\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/resolver.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/resolver.py	(date 1602088700433)
@@ -90,7 +90,8 @@
                 problem = check_invalid_constraint_type(req)
                 if problem:
                     raise InstallationError(problem)
-
+                if not req.match_markers():
+                    continue
                 name = canonicalize_name(req.name)
                 if name in constraints:
                     constraints[name] = constraints[name] & req.specifier
Index: env/lib/python3.8/site-packages/setuptools/_distutils/util.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"distutils.util\n\nMiscellaneous utility functions -- anything that doesn't fit into\none of the other *util.py modules.\n\"\"\"\n\nimport os\nimport re\nimport importlib.util\nimport string\nimport sys\nfrom distutils.errors import DistutilsPlatformError\nfrom distutils.dep_util import newer\nfrom distutils.spawn import spawn\nfrom distutils import log\nfrom distutils.errors import DistutilsByteCompileError\n\ndef get_host_platform():\n    \"\"\"Return a string that identifies the current platform.  This is used mainly to\n    distinguish platform-specific build directories and platform-specific built\n    distributions.  Typically includes the OS name and version and the\n    architecture (as supplied by 'os.uname()'), although the exact information\n    included depends on the OS; eg. on Linux, the kernel version isn't\n    particularly important.\n\n    Examples of returned values:\n       linux-i586\n       linux-alpha (?)\n       solaris-2.6-sun4u\n\n    Windows will return one of:\n       win-amd64 (64bit Windows on AMD64 (aka x86_64, Intel64, EM64T, etc)\n       win32 (all others - specifically, sys.platform is returned)\n\n    For other non-POSIX platforms, currently just returns 'sys.platform'.\n\n    \"\"\"\n    if os.name == 'nt':\n        if 'amd64' in sys.version.lower():\n            return 'win-amd64'\n        if '(arm)' in sys.version.lower():\n            return 'win-arm32'\n        if '(arm64)' in sys.version.lower():\n            return 'win-arm64'\n        return sys.platform\n\n    # Set for cross builds explicitly\n    if \"_PYTHON_HOST_PLATFORM\" in os.environ:\n        return os.environ[\"_PYTHON_HOST_PLATFORM\"]\n\n    if os.name != \"posix\" or not hasattr(os, 'uname'):\n        # XXX what about the architecture? NT is Intel or Alpha,\n        # Mac OS is M68k or PPC, etc.\n        return sys.platform\n\n    # Try to distinguish various flavours of Unix\n\n    (osname, host, release, version, machine) = os.uname()\n\n    # Convert the OS name to lowercase, remove '/' characters, and translate\n    # spaces (for \"Power Macintosh\")\n    osname = osname.lower().replace('/', '')\n    machine = machine.replace(' ', '_')\n    machine = machine.replace('/', '-')\n\n    if osname[:5] == \"linux\":\n        # At least on Linux/Intel, 'machine' is the processor --\n        # i386, etc.\n        # XXX what about Alpha, SPARC, etc?\n        return  \"%s-%s\" % (osname, machine)\n    elif osname[:5] == \"sunos\":\n        if release[0] >= \"5\":           # SunOS 5 == Solaris 2\n            osname = \"solaris\"\n            release = \"%d.%s\" % (int(release[0]) - 3, release[2:])\n            # We can't use \"platform.architecture()[0]\" because a\n            # bootstrap problem. We use a dict to get an error\n            # if some suspicious happens.\n            bitness = {2147483647:\"32bit\", 9223372036854775807:\"64bit\"}\n            machine += \".%s\" % bitness[sys.maxsize]\n        # fall through to standard osname-release-machine representation\n    elif osname[:3] == \"aix\":\n        from _aix_support import aix_platform\n        return aix_platform()\n    elif osname[:6] == \"cygwin\":\n        osname = \"cygwin\"\n        rel_re = re.compile (r'[\\d.]+', re.ASCII)\n        m = rel_re.match(release)\n        if m:\n            release = m.group()\n    elif osname[:6] == \"darwin\":\n        import _osx_support, distutils.sysconfig\n        osname, release, machine = _osx_support.get_platform_osx(\n                                        distutils.sysconfig.get_config_vars(),\n                                        osname, release, machine)\n\n    return \"%s-%s-%s\" % (osname, release, machine)\n\ndef get_platform():\n    if os.name == 'nt':\n        TARGET_TO_PLAT = {\n            'x86' : 'win32',\n            'x64' : 'win-amd64',\n            'arm' : 'win-arm32',\n        }\n        return TARGET_TO_PLAT.get(os.environ.get('VSCMD_ARG_TGT_ARCH')) or get_host_platform()\n    else:\n        return get_host_platform()\n\ndef convert_path (pathname):\n    \"\"\"Return 'pathname' as a name that will work on the native filesystem,\n    i.e. split it on '/' and put it back together again using the current\n    directory separator.  Needed because filenames in the setup script are\n    always supplied in Unix style, and have to be converted to the local\n    convention before we can actually use them in the filesystem.  Raises\n    ValueError on non-Unix-ish systems if 'pathname' either starts or\n    ends with a slash.\n    \"\"\"\n    if os.sep == '/':\n        return pathname\n    if not pathname:\n        return pathname\n    if pathname[0] == '/':\n        raise ValueError(\"path '%s' cannot be absolute\" % pathname)\n    if pathname[-1] == '/':\n        raise ValueError(\"path '%s' cannot end with '/'\" % pathname)\n\n    paths = pathname.split('/')\n    while '.' in paths:\n        paths.remove('.')\n    if not paths:\n        return os.curdir\n    return os.path.join(*paths)\n\n# convert_path ()\n\n\ndef change_root (new_root, pathname):\n    \"\"\"Return 'pathname' with 'new_root' prepended.  If 'pathname' is\n    relative, this is equivalent to \"os.path.join(new_root,pathname)\".\n    Otherwise, it requires making 'pathname' relative and then joining the\n    two, which is tricky on DOS/Windows and Mac OS.\n    \"\"\"\n    if os.name == 'posix':\n        if not os.path.isabs(pathname):\n            return os.path.join(new_root, pathname)\n        else:\n            return os.path.join(new_root, pathname[1:])\n\n    elif os.name == 'nt':\n        (drive, path) = os.path.splitdrive(pathname)\n        if path[0] == '\\\\':\n            path = path[1:]\n        return os.path.join(new_root, path)\n\n    else:\n        raise DistutilsPlatformError(\"nothing known about platform '%s'\" % os.name)\n\n\n_environ_checked = 0\ndef check_environ ():\n    \"\"\"Ensure that 'os.environ' has all the environment variables we\n    guarantee that users can use in config files, command-line options,\n    etc.  Currently this includes:\n      HOME - user's home directory (Unix only)\n      PLAT - description of the current platform, including hardware\n             and OS (see 'get_platform()')\n    \"\"\"\n    global _environ_checked\n    if _environ_checked:\n        return\n\n    if os.name == 'posix' and 'HOME' not in os.environ:\n        try:\n            import pwd\n            os.environ['HOME'] = pwd.getpwuid(os.getuid())[5]\n        except (ImportError, KeyError):\n            # bpo-10496: if the current user identifier doesn't exist in the\n            # password database, do nothing\n            pass\n\n    if 'PLAT' not in os.environ:\n        os.environ['PLAT'] = get_platform()\n\n    _environ_checked = 1\n\n\ndef subst_vars (s, local_vars):\n    \"\"\"Perform shell/Perl-style variable substitution on 'string'.  Every\n    occurrence of '$' followed by a name is considered a variable, and\n    variable is substituted by the value found in the 'local_vars'\n    dictionary, or in 'os.environ' if it's not in 'local_vars'.\n    'os.environ' is first checked/augmented to guarantee that it contains\n    certain values: see 'check_environ()'.  Raise ValueError for any\n    variables not found in either 'local_vars' or 'os.environ'.\n    \"\"\"\n    check_environ()\n    def _subst (match, local_vars=local_vars):\n        var_name = match.group(1)\n        if var_name in local_vars:\n            return str(local_vars[var_name])\n        else:\n            return os.environ[var_name]\n\n    try:\n        return re.sub(r'\\$([a-zA-Z_][a-zA-Z_0-9]*)', _subst, s)\n    except KeyError as var:\n        raise ValueError(\"invalid variable '$%s'\" % var)\n\n# subst_vars ()\n\n\ndef grok_environment_error (exc, prefix=\"error: \"):\n    # Function kept for backward compatibility.\n    # Used to try clever things with EnvironmentErrors,\n    # but nowadays str(exception) produces good messages.\n    return prefix + str(exc)\n\n\n# Needed by 'split_quoted()'\n_wordchars_re = _squote_re = _dquote_re = None\ndef _init_regex():\n    global _wordchars_re, _squote_re, _dquote_re\n    _wordchars_re = re.compile(r'[^\\\\\\'\\\"%s ]*' % string.whitespace)\n    _squote_re = re.compile(r\"'(?:[^'\\\\]|\\\\.)*'\")\n    _dquote_re = re.compile(r'\"(?:[^\"\\\\]|\\\\.)*\"')\n\ndef split_quoted (s):\n    \"\"\"Split a string up according to Unix shell-like rules for quotes and\n    backslashes.  In short: words are delimited by spaces, as long as those\n    spaces are not escaped by a backslash, or inside a quoted string.\n    Single and double quotes are equivalent, and the quote characters can\n    be backslash-escaped.  The backslash is stripped from any two-character\n    escape sequence, leaving only the escaped character.  The quote\n    characters are stripped from any quoted string.  Returns a list of\n    words.\n    \"\"\"\n\n    # This is a nice algorithm for splitting up a single string, since it\n    # doesn't require character-by-character examination.  It was a little\n    # bit of a brain-bender to get it working right, though...\n    if _wordchars_re is None: _init_regex()\n\n    s = s.strip()\n    words = []\n    pos = 0\n\n    while s:\n        m = _wordchars_re.match(s, pos)\n        end = m.end()\n        if end == len(s):\n            words.append(s[:end])\n            break\n\n        if s[end] in string.whitespace: # unescaped, unquoted whitespace: now\n            words.append(s[:end])       # we definitely have a word delimiter\n            s = s[end:].lstrip()\n            pos = 0\n\n        elif s[end] == '\\\\':            # preserve whatever is being escaped;\n                                        # will become part of the current word\n            s = s[:end] + s[end+1:]\n            pos = end+1\n\n        else:\n            if s[end] == \"'\":           # slurp singly-quoted string\n                m = _squote_re.match(s, end)\n            elif s[end] == '\"':         # slurp doubly-quoted string\n                m = _dquote_re.match(s, end)\n            else:\n                raise RuntimeError(\"this can't happen (bad char '%c')\" % s[end])\n\n            if m is None:\n                raise ValueError(\"bad string (mismatched %s quotes?)\" % s[end])\n\n            (beg, end) = m.span()\n            s = s[:beg] + s[beg+1:end-1] + s[end:]\n            pos = m.end() - 2\n\n        if pos >= len(s):\n            words.append(s)\n            break\n\n    return words\n\n# split_quoted ()\n\n\ndef execute (func, args, msg=None, verbose=0, dry_run=0):\n    \"\"\"Perform some action that affects the outside world (eg.  by\n    writing to the filesystem).  Such actions are special because they\n    are disabled by the 'dry_run' flag.  This method takes care of all\n    that bureaucracy for you; all you have to do is supply the\n    function to call and an argument tuple for it (to embody the\n    \"external action\" being performed), and an optional message to\n    print.\n    \"\"\"\n    if msg is None:\n        msg = \"%s%r\" % (func.__name__, args)\n        if msg[-2:] == ',)':        # correct for singleton tuple\n            msg = msg[0:-2] + ')'\n\n    log.info(msg)\n    if not dry_run:\n        func(*args)\n\n\ndef strtobool (val):\n    \"\"\"Convert a string representation of truth to true (1) or false (0).\n\n    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values\n    are 'n', 'no', 'f', 'false', 'off', and '0'.  Raises ValueError if\n    'val' is anything else.\n    \"\"\"\n    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    elif val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    else:\n        raise ValueError(\"invalid truth value %r\" % (val,))\n\n\ndef byte_compile (py_files,\n                  optimize=0, force=0,\n                  prefix=None, base_dir=None,\n                  verbose=1, dry_run=0,\n                  direct=None):\n    \"\"\"Byte-compile a collection of Python source files to .pyc\n    files in a __pycache__ subdirectory.  'py_files' is a list\n    of files to compile; any files that don't end in \".py\" are silently\n    skipped.  'optimize' must be one of the following:\n      0 - don't optimize\n      1 - normal optimization (like \"python -O\")\n      2 - extra optimization (like \"python -OO\")\n    If 'force' is true, all files are recompiled regardless of\n    timestamps.\n\n    The source filename encoded in each bytecode file defaults to the\n    filenames listed in 'py_files'; you can modify these with 'prefix' and\n    'basedir'.  'prefix' is a string that will be stripped off of each\n    source filename, and 'base_dir' is a directory name that will be\n    prepended (after 'prefix' is stripped).  You can supply either or both\n    (or neither) of 'prefix' and 'base_dir', as you wish.\n\n    If 'dry_run' is true, doesn't actually do anything that would\n    affect the filesystem.\n\n    Byte-compilation is either done directly in this interpreter process\n    with the standard py_compile module, or indirectly by writing a\n    temporary script and executing it.  Normally, you should let\n    'byte_compile()' figure out to use direct compilation or not (see\n    the source for details).  The 'direct' flag is used by the script\n    generated in indirect mode; unless you know what you're doing, leave\n    it set to None.\n    \"\"\"\n\n    # Late import to fix a bootstrap issue: _posixsubprocess is built by\n    # setup.py, but setup.py uses distutils.\n    import subprocess\n\n    # nothing is done if sys.dont_write_bytecode is True\n    if sys.dont_write_bytecode:\n        raise DistutilsByteCompileError('byte-compiling is disabled.')\n\n    # First, if the caller didn't force us into direct or indirect mode,\n    # figure out which mode we should be in.  We take a conservative\n    # approach: choose direct mode *only* if the current interpreter is\n    # in debug mode and optimize is 0.  If we're not in debug mode (-O\n    # or -OO), we don't know which level of optimization this\n    # interpreter is running with, so we can't do direct\n    # byte-compilation and be certain that it's the right thing.  Thus,\n    # always compile indirectly if the current interpreter is in either\n    # optimize mode, or if either optimization level was requested by\n    # the caller.\n    if direct is None:\n        direct = (__debug__ and optimize == 0)\n\n    # \"Indirect\" byte-compilation: write a temporary script and then\n    # run it with the appropriate flags.\n    if not direct:\n        try:\n            from tempfile import mkstemp\n            (script_fd, script_name) = mkstemp(\".py\")\n        except ImportError:\n            from tempfile import mktemp\n            (script_fd, script_name) = None, mktemp(\".py\")\n        log.info(\"writing byte-compilation script '%s'\", script_name)\n        if not dry_run:\n            if script_fd is not None:\n                script = os.fdopen(script_fd, \"w\")\n            else:\n                script = open(script_name, \"w\")\n\n            with script:\n                script.write(\"\"\"\\\nfrom distutils.util import byte_compile\nfiles = [\n\"\"\")\n\n                # XXX would be nice to write absolute filenames, just for\n                # safety's sake (script should be more robust in the face of\n                # chdir'ing before running it).  But this requires abspath'ing\n                # 'prefix' as well, and that breaks the hack in build_lib's\n                # 'byte_compile()' method that carefully tacks on a trailing\n                # slash (os.sep really) to make sure the prefix here is \"just\n                # right\".  This whole prefix business is rather delicate -- the\n                # problem is that it's really a directory, but I'm treating it\n                # as a dumb string, so trailing slashes and so forth matter.\n\n                #py_files = map(os.path.abspath, py_files)\n                #if prefix:\n                #    prefix = os.path.abspath(prefix)\n\n                script.write(\",\\n\".join(map(repr, py_files)) + \"]\\n\")\n                script.write(\"\"\"\nbyte_compile(files, optimize=%r, force=%r,\n             prefix=%r, base_dir=%r,\n             verbose=%r, dry_run=0,\n             direct=1)\n\"\"\" % (optimize, force, prefix, base_dir, verbose))\n\n        cmd = [sys.executable]\n        cmd.extend(subprocess._optim_args_from_interpreter_flags())\n        cmd.append(script_name)\n        spawn(cmd, dry_run=dry_run)\n        execute(os.remove, (script_name,), \"removing %s\" % script_name,\n                dry_run=dry_run)\n\n    # \"Direct\" byte-compilation: use the py_compile module to compile\n    # right here, right now.  Note that the script generated in indirect\n    # mode simply calls 'byte_compile()' in direct mode, a weird sort of\n    # cross-process recursion.  Hey, it works!\n    else:\n        from py_compile import compile\n\n        for file in py_files:\n            if file[-3:] != \".py\":\n                # This lets us be lazy and not filter filenames in\n                # the \"install_lib\" command.\n                continue\n\n            # Terminology from the py_compile module:\n            #   cfile - byte-compiled file\n            #   dfile - purported source filename (same as 'file' by default)\n            if optimize >= 0:\n                opt = '' if optimize == 0 else optimize\n                cfile = importlib.util.cache_from_source(\n                    file, optimization=opt)\n            else:\n                cfile = importlib.util.cache_from_source(file)\n            dfile = file\n            if prefix:\n                if file[:len(prefix)] != prefix:\n                    raise ValueError(\"invalid prefix: filename %r doesn't start with %r\"\n                           % (file, prefix))\n                dfile = dfile[len(prefix):]\n            if base_dir:\n                dfile = os.path.join(base_dir, dfile)\n\n            cfile_base = os.path.basename(cfile)\n            if direct:\n                if force or newer(file, cfile):\n                    log.info(\"byte-compiling %s to %s\", file, cfile_base)\n                    if not dry_run:\n                        compile(file, cfile, dfile)\n                else:\n                    log.debug(\"skipping byte-compilation of %s to %s\",\n                              file, cfile_base)\n\n# byte_compile ()\n\ndef rfc822_escape (header):\n    \"\"\"Return a version of the string escaped for inclusion in an\n    RFC-822 header, by ensuring there are 8 spaces space after each newline.\n    \"\"\"\n    lines = header.split('\\n')\n    sep = '\\n' + 8 * ' '\n    return sep.join(lines)\n\n# 2to3 support\n\ndef run_2to3(files, fixer_names=None, options=None, explicit=None):\n    \"\"\"Invoke 2to3 on a list of Python files.\n    The files should all come from the build area, as the\n    modification is done in-place. To reduce the build time,\n    only files modified since the last invocation of this\n    function should be passed in the files argument.\"\"\"\n\n    if not files:\n        return\n\n    # Make this class local, to delay import of 2to3\n    from lib2to3.refactor import RefactoringTool, get_fixers_from_package\n    class DistutilsRefactoringTool(RefactoringTool):\n        def log_error(self, msg, *args, **kw):\n            log.error(msg, *args)\n\n        def log_message(self, msg, *args):\n            log.info(msg, *args)\n\n        def log_debug(self, msg, *args):\n            log.debug(msg, *args)\n\n    if fixer_names is None:\n        fixer_names = get_fixers_from_package('lib2to3.fixes')\n    r = DistutilsRefactoringTool(fixer_names, options=options)\n    r.refactor(files, write=True)\n\ndef copydir_run_2to3(src, dest, template=None, fixer_names=None,\n                     options=None, explicit=None):\n    \"\"\"Recursively copy a directory, only copying new and changed files,\n    running run_2to3 over all newly copied Python modules afterward.\n\n    If you give a template string, it's parsed like a MANIFEST.in.\n    \"\"\"\n    from distutils.dir_util import mkpath\n    from distutils.file_util import copy_file\n    from distutils.filelist import FileList\n    filelist = FileList()\n    curdir = os.getcwd()\n    os.chdir(src)\n    try:\n        filelist.findall()\n    finally:\n        os.chdir(curdir)\n    filelist.files[:] = filelist.allfiles\n    if template:\n        for line in template.splitlines():\n            line = line.strip()\n            if not line: continue\n            filelist.process_template_line(line)\n    copied = []\n    for filename in filelist.files:\n        outname = os.path.join(dest, filename)\n        mkpath(os.path.dirname(outname))\n        res = copy_file(os.path.join(src, filename), outname, update=1)\n        if res[1]: copied.append(outname)\n    run_2to3([fn for fn in copied if fn.lower().endswith('.py')],\n             fixer_names=fixer_names, options=options, explicit=explicit)\n    return copied\n\nclass Mixin2to3:\n    '''Mixin class for commands that run 2to3.\n    To configure 2to3, setup scripts may either change\n    the class variables, or inherit from individual commands\n    to override how 2to3 is invoked.'''\n\n    # provide list of fixers to run;\n    # defaults to all from lib2to3.fixers\n    fixer_names = None\n\n    # options dictionary\n    options = None\n\n    # list of fixers to invoke even though they are marked as explicit\n    explicit = None\n\n    def run_2to3(self, files):\n        return run_2to3(files, self.fixer_names, self.options, self.explicit)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/_distutils/util.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/_distutils/util.py	(date 1602088701609)
@@ -14,6 +14,8 @@
 from distutils.spawn import spawn
 from distutils import log
 from distutils.errors import DistutilsByteCompileError
+from .py35compat import _optim_args_from_interpreter_flags
+
 
 def get_host_platform():
     """Return a string that identifies the current platform.  This is used mainly to
@@ -79,8 +81,8 @@
             machine += ".%s" % bitness[sys.maxsize]
         # fall through to standard osname-release-machine representation
     elif osname[:3] == "aix":
-        from _aix_support import aix_platform
-        return aix_platform()
+        from .py38compat import aix_platform
+        return aix_platform(osname, version, release)
     elif osname[:6] == "cygwin":
         osname = "cygwin"
         rel_re = re.compile (r'[\d.]+', re.ASCII)
@@ -420,7 +422,7 @@
 """ % (optimize, force, prefix, base_dir, verbose))
 
         cmd = [sys.executable]
-        cmd.extend(subprocess._optim_args_from_interpreter_flags())
+        cmd.extend(_optim_args_from_interpreter_flags())
         cmd.append(script_name)
         spawn(cmd, dry_run=dry_run)
         execute(os.remove, (script_name,), "removing %s" % script_name,
Index: env/lib/python3.8/site-packages/setuptools/_distutils/_msvccompiler.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"distutils._msvccompiler\n\nContains MSVCCompiler, an implementation of the abstract CCompiler class\nfor Microsoft Visual Studio 2015.\n\nThe module is compatible with VS 2015 and later. You can find legacy support\nfor older versions in distutils.msvc9compiler and distutils.msvccompiler.\n\"\"\"\n\n# Written by Perry Stoll\n# hacked by Robin Becker and Thomas Heller to do a better job of\n#   finding DevStudio (through the registry)\n# ported to VS 2005 and VS 2008 by Christian Heimes\n# ported to VS 2015 by Steve Dower\n\nimport os\nimport subprocess\nimport contextlib\nwith contextlib.suppress(ImportError):\n    import winreg\n\nfrom distutils.errors import DistutilsExecError, DistutilsPlatformError, \\\n                             CompileError, LibError, LinkError\nfrom distutils.ccompiler import CCompiler, gen_lib_options\nfrom distutils import log\nfrom distutils.util import get_platform\n\nfrom itertools import count\n\ndef _find_vc2015():\n    try:\n        key = winreg.OpenKeyEx(\n            winreg.HKEY_LOCAL_MACHINE,\n            r\"Software\\Microsoft\\VisualStudio\\SxS\\VC7\",\n            access=winreg.KEY_READ | winreg.KEY_WOW64_32KEY\n        )\n    except OSError:\n        log.debug(\"Visual C++ is not registered\")\n        return None, None\n\n    best_version = 0\n    best_dir = None\n    with key:\n        for i in count():\n            try:\n                v, vc_dir, vt = winreg.EnumValue(key, i)\n            except OSError:\n                break\n            if v and vt == winreg.REG_SZ and os.path.isdir(vc_dir):\n                try:\n                    version = int(float(v))\n                except (ValueError, TypeError):\n                    continue\n                if version >= 14 and version > best_version:\n                    best_version, best_dir = version, vc_dir\n    return best_version, best_dir\n\ndef _find_vc2017():\n    \"\"\"Returns \"15, path\" based on the result of invoking vswhere.exe\n    If no install is found, returns \"None, None\"\n\n    The version is returned to avoid unnecessarily changing the function\n    result. It may be ignored when the path is not None.\n\n    If vswhere.exe is not available, by definition, VS 2017 is not\n    installed.\n    \"\"\"\n    root = os.environ.get(\"ProgramFiles(x86)\") or os.environ.get(\"ProgramFiles\")\n    if not root:\n        return None, None\n\n    try:\n        path = subprocess.check_output([\n            os.path.join(root, \"Microsoft Visual Studio\", \"Installer\", \"vswhere.exe\"),\n            \"-latest\",\n            \"-prerelease\",\n            \"-requires\", \"Microsoft.VisualStudio.Component.VC.Tools.x86.x64\",\n            \"-property\", \"installationPath\",\n            \"-products\", \"*\",\n        ], encoding=\"mbcs\", errors=\"strict\").strip()\n    except (subprocess.CalledProcessError, OSError, UnicodeDecodeError):\n        return None, None\n\n    path = os.path.join(path, \"VC\", \"Auxiliary\", \"Build\")\n    if os.path.isdir(path):\n        return 15, path\n\n    return None, None\n\nPLAT_SPEC_TO_RUNTIME = {\n    'x86' : 'x86',\n    'x86_amd64' : 'x64',\n    'x86_arm' : 'arm',\n    'x86_arm64' : 'arm64'\n}\n\ndef _find_vcvarsall(plat_spec):\n    # bpo-38597: Removed vcruntime return value\n    _, best_dir = _find_vc2017()\n\n    if not best_dir:\n        best_version, best_dir = _find_vc2015()\n\n    if not best_dir:\n        log.debug(\"No suitable Visual C++ version found\")\n        return None, None\n\n    vcvarsall = os.path.join(best_dir, \"vcvarsall.bat\")\n    if not os.path.isfile(vcvarsall):\n        log.debug(\"%s cannot be found\", vcvarsall)\n        return None, None\n\n    return vcvarsall, None\n\ndef _get_vc_env(plat_spec):\n    if os.getenv(\"DISTUTILS_USE_SDK\"):\n        return {\n            key.lower(): value\n            for key, value in os.environ.items()\n        }\n\n    vcvarsall, _ = _find_vcvarsall(plat_spec)\n    if not vcvarsall:\n        raise DistutilsPlatformError(\"Unable to find vcvarsall.bat\")\n\n    try:\n        out = subprocess.check_output(\n            'cmd /u /c \"{}\" {} && set'.format(vcvarsall, plat_spec),\n            stderr=subprocess.STDOUT,\n        ).decode('utf-16le', errors='replace')\n    except subprocess.CalledProcessError as exc:\n        log.error(exc.output)\n        raise DistutilsPlatformError(\"Error executing {}\"\n                .format(exc.cmd))\n\n    env = {\n        key.lower(): value\n        for key, _, value in\n        (line.partition('=') for line in out.splitlines())\n        if key and value\n    }\n\n    return env\n\ndef _find_exe(exe, paths=None):\n    \"\"\"Return path to an MSVC executable program.\n\n    Tries to find the program in several places: first, one of the\n    MSVC program search paths from the registry; next, the directories\n    in the PATH environment variable.  If any of those work, return an\n    absolute path that is known to exist.  If none of them work, just\n    return the original program name, 'exe'.\n    \"\"\"\n    if not paths:\n        paths = os.getenv('path').split(os.pathsep)\n    for p in paths:\n        fn = os.path.join(os.path.abspath(p), exe)\n        if os.path.isfile(fn):\n            return fn\n    return exe\n\n# A map keyed by get_platform() return values to values accepted by\n# 'vcvarsall.bat'. Always cross-compile from x86 to work with the\n# lighter-weight MSVC installs that do not include native 64-bit tools.\nPLAT_TO_VCVARS = {\n    'win32' : 'x86',\n    'win-amd64' : 'x86_amd64',\n    'win-arm32' : 'x86_arm',\n    'win-arm64' : 'x86_arm64'\n}\n\nclass MSVCCompiler(CCompiler) :\n    \"\"\"Concrete class that implements an interface to Microsoft Visual C++,\n       as defined by the CCompiler abstract class.\"\"\"\n\n    compiler_type = 'msvc'\n\n    # Just set this so CCompiler's constructor doesn't barf.  We currently\n    # don't use the 'set_executables()' bureaucracy provided by CCompiler,\n    # as it really isn't necessary for this sort of single-compiler class.\n    # Would be nice to have a consistent interface with UnixCCompiler,\n    # though, so it's worth thinking about.\n    executables = {}\n\n    # Private class data (need to distinguish C from C++ source for compiler)\n    _c_extensions = ['.c']\n    _cpp_extensions = ['.cc', '.cpp', '.cxx']\n    _rc_extensions = ['.rc']\n    _mc_extensions = ['.mc']\n\n    # Needed for the filename generation methods provided by the\n    # base class, CCompiler.\n    src_extensions = (_c_extensions + _cpp_extensions +\n                      _rc_extensions + _mc_extensions)\n    res_extension = '.res'\n    obj_extension = '.obj'\n    static_lib_extension = '.lib'\n    shared_lib_extension = '.dll'\n    static_lib_format = shared_lib_format = '%s%s'\n    exe_extension = '.exe'\n\n\n    def __init__(self, verbose=0, dry_run=0, force=0):\n        CCompiler.__init__ (self, verbose, dry_run, force)\n        # target platform (.plat_name is consistent with 'bdist')\n        self.plat_name = None\n        self.initialized = False\n\n    def initialize(self, plat_name=None):\n        # multi-init means we would need to check platform same each time...\n        assert not self.initialized, \"don't init multiple times\"\n        if plat_name is None:\n            plat_name = get_platform()\n        # sanity check for platforms to prevent obscure errors later.\n        if plat_name not in PLAT_TO_VCVARS:\n            raise DistutilsPlatformError(\"--plat-name must be one of {}\"\n                                         .format(tuple(PLAT_TO_VCVARS)))\n\n        # Get the vcvarsall.bat spec for the requested platform.\n        plat_spec = PLAT_TO_VCVARS[plat_name]\n\n        vc_env = _get_vc_env(plat_spec)\n        if not vc_env:\n            raise DistutilsPlatformError(\"Unable to find a compatible \"\n                \"Visual Studio installation.\")\n\n        self._paths = vc_env.get('path', '')\n        paths = self._paths.split(os.pathsep)\n        self.cc = _find_exe(\"cl.exe\", paths)\n        self.linker = _find_exe(\"link.exe\", paths)\n        self.lib = _find_exe(\"lib.exe\", paths)\n        self.rc = _find_exe(\"rc.exe\", paths)   # resource compiler\n        self.mc = _find_exe(\"mc.exe\", paths)   # message compiler\n        self.mt = _find_exe(\"mt.exe\", paths)   # message compiler\n\n        for dir in vc_env.get('include', '').split(os.pathsep):\n            if dir:\n                self.add_include_dir(dir.rstrip(os.sep))\n\n        for dir in vc_env.get('lib', '').split(os.pathsep):\n            if dir:\n                self.add_library_dir(dir.rstrip(os.sep))\n\n        self.preprocess_options = None\n        # bpo-38597: Always compile with dynamic linking\n        # Future releases of Python 3.x will include all past\n        # versions of vcruntime*.dll for compatibility.\n        self.compile_options = [\n            '/nologo', '/Ox', '/W3', '/GL', '/DNDEBUG', '/MD'\n        ]\n\n        self.compile_options_debug = [\n            '/nologo', '/Od', '/MDd', '/Zi', '/W3', '/D_DEBUG'\n        ]\n\n        ldflags = [\n            '/nologo', '/INCREMENTAL:NO', '/LTCG'\n        ]\n\n        ldflags_debug = [\n            '/nologo', '/INCREMENTAL:NO', '/LTCG', '/DEBUG:FULL'\n        ]\n\n        self.ldflags_exe = [*ldflags, '/MANIFEST:EMBED,ID=1']\n        self.ldflags_exe_debug = [*ldflags_debug, '/MANIFEST:EMBED,ID=1']\n        self.ldflags_shared = [*ldflags, '/DLL', '/MANIFEST:EMBED,ID=2', '/MANIFESTUAC:NO']\n        self.ldflags_shared_debug = [*ldflags_debug, '/DLL', '/MANIFEST:EMBED,ID=2', '/MANIFESTUAC:NO']\n        self.ldflags_static = [*ldflags]\n        self.ldflags_static_debug = [*ldflags_debug]\n\n        self._ldflags = {\n            (CCompiler.EXECUTABLE, None): self.ldflags_exe,\n            (CCompiler.EXECUTABLE, False): self.ldflags_exe,\n            (CCompiler.EXECUTABLE, True): self.ldflags_exe_debug,\n            (CCompiler.SHARED_OBJECT, None): self.ldflags_shared,\n            (CCompiler.SHARED_OBJECT, False): self.ldflags_shared,\n            (CCompiler.SHARED_OBJECT, True): self.ldflags_shared_debug,\n            (CCompiler.SHARED_LIBRARY, None): self.ldflags_static,\n            (CCompiler.SHARED_LIBRARY, False): self.ldflags_static,\n            (CCompiler.SHARED_LIBRARY, True): self.ldflags_static_debug,\n        }\n\n        self.initialized = True\n\n    # -- Worker methods ------------------------------------------------\n\n    def object_filenames(self,\n                         source_filenames,\n                         strip_dir=0,\n                         output_dir=''):\n        ext_map = {\n            **{ext: self.obj_extension for ext in self.src_extensions},\n            **{ext: self.res_extension for ext in self._rc_extensions + self._mc_extensions},\n        }\n\n        output_dir = output_dir or ''\n\n        def make_out_path(p):\n            base, ext = os.path.splitext(p)\n            if strip_dir:\n                base = os.path.basename(base)\n            else:\n                _, base = os.path.splitdrive(base)\n                if base.startswith((os.path.sep, os.path.altsep)):\n                    base = base[1:]\n            try:\n                # XXX: This may produce absurdly long paths. We should check\n                # the length of the result and trim base until we fit within\n                # 260 characters.\n                return os.path.join(output_dir, base + ext_map[ext])\n            except LookupError:\n                # Better to raise an exception instead of silently continuing\n                # and later complain about sources and targets having\n                # different lengths\n                raise CompileError(\"Don't know how to compile {}\".format(p))\n\n        return list(map(make_out_path, source_filenames))\n\n\n    def compile(self, sources,\n                output_dir=None, macros=None, include_dirs=None, debug=0,\n                extra_preargs=None, extra_postargs=None, depends=None):\n\n        if not self.initialized:\n            self.initialize()\n        compile_info = self._setup_compile(output_dir, macros, include_dirs,\n                                           sources, depends, extra_postargs)\n        macros, objects, extra_postargs, pp_opts, build = compile_info\n\n        compile_opts = extra_preargs or []\n        compile_opts.append('/c')\n        if debug:\n            compile_opts.extend(self.compile_options_debug)\n        else:\n            compile_opts.extend(self.compile_options)\n\n\n        add_cpp_opts = False\n\n        for obj in objects:\n            try:\n                src, ext = build[obj]\n            except KeyError:\n                continue\n            if debug:\n                # pass the full pathname to MSVC in debug mode,\n                # this allows the debugger to find the source file\n                # without asking the user to browse for it\n                src = os.path.abspath(src)\n\n            if ext in self._c_extensions:\n                input_opt = \"/Tc\" + src\n            elif ext in self._cpp_extensions:\n                input_opt = \"/Tp\" + src\n                add_cpp_opts = True\n            elif ext in self._rc_extensions:\n                # compile .RC to .RES file\n                input_opt = src\n                output_opt = \"/fo\" + obj\n                try:\n                    self.spawn([self.rc] + pp_opts + [output_opt, input_opt])\n                except DistutilsExecError as msg:\n                    raise CompileError(msg)\n                continue\n            elif ext in self._mc_extensions:\n                # Compile .MC to .RC file to .RES file.\n                #   * '-h dir' specifies the directory for the\n                #     generated include file\n                #   * '-r dir' specifies the target directory of the\n                #     generated RC file and the binary message resource\n                #     it includes\n                #\n                # For now (since there are no options to change this),\n                # we use the source-directory for the include file and\n                # the build directory for the RC file and message\n                # resources. This works at least for win32all.\n                h_dir = os.path.dirname(src)\n                rc_dir = os.path.dirname(obj)\n                try:\n                    # first compile .MC to .RC and .H file\n                    self.spawn([self.mc, '-h', h_dir, '-r', rc_dir, src])\n                    base, _ = os.path.splitext(os.path.basename (src))\n                    rc_file = os.path.join(rc_dir, base + '.rc')\n                    # then compile .RC to .RES file\n                    self.spawn([self.rc, \"/fo\" + obj, rc_file])\n\n                except DistutilsExecError as msg:\n                    raise CompileError(msg)\n                continue\n            else:\n                # how to handle this file?\n                raise CompileError(\"Don't know how to compile {} to {}\"\n                                   .format(src, obj))\n\n            args = [self.cc] + compile_opts + pp_opts\n            if add_cpp_opts:\n                args.append('/EHsc')\n            args.append(input_opt)\n            args.append(\"/Fo\" + obj)\n            args.extend(extra_postargs)\n\n            try:\n                self.spawn(args)\n            except DistutilsExecError as msg:\n                raise CompileError(msg)\n\n        return objects\n\n\n    def create_static_lib(self,\n                          objects,\n                          output_libname,\n                          output_dir=None,\n                          debug=0,\n                          target_lang=None):\n\n        if not self.initialized:\n            self.initialize()\n        objects, output_dir = self._fix_object_args(objects, output_dir)\n        output_filename = self.library_filename(output_libname,\n                                                output_dir=output_dir)\n\n        if self._need_link(objects, output_filename):\n            lib_args = objects + ['/OUT:' + output_filename]\n            if debug:\n                pass # XXX what goes here?\n            try:\n                log.debug('Executing \"%s\" %s', self.lib, ' '.join(lib_args))\n                self.spawn([self.lib] + lib_args)\n            except DistutilsExecError as msg:\n                raise LibError(msg)\n        else:\n            log.debug(\"skipping %s (up-to-date)\", output_filename)\n\n\n    def link(self,\n             target_desc,\n             objects,\n             output_filename,\n             output_dir=None,\n             libraries=None,\n             library_dirs=None,\n             runtime_library_dirs=None,\n             export_symbols=None,\n             debug=0,\n             extra_preargs=None,\n             extra_postargs=None,\n             build_temp=None,\n             target_lang=None):\n\n        if not self.initialized:\n            self.initialize()\n        objects, output_dir = self._fix_object_args(objects, output_dir)\n        fixed_args = self._fix_lib_args(libraries, library_dirs,\n                                        runtime_library_dirs)\n        libraries, library_dirs, runtime_library_dirs = fixed_args\n\n        if runtime_library_dirs:\n            self.warn(\"I don't know what to do with 'runtime_library_dirs': \"\n                       + str(runtime_library_dirs))\n\n        lib_opts = gen_lib_options(self,\n                                   library_dirs, runtime_library_dirs,\n                                   libraries)\n        if output_dir is not None:\n            output_filename = os.path.join(output_dir, output_filename)\n\n        if self._need_link(objects, output_filename):\n            ldflags = self._ldflags[target_desc, debug]\n\n            export_opts = [\"/EXPORT:\" + sym for sym in (export_symbols or [])]\n\n            ld_args = (ldflags + lib_opts + export_opts +\n                       objects + ['/OUT:' + output_filename])\n\n            # The MSVC linker generates .lib and .exp files, which cannot be\n            # suppressed by any linker switches. The .lib files may even be\n            # needed! Make sure they are generated in the temporary build\n            # directory. Since they have different names for debug and release\n            # builds, they can go into the same directory.\n            build_temp = os.path.dirname(objects[0])\n            if export_symbols is not None:\n                (dll_name, dll_ext) = os.path.splitext(\n                    os.path.basename(output_filename))\n                implib_file = os.path.join(\n                    build_temp,\n                    self.library_filename(dll_name))\n                ld_args.append ('/IMPLIB:' + implib_file)\n\n            if extra_preargs:\n                ld_args[:0] = extra_preargs\n            if extra_postargs:\n                ld_args.extend(extra_postargs)\n\n            output_dir = os.path.dirname(os.path.abspath(output_filename))\n            self.mkpath(output_dir)\n            try:\n                log.debug('Executing \"%s\" %s', self.linker, ' '.join(ld_args))\n                self.spawn([self.linker] + ld_args)\n            except DistutilsExecError as msg:\n                raise LinkError(msg)\n        else:\n            log.debug(\"skipping %s (up-to-date)\", output_filename)\n\n    def spawn(self, cmd):\n        env = dict(os.environ, PATH=self._paths)\n        return super().spawn(cmd, env=env)\n\n    # -- Miscellaneous methods -----------------------------------------\n    # These are all used by the 'gen_lib_options() function, in\n    # ccompiler.py.\n\n    def library_dir_option(self, dir):\n        return \"/LIBPATH:\" + dir\n\n    def runtime_library_dir_option(self, dir):\n        raise DistutilsPlatformError(\n              \"don't know how to set runtime library search path for MSVC\")\n\n    def library_option(self, lib):\n        return self.library_filename(lib)\n\n    def find_library_file(self, dirs, lib, debug=0):\n        # Prefer a debugging library if found (and requested), but deal\n        # with it if we don't have one.\n        if debug:\n            try_names = [lib + \"_d\", lib]\n        else:\n            try_names = [lib]\n        for dir in dirs:\n            for name in try_names:\n                libfile = os.path.join(dir, self.library_filename(name))\n                if os.path.isfile(libfile):\n                    return libfile\n        else:\n            # Oops, didn't find it in *any* of 'dirs'\n            return None\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/_distutils/_msvccompiler.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/_distutils/_msvccompiler.py	(date 1602088701601)
@@ -16,6 +16,8 @@
 import os
 import subprocess
 import contextlib
+import warnings
+import unittest.mock
 with contextlib.suppress(ImportError):
     import winreg
 
@@ -504,7 +506,29 @@
 
     def spawn(self, cmd):
         env = dict(os.environ, PATH=self._paths)
-        return super().spawn(cmd, env=env)
+        with self._fallback_spawn(cmd, env) as fallback:
+            return super().spawn(cmd, env=env)
+        return fallback.value
+
+    @contextlib.contextmanager
+    def _fallback_spawn(self, cmd, env):
+        """
+        Discovered in pypa/distutils#15, some tools monkeypatch the compiler,
+        so the 'env' kwarg causes a TypeError. Detect this condition and
+        restore the legacy, unsafe behavior.
+        """
+        bag = type('Bag', (), {})()
+        try:
+            yield bag
+        except TypeError as exc:
+            if "unexpected keyword argument 'env'" not in str(exc):
+                raise
+        else:
+            return
+        warnings.warn(
+            "Fallback spawn triggered. Please update distutils monkeypatch.")
+        with unittest.mock.patch('os.environ', env):
+            bag.value = super().spawn(cmd)
 
     # -- Miscellaneous methods -----------------------------------------
     # These are all used by the 'gen_lib_options() function, in
Index: env/lib/python3.8/site-packages/pip/_internal/commands/search.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import absolute_import\n\nimport logging\nimport sys\nimport textwrap\nfrom collections import OrderedDict\n\nfrom pip._vendor import pkg_resources\nfrom pip._vendor.packaging.version import parse as parse_version\n# NOTE: XMLRPC Client is not annotated in typeshed as on 2017-07-17, which is\n#       why we ignore the type on this import\nfrom pip._vendor.six.moves import xmlrpc_client  # type: ignore\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.req_command import SessionCommandMixin\nfrom pip._internal.cli.status_codes import NO_MATCHES_FOUND, SUCCESS\nfrom pip._internal.exceptions import CommandError\nfrom pip._internal.models.index import PyPI\nfrom pip._internal.network.xmlrpc import PipXmlrpcTransport\nfrom pip._internal.utils.compat import get_terminal_size\nfrom pip._internal.utils.logging import indent_log\nfrom pip._internal.utils.misc import get_distribution, write_output\nfrom pip._internal.utils.typing import MYPY_CHECK_RUNNING\n\nif MYPY_CHECK_RUNNING:\n    from optparse import Values\n    from typing import List, Dict, Optional\n    from typing_extensions import TypedDict\n    TransformedHit = TypedDict(\n        'TransformedHit',\n        {'name': str, 'summary': str, 'versions': List[str]},\n    )\n\nlogger = logging.getLogger(__name__)\n\n\nclass SearchCommand(Command, SessionCommandMixin):\n    \"\"\"Search for PyPI packages whose name or summary contains <query>.\"\"\"\n\n    usage = \"\"\"\n      %prog [options] <query>\"\"\"\n    ignore_require_venv = True\n\n    def add_options(self):\n        # type: () -> None\n        self.cmd_opts.add_option(\n            '-i', '--index',\n            dest='index',\n            metavar='URL',\n            default=PyPI.pypi_url,\n            help='Base URL of Python Package Index (default %default)')\n\n        self.parser.insert_option_group(0, self.cmd_opts)\n\n    def run(self, options, args):\n        # type: (Values, List[str]) -> int\n        if not args:\n            raise CommandError('Missing required argument (search query).')\n        query = args\n        pypi_hits = self.search(query, options)\n        hits = transform_hits(pypi_hits)\n\n        terminal_width = None\n        if sys.stdout.isatty():\n            terminal_width = get_terminal_size()[0]\n\n        print_results(hits, terminal_width=terminal_width)\n        if pypi_hits:\n            return SUCCESS\n        return NO_MATCHES_FOUND\n\n    def search(self, query, options):\n        # type: (List[str], Values) -> List[Dict[str, str]]\n        index_url = options.index\n\n        session = self.get_default_session(options)\n\n        transport = PipXmlrpcTransport(index_url, session)\n        pypi = xmlrpc_client.ServerProxy(index_url, transport)\n        hits = pypi.search({'name': query, 'summary': query}, 'or')\n        return hits\n\n\ndef transform_hits(hits):\n    # type: (List[Dict[str, str]]) -> List[TransformedHit]\n    \"\"\"\n    The list from pypi is really a list of versions. We want a list of\n    packages with the list of versions stored inline. This converts the\n    list from pypi into one we can use.\n    \"\"\"\n    packages = OrderedDict()  # type: OrderedDict[str, TransformedHit]\n    for hit in hits:\n        name = hit['name']\n        summary = hit['summary']\n        version = hit['version']\n\n        if name not in packages.keys():\n            packages[name] = {\n                'name': name,\n                'summary': summary,\n                'versions': [version],\n            }\n        else:\n            packages[name]['versions'].append(version)\n\n            # if this is the highest version, replace summary and score\n            if version == highest_version(packages[name]['versions']):\n                packages[name]['summary'] = summary\n\n    return list(packages.values())\n\n\ndef print_results(hits, name_column_width=None, terminal_width=None):\n    # type: (List[TransformedHit], Optional[int], Optional[int]) -> None\n    if not hits:\n        return\n    if name_column_width is None:\n        name_column_width = max([\n            len(hit['name']) + len(highest_version(hit.get('versions', ['-'])))\n            for hit in hits\n        ]) + 4\n\n    installed_packages = [p.project_name for p in pkg_resources.working_set]\n    for hit in hits:\n        name = hit['name']\n        summary = hit['summary'] or ''\n        latest = highest_version(hit.get('versions', ['-']))\n        if terminal_width is not None:\n            target_width = terminal_width - name_column_width - 5\n            if target_width > 10:\n                # wrap and indent summary to fit terminal\n                summary_lines = textwrap.wrap(summary, target_width)\n                summary = ('\\n' + ' ' * (name_column_width + 3)).join(\n                    summary_lines)\n\n        line = '{name_latest:{name_column_width}} - {summary}'.format(\n            name_latest='{name} ({latest})'.format(**locals()),\n            **locals())\n        try:\n            write_output(line)\n            if name in installed_packages:\n                dist = get_distribution(name)\n                with indent_log():\n                    if dist.version == latest:\n                        write_output('INSTALLED: %s (latest)', dist.version)\n                    else:\n                        write_output('INSTALLED: %s', dist.version)\n                        if parse_version(latest).pre:\n                            write_output('LATEST:    %s (pre-release; install'\n                                         ' with \"pip install --pre\")', latest)\n                        else:\n                            write_output('LATEST:    %s', latest)\n        except UnicodeEncodeError:\n            pass\n\n\ndef highest_version(versions):\n    # type: (List[str]) -> str\n    return max(versions, key=parse_version)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pip/_internal/commands/search.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pip/_internal/commands/search.py	(date 1602088700393)
@@ -140,6 +140,7 @@
             write_output(line)
             if name in installed_packages:
                 dist = get_distribution(name)
+                assert dist is not None
                 with indent_log():
                     if dist.version == latest:
                         write_output('INSTALLED: %s (latest)', dist.version)
Index: env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/version.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import absolute_import, division, print_function\n\nimport collections\nimport itertools\nimport re\n\nfrom ._structures import Infinity\n\n\n__all__ = [\"parse\", \"Version\", \"LegacyVersion\", \"InvalidVersion\", \"VERSION_PATTERN\"]\n\n\n_Version = collections.namedtuple(\n    \"_Version\", [\"epoch\", \"release\", \"dev\", \"pre\", \"post\", \"local\"]\n)\n\n\ndef parse(version):\n    \"\"\"\n    Parse the given version string and return either a :class:`Version` object\n    or a :class:`LegacyVersion` object depending on if the given version is\n    a valid PEP 440 version or a legacy version.\n    \"\"\"\n    try:\n        return Version(version)\n    except InvalidVersion:\n        return LegacyVersion(version)\n\n\nclass InvalidVersion(ValueError):\n    \"\"\"\n    An invalid version was found, users should refer to PEP 440.\n    \"\"\"\n\n\nclass _BaseVersion(object):\n    def __hash__(self):\n        return hash(self._key)\n\n    def __lt__(self, other):\n        return self._compare(other, lambda s, o: s < o)\n\n    def __le__(self, other):\n        return self._compare(other, lambda s, o: s <= o)\n\n    def __eq__(self, other):\n        return self._compare(other, lambda s, o: s == o)\n\n    def __ge__(self, other):\n        return self._compare(other, lambda s, o: s >= o)\n\n    def __gt__(self, other):\n        return self._compare(other, lambda s, o: s > o)\n\n    def __ne__(self, other):\n        return self._compare(other, lambda s, o: s != o)\n\n    def _compare(self, other, method):\n        if not isinstance(other, _BaseVersion):\n            return NotImplemented\n\n        return method(self._key, other._key)\n\n\nclass LegacyVersion(_BaseVersion):\n    def __init__(self, version):\n        self._version = str(version)\n        self._key = _legacy_cmpkey(self._version)\n\n    def __str__(self):\n        return self._version\n\n    def __repr__(self):\n        return \"<LegacyVersion({0})>\".format(repr(str(self)))\n\n    @property\n    def public(self):\n        return self._version\n\n    @property\n    def base_version(self):\n        return self._version\n\n    @property\n    def epoch(self):\n        return -1\n\n    @property\n    def release(self):\n        return None\n\n    @property\n    def pre(self):\n        return None\n\n    @property\n    def post(self):\n        return None\n\n    @property\n    def dev(self):\n        return None\n\n    @property\n    def local(self):\n        return None\n\n    @property\n    def is_prerelease(self):\n        return False\n\n    @property\n    def is_postrelease(self):\n        return False\n\n    @property\n    def is_devrelease(self):\n        return False\n\n\n_legacy_version_component_re = re.compile(r\"(\\d+ | [a-z]+ | \\.| -)\", re.VERBOSE)\n\n_legacy_version_replacement_map = {\n    \"pre\": \"c\",\n    \"preview\": \"c\",\n    \"-\": \"final-\",\n    \"rc\": \"c\",\n    \"dev\": \"@\",\n}\n\n\ndef _parse_version_parts(s):\n    for part in _legacy_version_component_re.split(s):\n        part = _legacy_version_replacement_map.get(part, part)\n\n        if not part or part == \".\":\n            continue\n\n        if part[:1] in \"0123456789\":\n            # pad for numeric comparison\n            yield part.zfill(8)\n        else:\n            yield \"*\" + part\n\n    # ensure that alpha/beta/candidate are before final\n    yield \"*final\"\n\n\ndef _legacy_cmpkey(version):\n    # We hardcode an epoch of -1 here. A PEP 440 version can only have a epoch\n    # greater than or equal to 0. This will effectively put the LegacyVersion,\n    # which uses the defacto standard originally implemented by setuptools,\n    # as before all PEP 440 versions.\n    epoch = -1\n\n    # This scheme is taken from pkg_resources.parse_version setuptools prior to\n    # it's adoption of the packaging library.\n    parts = []\n    for part in _parse_version_parts(version.lower()):\n        if part.startswith(\"*\"):\n            # remove \"-\" before a prerelease tag\n            if part < \"*final\":\n                while parts and parts[-1] == \"*final-\":\n                    parts.pop()\n\n            # remove trailing zeros from each series of numeric parts\n            while parts and parts[-1] == \"00000000\":\n                parts.pop()\n\n        parts.append(part)\n    parts = tuple(parts)\n\n    return epoch, parts\n\n\n# Deliberately not anchored to the start and end of the string, to make it\n# easier for 3rd party code to reuse\nVERSION_PATTERN = r\"\"\"\n    v?\n    (?:\n        (?:(?P<epoch>[0-9]+)!)?                           # epoch\n        (?P<release>[0-9]+(?:\\.[0-9]+)*)                  # release segment\n        (?P<pre>                                          # pre-release\n            [-_\\.]?\n            (?P<pre_l>(a|b|c|rc|alpha|beta|pre|preview))\n            [-_\\.]?\n            (?P<pre_n>[0-9]+)?\n        )?\n        (?P<post>                                         # post release\n            (?:-(?P<post_n1>[0-9]+))\n            |\n            (?:\n                [-_\\.]?\n                (?P<post_l>post|rev|r)\n                [-_\\.]?\n                (?P<post_n2>[0-9]+)?\n            )\n        )?\n        (?P<dev>                                          # dev release\n            [-_\\.]?\n            (?P<dev_l>dev)\n            [-_\\.]?\n            (?P<dev_n>[0-9]+)?\n        )?\n    )\n    (?:\\+(?P<local>[a-z0-9]+(?:[-_\\.][a-z0-9]+)*))?       # local version\n\"\"\"\n\n\nclass Version(_BaseVersion):\n\n    _regex = re.compile(r\"^\\s*\" + VERSION_PATTERN + r\"\\s*$\", re.VERBOSE | re.IGNORECASE)\n\n    def __init__(self, version):\n        # Validate the version and parse it into pieces\n        match = self._regex.search(version)\n        if not match:\n            raise InvalidVersion(\"Invalid version: '{0}'\".format(version))\n\n        # Store the parsed out pieces of the version\n        self._version = _Version(\n            epoch=int(match.group(\"epoch\")) if match.group(\"epoch\") else 0,\n            release=tuple(int(i) for i in match.group(\"release\").split(\".\")),\n            pre=_parse_letter_version(match.group(\"pre_l\"), match.group(\"pre_n\")),\n            post=_parse_letter_version(\n                match.group(\"post_l\"), match.group(\"post_n1\") or match.group(\"post_n2\")\n            ),\n            dev=_parse_letter_version(match.group(\"dev_l\"), match.group(\"dev_n\")),\n            local=_parse_local_version(match.group(\"local\")),\n        )\n\n        # Generate a key which will be used for sorting\n        self._key = _cmpkey(\n            self._version.epoch,\n            self._version.release,\n            self._version.pre,\n            self._version.post,\n            self._version.dev,\n            self._version.local,\n        )\n\n    def __repr__(self):\n        return \"<Version({0})>\".format(repr(str(self)))\n\n    def __str__(self):\n        parts = []\n\n        # Epoch\n        if self.epoch != 0:\n            parts.append(\"{0}!\".format(self.epoch))\n\n        # Release segment\n        parts.append(\".\".join(str(x) for x in self.release))\n\n        # Pre-release\n        if self.pre is not None:\n            parts.append(\"\".join(str(x) for x in self.pre))\n\n        # Post-release\n        if self.post is not None:\n            parts.append(\".post{0}\".format(self.post))\n\n        # Development release\n        if self.dev is not None:\n            parts.append(\".dev{0}\".format(self.dev))\n\n        # Local version segment\n        if self.local is not None:\n            parts.append(\"+{0}\".format(self.local))\n\n        return \"\".join(parts)\n\n    @property\n    def epoch(self):\n        return self._version.epoch\n\n    @property\n    def release(self):\n        return self._version.release\n\n    @property\n    def pre(self):\n        return self._version.pre\n\n    @property\n    def post(self):\n        return self._version.post[1] if self._version.post else None\n\n    @property\n    def dev(self):\n        return self._version.dev[1] if self._version.dev else None\n\n    @property\n    def local(self):\n        if self._version.local:\n            return \".\".join(str(x) for x in self._version.local)\n        else:\n            return None\n\n    @property\n    def public(self):\n        return str(self).split(\"+\", 1)[0]\n\n    @property\n    def base_version(self):\n        parts = []\n\n        # Epoch\n        if self.epoch != 0:\n            parts.append(\"{0}!\".format(self.epoch))\n\n        # Release segment\n        parts.append(\".\".join(str(x) for x in self.release))\n\n        return \"\".join(parts)\n\n    @property\n    def is_prerelease(self):\n        return self.dev is not None or self.pre is not None\n\n    @property\n    def is_postrelease(self):\n        return self.post is not None\n\n    @property\n    def is_devrelease(self):\n        return self.dev is not None\n\n\ndef _parse_letter_version(letter, number):\n    if letter:\n        # We consider there to be an implicit 0 in a pre-release if there is\n        # not a numeral associated with it.\n        if number is None:\n            number = 0\n\n        # We normalize any letters to their lower case form\n        letter = letter.lower()\n\n        # We consider some words to be alternate spellings of other words and\n        # in those cases we want to normalize the spellings to our preferred\n        # spelling.\n        if letter == \"alpha\":\n            letter = \"a\"\n        elif letter == \"beta\":\n            letter = \"b\"\n        elif letter in [\"c\", \"pre\", \"preview\"]:\n            letter = \"rc\"\n        elif letter in [\"rev\", \"r\"]:\n            letter = \"post\"\n\n        return letter, int(number)\n    if not letter and number:\n        # We assume if we are given a number, but we are not given a letter\n        # then this is using the implicit post release syntax (e.g. 1.0-1)\n        letter = \"post\"\n\n        return letter, int(number)\n\n\n_local_version_separators = re.compile(r\"[\\._-]\")\n\n\ndef _parse_local_version(local):\n    \"\"\"\n    Takes a string like abc.1.twelve and turns it into (\"abc\", 1, \"twelve\").\n    \"\"\"\n    if local is not None:\n        return tuple(\n            part.lower() if not part.isdigit() else int(part)\n            for part in _local_version_separators.split(local)\n        )\n\n\ndef _cmpkey(epoch, release, pre, post, dev, local):\n    # When we compare a release version, we want to compare it with all of the\n    # trailing zeros removed. So we'll use a reverse the list, drop all the now\n    # leading zeros until we come to something non zero, then take the rest\n    # re-reverse it back into the correct order and make it a tuple and use\n    # that for our sorting key.\n    release = tuple(\n        reversed(list(itertools.dropwhile(lambda x: x == 0, reversed(release))))\n    )\n\n    # We need to \"trick\" the sorting algorithm to put 1.0.dev0 before 1.0a0.\n    # We'll do this by abusing the pre segment, but we _only_ want to do this\n    # if there is not a pre or a post segment. If we have one of those then\n    # the normal sorting rules will handle this case correctly.\n    if pre is None and post is None and dev is not None:\n        pre = -Infinity\n    # Versions without a pre-release (except as noted above) should sort after\n    # those with one.\n    elif pre is None:\n        pre = Infinity\n\n    # Versions without a post segment should sort before those with one.\n    if post is None:\n        post = -Infinity\n\n    # Versions without a development segment should sort after those with one.\n    if dev is None:\n        dev = Infinity\n\n    if local is None:\n        # Versions without a local segment should sort before those with one.\n        local = -Infinity\n    else:\n        # Versions with a local segment need that segment parsed to implement\n        # the sorting rules in PEP440.\n        # - Alpha numeric segments sort before numeric segments\n        # - Alpha numeric segments sort lexicographically\n        # - Numeric segments sort numerically\n        # - Shorter versions sort before longer versions when the prefixes\n        #   match exactly\n        local = tuple((i, \"\") if isinstance(i, int) else (-Infinity, i) for i in local)\n\n    return epoch, release, pre, post, dev, local\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/version.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/version.py	(date 1602088701593)
@@ -7,8 +7,35 @@
 import itertools
 import re
 
-from ._structures import Infinity
+from ._structures import Infinity, NegativeInfinity
+from ._typing import TYPE_CHECKING
+
+if TYPE_CHECKING:  # pragma: no cover
+    from typing import Callable, Iterator, List, Optional, SupportsInt, Tuple, Union
 
+    from ._structures import InfinityType, NegativeInfinityType
+
+    InfiniteTypes = Union[InfinityType, NegativeInfinityType]
+    PrePostDevType = Union[InfiniteTypes, Tuple[str, int]]
+    SubLocalType = Union[InfiniteTypes, int, str]
+    LocalType = Union[
+        NegativeInfinityType,
+        Tuple[
+            Union[
+                SubLocalType,
+                Tuple[SubLocalType, str],
+                Tuple[NegativeInfinityType, SubLocalType],
+            ],
+            ...,
+        ],
+    ]
+    CmpKey = Tuple[
+        int, Tuple[int, ...], PrePostDevType, PrePostDevType, PrePostDevType, LocalType
+    ]
+    LegacyCmpKey = Tuple[int, Tuple[str, ...]]
+    VersionComparisonMethod = Callable[
+        [Union[CmpKey, LegacyCmpKey], Union[CmpKey, LegacyCmpKey]], bool
+    ]
 
 __all__ = ["parse", "Version", "LegacyVersion", "InvalidVersion", "VERSION_PATTERN"]
 
@@ -19,6 +46,7 @@
 
 
 def parse(version):
+    # type: (str) -> Union[LegacyVersion, Version]
     """
     Parse the given version string and return either a :class:`Version` object
     or a :class:`LegacyVersion` object depending on if the given version is
@@ -37,28 +65,38 @@
 
 
 class _BaseVersion(object):
+    _key = None  # type: Union[CmpKey, LegacyCmpKey]
+
     def __hash__(self):
+        # type: () -> int
         return hash(self._key)
 
     def __lt__(self, other):
+        # type: (_BaseVersion) -> bool
         return self._compare(other, lambda s, o: s < o)
 
     def __le__(self, other):
+        # type: (_BaseVersion) -> bool
         return self._compare(other, lambda s, o: s <= o)
 
     def __eq__(self, other):
+        # type: (object) -> bool
         return self._compare(other, lambda s, o: s == o)
 
     def __ge__(self, other):
+        # type: (_BaseVersion) -> bool
         return self._compare(other, lambda s, o: s >= o)
 
     def __gt__(self, other):
+        # type: (_BaseVersion) -> bool
         return self._compare(other, lambda s, o: s > o)
 
     def __ne__(self, other):
+        # type: (object) -> bool
         return self._compare(other, lambda s, o: s != o)
 
     def _compare(self, other, method):
+        # type: (object, VersionComparisonMethod) -> Union[bool, NotImplemented]
         if not isinstance(other, _BaseVersion):
             return NotImplemented
 
@@ -67,57 +105,71 @@
 
 class LegacyVersion(_BaseVersion):
     def __init__(self, version):
+        # type: (str) -> None
         self._version = str(version)
         self._key = _legacy_cmpkey(self._version)
 
     def __str__(self):
+        # type: () -> str
         return self._version
 
     def __repr__(self):
+        # type: () -> str
         return "<LegacyVersion({0})>".format(repr(str(self)))
 
     @property
     def public(self):
+        # type: () -> str
         return self._version
 
     @property
     def base_version(self):
+        # type: () -> str
         return self._version
 
     @property
     def epoch(self):
+        # type: () -> int
         return -1
 
     @property
     def release(self):
+        # type: () -> None
         return None
 
     @property
     def pre(self):
+        # type: () -> None
         return None
 
     @property
     def post(self):
+        # type: () -> None
         return None
 
     @property
     def dev(self):
+        # type: () -> None
         return None
 
     @property
     def local(self):
+        # type: () -> None
         return None
 
     @property
     def is_prerelease(self):
+        # type: () -> bool
         return False
 
     @property
     def is_postrelease(self):
+        # type: () -> bool
         return False
 
     @property
     def is_devrelease(self):
+        # type: () -> bool
         return False
 
 
@@ -133,6 +185,7 @@
 
 
 def _parse_version_parts(s):
+    # type: (str) -> Iterator[str]
     for part in _legacy_version_component_re.split(s):
         part = _legacy_version_replacement_map.get(part, part)
 
@@ -150,6 +203,8 @@
 
 
 def _legacy_cmpkey(version):
+    # type: (str) -> LegacyCmpKey
+
     # We hardcode an epoch of -1 here. A PEP 440 version can only have a epoch
     # greater than or equal to 0. This will effectively put the LegacyVersion,
     # which uses the defacto standard originally implemented by setuptools,
@@ -158,7 +213,7 @@
 
     # This scheme is taken from pkg_resources.parse_version setuptools prior to
     # it's adoption of the packaging library.
-    parts = []
+    parts = []  # type: List[str]
     for part in _parse_version_parts(version.lower()):
         if part.startswith("*"):
             # remove "-" before a prerelease tag
@@ -171,9 +226,8 @@
                 parts.pop()
 
         parts.append(part)
-    parts = tuple(parts)
 
-    return epoch, parts
+    return epoch, tuple(parts)
 
 
 # Deliberately not anchored to the start and end of the string, to make it
@@ -215,6 +269,8 @@
     _regex = re.compile(r"^\s*" + VERSION_PATTERN + r"\s*$", re.VERBOSE | re.IGNORECASE)
 
     def __init__(self, version):
+        # type: (str) -> None
+
         # Validate the version and parse it into pieces
         match = self._regex.search(version)
         if not match:
@@ -243,9 +299,11 @@
         )
 
     def __repr__(self):
+        # type: () -> str
         return "<Version({0})>".format(repr(str(self)))
 
     def __str__(self):
+        # type: () -> str
         parts = []
 
         # Epoch
@@ -275,26 +333,35 @@
 
     @property
     def epoch(self):
-        return self._version.epoch
+        # type: () -> int
+        _epoch = self._version.epoch  # type: int
+        return _epoch
 
     @property
     def release(self):
-        return self._version.release
+        # type: () -> Tuple[int, ...]
+        _release = self._version.release  # type: Tuple[int, ...]
+        return _release
 
     @property
     def pre(self):
-        return self._version.pre
+        # type: () -> Optional[Tuple[str, int]]
+        _pre = self._version.pre  # type: Optional[Tuple[str, int]]
+        return _pre
 
     @property
     def post(self):
+        # type: () -> Optional[Tuple[str, int]]
         return self._version.post[1] if self._version.post else None
 
     @property
     def dev(self):
+        # type: () -> Optional[Tuple[str, int]]
         return self._version.dev[1] if self._version.dev else None
 
     @property
     def local(self):
+        # type: () -> Optional[str]
         if self._version.local:
             return ".".join(str(x) for x in self._version.local)
         else:
@@ -302,10 +369,12 @@
 
     @property
     def public(self):
+        # type: () -> str
         return str(self).split("+", 1)[0]
 
     @property
     def base_version(self):
+        # type: () -> str
         parts = []
 
         # Epoch
@@ -319,18 +388,41 @@
 
     @property
     def is_prerelease(self):
+        # type: () -> bool
         return self.dev is not None or self.pre is not None
 
     @property
     def is_postrelease(self):
+        # type: () -> bool
         return self.post is not None
 
     @property
     def is_devrelease(self):
+        # type: () -> bool
         return self.dev is not None
 
+    @property
+    def major(self):
+        # type: () -> int
+        return self.release[0] if len(self.release) >= 1 else 0
+
+    @property
+    def minor(self):
+        # type: () -> int
+        return self.release[1] if len(self.release) >= 2 else 0
+
+    @property
+    def micro(self):
+        # type: () -> int
+        return self.release[2] if len(self.release) >= 3 else 0
+
 
-def _parse_letter_version(letter, number):
+def _parse_letter_version(
+    letter,  # type: str
+    number,  # type: Union[str, bytes, SupportsInt]
+):
+    # type: (...) -> Optional[Tuple[str, int]]
+
     if letter:
         # We consider there to be an implicit 0 in a pre-release if there is
         # not a numeral associated with it.
@@ -360,11 +452,14 @@
 
         return letter, int(number)
 
+    return None
+
 
 _local_version_separators = re.compile(r"[\._-]")
 
 
 def _parse_local_version(local):
+    # type: (str) -> Optional[LocalType]
     """
     Takes a string like abc.1.twelve and turns it into ("abc", 1, "twelve").
     """
@@ -373,15 +468,25 @@
             part.lower() if not part.isdigit() else int(part)
             for part in _local_version_separators.split(local)
         )
+    return None
 
 
-def _cmpkey(epoch, release, pre, post, dev, local):
+def _cmpkey(
+    epoch,  # type: int
+    release,  # type: Tuple[int, ...]
+    pre,  # type: Optional[Tuple[str, int]]
+    post,  # type: Optional[Tuple[str, int]]
+    dev,  # type: Optional[Tuple[str, int]]
+    local,  # type: Optional[Tuple[SubLocalType]]
+):
+    # type: (...) -> CmpKey
+
     # When we compare a release version, we want to compare it with all of the
     # trailing zeros removed. So we'll use a reverse the list, drop all the now
     # leading zeros until we come to something non zero, then take the rest
     # re-reverse it back into the correct order and make it a tuple and use
     # that for our sorting key.
-    release = tuple(
+    _release = tuple(
         reversed(list(itertools.dropwhile(lambda x: x == 0, reversed(release))))
     )
 
@@ -390,23 +495,31 @@
     # if there is not a pre or a post segment. If we have one of those then
     # the normal sorting rules will handle this case correctly.
     if pre is None and post is None and dev is not None:
-        pre = -Infinity
+        _pre = NegativeInfinity  # type: PrePostDevType
     # Versions without a pre-release (except as noted above) should sort after
     # those with one.
     elif pre is None:
-        pre = Infinity
+        _pre = Infinity
+    else:
+        _pre = pre
 
     # Versions without a post segment should sort before those with one.
     if post is None:
-        post = -Infinity
+        _post = NegativeInfinity  # type: PrePostDevType
+
+    else:
+        _post = post
 
     # Versions without a development segment should sort after those with one.
     if dev is None:
-        dev = Infinity
+        _dev = Infinity  # type: PrePostDevType
+
+    else:
+        _dev = dev
 
     if local is None:
         # Versions without a local segment should sort before those with one.
-        local = -Infinity
+        _local = NegativeInfinity  # type: LocalType
     else:
         # Versions with a local segment need that segment parsed to implement
         # the sorting rules in PEP440.
@@ -415,6 +528,8 @@
         # - Numeric segments sort numerically
         # - Shorter versions sort before longer versions when the prefixes
         #   match exactly
-        local = tuple((i, "") if isinstance(i, int) else (-Infinity, i) for i in local)
+        _local = tuple(
+            (i, "") if isinstance(i, int) else (NegativeInfinity, i) for i in local
+        )
 
-    return epoch, release, pre, post, dev, local
+    return epoch, _release, _pre, _post, _dev, _local
Index: env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/markers.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import absolute_import, division, print_function\n\nimport operator\nimport os\nimport platform\nimport sys\n\nfrom pkg_resources.extern.pyparsing import ParseException, ParseResults, stringStart, stringEnd\nfrom pkg_resources.extern.pyparsing import ZeroOrMore, Group, Forward, QuotedString\nfrom pkg_resources.extern.pyparsing import Literal as L  # noqa\n\nfrom ._compat import string_types\nfrom .specifiers import Specifier, InvalidSpecifier\n\n\n__all__ = [\n    \"InvalidMarker\",\n    \"UndefinedComparison\",\n    \"UndefinedEnvironmentName\",\n    \"Marker\",\n    \"default_environment\",\n]\n\n\nclass InvalidMarker(ValueError):\n    \"\"\"\n    An invalid marker was found, users should refer to PEP 508.\n    \"\"\"\n\n\nclass UndefinedComparison(ValueError):\n    \"\"\"\n    An invalid operation was attempted on a value that doesn't support it.\n    \"\"\"\n\n\nclass UndefinedEnvironmentName(ValueError):\n    \"\"\"\n    A name was attempted to be used that does not exist inside of the\n    environment.\n    \"\"\"\n\n\nclass Node(object):\n    def __init__(self, value):\n        self.value = value\n\n    def __str__(self):\n        return str(self.value)\n\n    def __repr__(self):\n        return \"<{0}({1!r})>\".format(self.__class__.__name__, str(self))\n\n    def serialize(self):\n        raise NotImplementedError\n\n\nclass Variable(Node):\n    def serialize(self):\n        return str(self)\n\n\nclass Value(Node):\n    def serialize(self):\n        return '\"{0}\"'.format(self)\n\n\nclass Op(Node):\n    def serialize(self):\n        return str(self)\n\n\nVARIABLE = (\n    L(\"implementation_version\")\n    | L(\"platform_python_implementation\")\n    | L(\"implementation_name\")\n    | L(\"python_full_version\")\n    | L(\"platform_release\")\n    | L(\"platform_version\")\n    | L(\"platform_machine\")\n    | L(\"platform_system\")\n    | L(\"python_version\")\n    | L(\"sys_platform\")\n    | L(\"os_name\")\n    | L(\"os.name\")\n    | L(\"sys.platform\")  # PEP-345\n    | L(\"platform.version\")  # PEP-345\n    | L(\"platform.machine\")  # PEP-345\n    | L(\"platform.python_implementation\")  # PEP-345\n    | L(\"python_implementation\")  # PEP-345\n    | L(\"extra\")  # undocumented setuptools legacy\n)\nALIASES = {\n    \"os.name\": \"os_name\",\n    \"sys.platform\": \"sys_platform\",\n    \"platform.version\": \"platform_version\",\n    \"platform.machine\": \"platform_machine\",\n    \"platform.python_implementation\": \"platform_python_implementation\",\n    \"python_implementation\": \"platform_python_implementation\",\n}\nVARIABLE.setParseAction(lambda s, l, t: Variable(ALIASES.get(t[0], t[0])))\n\nVERSION_CMP = (\n    L(\"===\") | L(\"==\") | L(\">=\") | L(\"<=\") | L(\"!=\") | L(\"~=\") | L(\">\") | L(\"<\")\n)\n\nMARKER_OP = VERSION_CMP | L(\"not in\") | L(\"in\")\nMARKER_OP.setParseAction(lambda s, l, t: Op(t[0]))\n\nMARKER_VALUE = QuotedString(\"'\") | QuotedString('\"')\nMARKER_VALUE.setParseAction(lambda s, l, t: Value(t[0]))\n\nBOOLOP = L(\"and\") | L(\"or\")\n\nMARKER_VAR = VARIABLE | MARKER_VALUE\n\nMARKER_ITEM = Group(MARKER_VAR + MARKER_OP + MARKER_VAR)\nMARKER_ITEM.setParseAction(lambda s, l, t: tuple(t[0]))\n\nLPAREN = L(\"(\").suppress()\nRPAREN = L(\")\").suppress()\n\nMARKER_EXPR = Forward()\nMARKER_ATOM = MARKER_ITEM | Group(LPAREN + MARKER_EXPR + RPAREN)\nMARKER_EXPR << MARKER_ATOM + ZeroOrMore(BOOLOP + MARKER_EXPR)\n\nMARKER = stringStart + MARKER_EXPR + stringEnd\n\n\ndef _coerce_parse_result(results):\n    if isinstance(results, ParseResults):\n        return [_coerce_parse_result(i) for i in results]\n    else:\n        return results\n\n\ndef _format_marker(marker, first=True):\n    assert isinstance(marker, (list, tuple, string_types))\n\n    # Sometimes we have a structure like [[...]] which is a single item list\n    # where the single item is itself it's own list. In that case we want skip\n    # the rest of this function so that we don't get extraneous () on the\n    # outside.\n    if (\n        isinstance(marker, list)\n        and len(marker) == 1\n        and isinstance(marker[0], (list, tuple))\n    ):\n        return _format_marker(marker[0])\n\n    if isinstance(marker, list):\n        inner = (_format_marker(m, first=False) for m in marker)\n        if first:\n            return \" \".join(inner)\n        else:\n            return \"(\" + \" \".join(inner) + \")\"\n    elif isinstance(marker, tuple):\n        return \" \".join([m.serialize() for m in marker])\n    else:\n        return marker\n\n\n_operators = {\n    \"in\": lambda lhs, rhs: lhs in rhs,\n    \"not in\": lambda lhs, rhs: lhs not in rhs,\n    \"<\": operator.lt,\n    \"<=\": operator.le,\n    \"==\": operator.eq,\n    \"!=\": operator.ne,\n    \">=\": operator.ge,\n    \">\": operator.gt,\n}\n\n\ndef _eval_op(lhs, op, rhs):\n    try:\n        spec = Specifier(\"\".join([op.serialize(), rhs]))\n    except InvalidSpecifier:\n        pass\n    else:\n        return spec.contains(lhs)\n\n    oper = _operators.get(op.serialize())\n    if oper is None:\n        raise UndefinedComparison(\n            \"Undefined {0!r} on {1!r} and {2!r}.\".format(op, lhs, rhs)\n        )\n\n    return oper(lhs, rhs)\n\n\n_undefined = object()\n\n\ndef _get_env(environment, name):\n    value = environment.get(name, _undefined)\n\n    if value is _undefined:\n        raise UndefinedEnvironmentName(\n            \"{0!r} does not exist in evaluation environment.\".format(name)\n        )\n\n    return value\n\n\ndef _evaluate_markers(markers, environment):\n    groups = [[]]\n\n    for marker in markers:\n        assert isinstance(marker, (list, tuple, string_types))\n\n        if isinstance(marker, list):\n            groups[-1].append(_evaluate_markers(marker, environment))\n        elif isinstance(marker, tuple):\n            lhs, op, rhs = marker\n\n            if isinstance(lhs, Variable):\n                lhs_value = _get_env(environment, lhs.value)\n                rhs_value = rhs.value\n            else:\n                lhs_value = lhs.value\n                rhs_value = _get_env(environment, rhs.value)\n\n            groups[-1].append(_eval_op(lhs_value, op, rhs_value))\n        else:\n            assert marker in [\"and\", \"or\"]\n            if marker == \"or\":\n                groups.append([])\n\n    return any(all(item) for item in groups)\n\n\ndef format_full_version(info):\n    version = \"{0.major}.{0.minor}.{0.micro}\".format(info)\n    kind = info.releaselevel\n    if kind != \"final\":\n        version += kind[0] + str(info.serial)\n    return version\n\n\ndef default_environment():\n    if hasattr(sys, \"implementation\"):\n        iver = format_full_version(sys.implementation.version)\n        implementation_name = sys.implementation.name\n    else:\n        iver = \"0\"\n        implementation_name = \"\"\n\n    return {\n        \"implementation_name\": implementation_name,\n        \"implementation_version\": iver,\n        \"os_name\": os.name,\n        \"platform_machine\": platform.machine(),\n        \"platform_release\": platform.release(),\n        \"platform_system\": platform.system(),\n        \"platform_version\": platform.version(),\n        \"python_full_version\": platform.python_version(),\n        \"platform_python_implementation\": platform.python_implementation(),\n        \"python_version\": \".\".join(platform.python_version_tuple()[:2]),\n        \"sys_platform\": sys.platform,\n    }\n\n\nclass Marker(object):\n    def __init__(self, marker):\n        try:\n            self._markers = _coerce_parse_result(MARKER.parseString(marker))\n        except ParseException as e:\n            err_str = \"Invalid marker: {0!r}, parse error at {1!r}\".format(\n                marker, marker[e.loc : e.loc + 8]\n            )\n            raise InvalidMarker(err_str)\n\n    def __str__(self):\n        return _format_marker(self._markers)\n\n    def __repr__(self):\n        return \"<Marker({0!r})>\".format(str(self))\n\n    def evaluate(self, environment=None):\n        \"\"\"Evaluate a marker.\n\n        Return the boolean from evaluating the given marker against the\n        environment. environment is an optional argument to override all or\n        part of the determined environment.\n\n        The environment is determined from the current Python process.\n        \"\"\"\n        current_environment = default_environment()\n        if environment is not None:\n            current_environment.update(environment)\n\n        return _evaluate_markers(self._markers, current_environment)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/markers.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/markers.py	(date 1602088701593)
@@ -13,8 +13,14 @@
 from pkg_resources.extern.pyparsing import Literal as L  # noqa
 
 from ._compat import string_types
+from ._typing import TYPE_CHECKING
 from .specifiers import Specifier, InvalidSpecifier
 
+if TYPE_CHECKING:  # pragma: no cover
+    from typing import Any, Callable, Dict, List, Optional, Tuple, Union
+
+    Operator = Callable[[str, str], bool]
+
 
 __all__ = [
     "InvalidMarker",
@@ -46,30 +52,37 @@
 
 class Node(object):
     def __init__(self, value):
+        # type: (Any) -> None
         self.value = value
 
     def __str__(self):
+        # type: () -> str
         return str(self.value)
 
     def __repr__(self):
+        # type: () -> str
         return "<{0}({1!r})>".format(self.__class__.__name__, str(self))
 
     def serialize(self):
+        # type: () -> str
         raise NotImplementedError
 
 
 class Variable(Node):
     def serialize(self):
+        # type: () -> str
         return str(self)
 
 
 class Value(Node):
     def serialize(self):
+        # type: () -> str
         return '"{0}"'.format(self)
 
 
 class Op(Node):
     def serialize(self):
+        # type: () -> str
         return str(self)
 
 
@@ -85,13 +98,13 @@
     | L("python_version")
     | L("sys_platform")
     | L("os_name")
-    | L("os.name")
+    | L("os.name")  # PEP-345
     | L("sys.platform")  # PEP-345
     | L("platform.version")  # PEP-345
     | L("platform.machine")  # PEP-345
     | L("platform.python_implementation")  # PEP-345
-    | L("python_implementation")  # PEP-345
-    | L("extra")  # undocumented setuptools legacy
+    | L("python_implementation")  # undocumented setuptools legacy
+    | L("extra")  # PEP-508
 )
 ALIASES = {
     "os.name": "os_name",
@@ -131,6 +144,7 @@
 
 
 def _coerce_parse_result(results):
+    # type: (Union[ParseResults, List[Any]]) -> List[Any]
     if isinstance(results, ParseResults):
         return [_coerce_parse_result(i) for i in results]
     else:
@@ -138,6 +152,8 @@
 
 
 def _format_marker(marker, first=True):
+    # type: (Union[List[str], Tuple[Node, ...], str], Optional[bool]) -> str
+
     assert isinstance(marker, (list, tuple, string_types))
 
     # Sometimes we have a structure like [[...]] which is a single item list
@@ -172,10 +188,11 @@
     "!=": operator.ne,
     ">=": operator.ge,
     ">": operator.gt,
-}
+}  # type: Dict[str, Operator]
 
 
 def _eval_op(lhs, op, rhs):
+    # type: (str, Op, str) -> bool
     try:
         spec = Specifier("".join([op.serialize(), rhs]))
     except InvalidSpecifier:
@@ -183,7 +200,7 @@
     else:
         return spec.contains(lhs)
 
-    oper = _operators.get(op.serialize())
+    oper = _operators.get(op.serialize())  # type: Optional[Operator]
     if oper is None:
         raise UndefinedComparison(
             "Undefined {0!r} on {1!r} and {2!r}.".format(op, lhs, rhs)
@@ -192,13 +209,18 @@
     return oper(lhs, rhs)
 
 
-_undefined = object()
+class Undefined(object):
+    pass
+
+
+_undefined = Undefined()
 
 
 def _get_env(environment, name):
-    value = environment.get(name, _undefined)
+    # type: (Dict[str, str], str) -> str
+    value = environment.get(name, _undefined)  # type: Union[str, Undefined]
 
-    if value is _undefined:
+    if isinstance(value, Undefined):
         raise UndefinedEnvironmentName(
             "{0!r} does not exist in evaluation environment.".format(name)
         )
@@ -207,7 +229,8 @@
 
 
 def _evaluate_markers(markers, environment):
-    groups = [[]]
+    # type: (List[Any], Dict[str, str]) -> bool
+    groups = [[]]  # type: List[List[bool]]
 
     for marker in markers:
         assert isinstance(marker, (list, tuple, string_types))
@@ -234,6 +257,7 @@
 
 
 def format_full_version(info):
+    # type: (sys._version_info) -> str
     version = "{0.major}.{0.minor}.{0.micro}".format(info)
     kind = info.releaselevel
     if kind != "final":
@@ -242,9 +266,13 @@
 
 
 def default_environment():
+    # type: () -> Dict[str, str]
     if hasattr(sys, "implementation"):
-        iver = format_full_version(sys.implementation.version)
-        implementation_name = sys.implementation.name
+        # Ignoring the `sys.implementation` reference for type checking due to
+        # mypy not liking that the attribute doesn't exist in Python 2.7 when
+        # run with the `--py27` flag.
+        iver = format_full_version(sys.implementation.version)  # type: ignore
+        implementation_name = sys.implementation.name  # type: ignore
     else:
         iver = "0"
         implementation_name = ""
@@ -266,6 +294,7 @@
 
 class Marker(object):
     def __init__(self, marker):
+        # type: (str) -> None
         try:
             self._markers = _coerce_parse_result(MARKER.parseString(marker))
         except ParseException as e:
@@ -275,12 +304,15 @@
             raise InvalidMarker(err_str)
 
     def __str__(self):
+        # type: () -> str
         return _format_marker(self._markers)
 
     def __repr__(self):
+        # type: () -> str
         return "<Marker({0!r})>".format(str(self))
 
     def evaluate(self, environment=None):
+        # type: (Optional[Dict[str, str]]) -> bool
         """Evaluate a marker.
 
         Return the boolean from evaluating the given marker against the
Index: env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/requirements.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import absolute_import, division, print_function\n\nimport string\nimport re\n\nfrom pkg_resources.extern.pyparsing import stringStart, stringEnd, originalTextFor, ParseException\nfrom pkg_resources.extern.pyparsing import ZeroOrMore, Word, Optional, Regex, Combine\nfrom pkg_resources.extern.pyparsing import Literal as L  # noqa\nfrom pkg_resources.extern.six.moves.urllib import parse as urlparse\n\nfrom .markers import MARKER_EXPR, Marker\nfrom .specifiers import LegacySpecifier, Specifier, SpecifierSet\n\n\nclass InvalidRequirement(ValueError):\n    \"\"\"\n    An invalid requirement was found, users should refer to PEP 508.\n    \"\"\"\n\n\nALPHANUM = Word(string.ascii_letters + string.digits)\n\nLBRACKET = L(\"[\").suppress()\nRBRACKET = L(\"]\").suppress()\nLPAREN = L(\"(\").suppress()\nRPAREN = L(\")\").suppress()\nCOMMA = L(\",\").suppress()\nSEMICOLON = L(\";\").suppress()\nAT = L(\"@\").suppress()\n\nPUNCTUATION = Word(\"-_.\")\nIDENTIFIER_END = ALPHANUM | (ZeroOrMore(PUNCTUATION) + ALPHANUM)\nIDENTIFIER = Combine(ALPHANUM + ZeroOrMore(IDENTIFIER_END))\n\nNAME = IDENTIFIER(\"name\")\nEXTRA = IDENTIFIER\n\nURI = Regex(r\"[^ ]+\")(\"url\")\nURL = AT + URI\n\nEXTRAS_LIST = EXTRA + ZeroOrMore(COMMA + EXTRA)\nEXTRAS = (LBRACKET + Optional(EXTRAS_LIST) + RBRACKET)(\"extras\")\n\nVERSION_PEP440 = Regex(Specifier._regex_str, re.VERBOSE | re.IGNORECASE)\nVERSION_LEGACY = Regex(LegacySpecifier._regex_str, re.VERBOSE | re.IGNORECASE)\n\nVERSION_ONE = VERSION_PEP440 ^ VERSION_LEGACY\nVERSION_MANY = Combine(\n    VERSION_ONE + ZeroOrMore(COMMA + VERSION_ONE), joinString=\",\", adjacent=False\n)(\"_raw_spec\")\n_VERSION_SPEC = Optional(((LPAREN + VERSION_MANY + RPAREN) | VERSION_MANY))\n_VERSION_SPEC.setParseAction(lambda s, l, t: t._raw_spec or \"\")\n\nVERSION_SPEC = originalTextFor(_VERSION_SPEC)(\"specifier\")\nVERSION_SPEC.setParseAction(lambda s, l, t: t[1])\n\nMARKER_EXPR = originalTextFor(MARKER_EXPR())(\"marker\")\nMARKER_EXPR.setParseAction(\n    lambda s, l, t: Marker(s[t._original_start : t._original_end])\n)\nMARKER_SEPARATOR = SEMICOLON\nMARKER = MARKER_SEPARATOR + MARKER_EXPR\n\nVERSION_AND_MARKER = VERSION_SPEC + Optional(MARKER)\nURL_AND_MARKER = URL + Optional(MARKER)\n\nNAMED_REQUIREMENT = NAME + Optional(EXTRAS) + (URL_AND_MARKER | VERSION_AND_MARKER)\n\nREQUIREMENT = stringStart + NAMED_REQUIREMENT + stringEnd\n# pkg_resources.extern.pyparsing isn't thread safe during initialization, so we do it eagerly, see\n# issue #104\nREQUIREMENT.parseString(\"x[]\")\n\n\nclass Requirement(object):\n    \"\"\"Parse a requirement.\n\n    Parse a given requirement string into its parts, such as name, specifier,\n    URL, and extras. Raises InvalidRequirement on a badly-formed requirement\n    string.\n    \"\"\"\n\n    # TODO: Can we test whether something is contained within a requirement?\n    #       If so how do we do that? Do we need to test against the _name_ of\n    #       the thing as well as the version? What about the markers?\n    # TODO: Can we normalize the name and extra name?\n\n    def __init__(self, requirement_string):\n        try:\n            req = REQUIREMENT.parseString(requirement_string)\n        except ParseException as e:\n            raise InvalidRequirement(\n                'Parse error at \"{0!r}\": {1}'.format(\n                    requirement_string[e.loc : e.loc + 8], e.msg\n                )\n            )\n\n        self.name = req.name\n        if req.url:\n            parsed_url = urlparse.urlparse(req.url)\n            if parsed_url.scheme == \"file\":\n                if urlparse.urlunparse(parsed_url) != req.url:\n                    raise InvalidRequirement(\"Invalid URL given\")\n            elif not (parsed_url.scheme and parsed_url.netloc) or (\n                not parsed_url.scheme and not parsed_url.netloc\n            ):\n                raise InvalidRequirement(\"Invalid URL: {0}\".format(req.url))\n            self.url = req.url\n        else:\n            self.url = None\n        self.extras = set(req.extras.asList() if req.extras else [])\n        self.specifier = SpecifierSet(req.specifier)\n        self.marker = req.marker if req.marker else None\n\n    def __str__(self):\n        parts = [self.name]\n\n        if self.extras:\n            parts.append(\"[{0}]\".format(\",\".join(sorted(self.extras))))\n\n        if self.specifier:\n            parts.append(str(self.specifier))\n\n        if self.url:\n            parts.append(\"@ {0}\".format(self.url))\n            if self.marker:\n                parts.append(\" \")\n\n        if self.marker:\n            parts.append(\"; {0}\".format(self.marker))\n\n        return \"\".join(parts)\n\n    def __repr__(self):\n        return \"<Requirement({0!r})>\".format(str(self))\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/requirements.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/requirements.py	(date 1602088701593)
@@ -9,11 +9,15 @@
 from pkg_resources.extern.pyparsing import stringStart, stringEnd, originalTextFor, ParseException
 from pkg_resources.extern.pyparsing import ZeroOrMore, Word, Optional, Regex, Combine
 from pkg_resources.extern.pyparsing import Literal as L  # noqa
-from pkg_resources.extern.six.moves.urllib import parse as urlparse
+from urllib import parse as urlparse
 
+from ._typing import TYPE_CHECKING
 from .markers import MARKER_EXPR, Marker
 from .specifiers import LegacySpecifier, Specifier, SpecifierSet
 
+if TYPE_CHECKING:  # pragma: no cover
+    from typing import List
+
 
 class InvalidRequirement(ValueError):
     """
@@ -89,6 +93,7 @@
     # TODO: Can we normalize the name and extra name?
 
     def __init__(self, requirement_string):
+        # type: (str) -> None
         try:
             req = REQUIREMENT.parseString(requirement_string)
         except ParseException as e:
@@ -116,7 +121,8 @@
         self.marker = req.marker if req.marker else None
 
     def __str__(self):
-        parts = [self.name]
+        # type: () -> str
+        parts = [self.name]  # type: List[str]
 
         if self.extras:
             parts.append("[{0}]".format(",".join(sorted(self.extras))))
@@ -135,4 +141,5 @@
         return "".join(parts)
 
     def __repr__(self):
+        # type: () -> str
         return "<Requirement({0!r})>".format(str(self))
Index: env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/_compat.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import absolute_import, division, print_function\n\nimport sys\n\n\nPY2 = sys.version_info[0] == 2\nPY3 = sys.version_info[0] == 3\n\n# flake8: noqa\n\nif PY3:\n    string_types = (str,)\nelse:\n    string_types = (basestring,)\n\n\ndef with_metaclass(meta, *bases):\n    \"\"\"\n    Create a base class with a metaclass.\n    \"\"\"\n    # This requires a bit of explanation: the basic idea is to make a dummy\n    # metaclass for one level of class instantiation that replaces itself with\n    # the actual metaclass.\n    class metaclass(meta):\n        def __new__(cls, name, this_bases, d):\n            return meta(name, bases, d)\n\n    return type.__new__(metaclass, \"temporary_class\", (), {})\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/_compat.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/_compat.py	(date 1602088701593)
@@ -5,6 +5,11 @@
 
 import sys
 
+from ._typing import TYPE_CHECKING
+
+if TYPE_CHECKING:  # pragma: no cover
+    from typing import Any, Dict, Tuple, Type
+
 
 PY2 = sys.version_info[0] == 2
 PY3 = sys.version_info[0] == 3
@@ -18,14 +23,16 @@
 
 
 def with_metaclass(meta, *bases):
+    # type: (Type[Any], Tuple[Type[Any], ...]) -> Any
     """
     Create a base class with a metaclass.
     """
     # This requires a bit of explanation: the basic idea is to make a dummy
     # metaclass for one level of class instantiation that replaces itself with
     # the actual metaclass.
-    class metaclass(meta):
+    class metaclass(meta):  # type: ignore
         def __new__(cls, name, this_bases, d):
+            # type: (Type[Any], str, Tuple[Any], Dict[Any, Any]) -> Any
             return meta(name, bases, d)
 
     return type.__new__(metaclass, "temporary_class", (), {})
Index: env/lib/python3.8/site-packages/pip/_internal/commands/install.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import absolute_import\n\nimport errno\nimport logging\nimport operator\nimport os\nimport shutil\nimport site\nfrom optparse import SUPPRESS_HELP\n\nfrom pip._vendor import pkg_resources\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nfrom pip._internal.cache import WheelCache\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.cmdoptions import make_target_python\nfrom pip._internal.cli.req_command import RequirementCommand, with_cleanup\nfrom pip._internal.cli.status_codes import ERROR, SUCCESS\nfrom pip._internal.exceptions import CommandError, InstallationError\nfrom pip._internal.locations import distutils_scheme\nfrom pip._internal.operations.check import check_install_conflicts\nfrom pip._internal.req import install_given_reqs\nfrom pip._internal.req.req_tracker import get_requirement_tracker\nfrom pip._internal.utils.datetime import today_is_later_than\nfrom pip._internal.utils.deprecation import deprecated\nfrom pip._internal.utils.distutils_args import parse_distutils_args\nfrom pip._internal.utils.filesystem import test_writable_dir\nfrom pip._internal.utils.misc import (\n    ensure_dir,\n    get_installed_version,\n    get_pip_version,\n    protect_pip_from_modification_on_windows,\n    write_output,\n)\nfrom pip._internal.utils.temp_dir import TempDirectory\nfrom pip._internal.utils.typing import MYPY_CHECK_RUNNING\nfrom pip._internal.utils.virtualenv import virtualenv_no_global\nfrom pip._internal.wheel_builder import build, should_build_for_install_command\n\nif MYPY_CHECK_RUNNING:\n    from optparse import Values\n    from typing import Iterable, List, Optional\n\n    from pip._internal.models.format_control import FormatControl\n    from pip._internal.operations.check import ConflictDetails\n    from pip._internal.req.req_install import InstallRequirement\n    from pip._internal.wheel_builder import BinaryAllowedPredicate\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_check_binary_allowed(format_control):\n    # type: (FormatControl) -> BinaryAllowedPredicate\n    def check_binary_allowed(req):\n        # type: (InstallRequirement) -> bool\n        if req.use_pep517:\n            return True\n        canonical_name = canonicalize_name(req.name)\n        allowed_formats = format_control.get_allowed_formats(canonical_name)\n        return \"binary\" in allowed_formats\n\n    return check_binary_allowed\n\n\nclass InstallCommand(RequirementCommand):\n    \"\"\"\n    Install packages from:\n\n    - PyPI (and other indexes) using requirement specifiers.\n    - VCS project urls.\n    - Local project directories.\n    - Local or remote source archives.\n\n    pip also supports installing from \"requirements files\", which provide\n    an easy way to specify a whole environment to be installed.\n    \"\"\"\n\n    usage = \"\"\"\n      %prog [options] <requirement specifier> [package-index-options] ...\n      %prog [options] -r <requirements file> [package-index-options] ...\n      %prog [options] [-e] <vcs project url> ...\n      %prog [options] [-e] <local project path> ...\n      %prog [options] <archive url/path> ...\"\"\"\n\n    def add_options(self):\n        # type: () -> None\n        self.cmd_opts.add_option(cmdoptions.requirements())\n        self.cmd_opts.add_option(cmdoptions.constraints())\n        self.cmd_opts.add_option(cmdoptions.no_deps())\n        self.cmd_opts.add_option(cmdoptions.pre())\n\n        self.cmd_opts.add_option(cmdoptions.editable())\n        self.cmd_opts.add_option(\n            '-t', '--target',\n            dest='target_dir',\n            metavar='dir',\n            default=None,\n            help='Install packages into <dir>. '\n                 'By default this will not replace existing files/folders in '\n                 '<dir>. Use --upgrade to replace existing packages in <dir> '\n                 'with new versions.'\n        )\n        cmdoptions.add_target_python_options(self.cmd_opts)\n\n        self.cmd_opts.add_option(\n            '--user',\n            dest='use_user_site',\n            action='store_true',\n            help=\"Install to the Python user install directory for your \"\n                 \"platform. Typically ~/.local/, or %APPDATA%\\\\Python on \"\n                 \"Windows. (See the Python documentation for site.USER_BASE \"\n                 \"for full details.)\")\n        self.cmd_opts.add_option(\n            '--no-user',\n            dest='use_user_site',\n            action='store_false',\n            help=SUPPRESS_HELP)\n        self.cmd_opts.add_option(\n            '--root',\n            dest='root_path',\n            metavar='dir',\n            default=None,\n            help=\"Install everything relative to this alternate root \"\n                 \"directory.\")\n        self.cmd_opts.add_option(\n            '--prefix',\n            dest='prefix_path',\n            metavar='dir',\n            default=None,\n            help=\"Installation prefix where lib, bin and other top-level \"\n                 \"folders are placed\")\n\n        self.cmd_opts.add_option(cmdoptions.build_dir())\n\n        self.cmd_opts.add_option(cmdoptions.src())\n\n        self.cmd_opts.add_option(\n            '-U', '--upgrade',\n            dest='upgrade',\n            action='store_true',\n            help='Upgrade all specified packages to the newest available '\n                 'version. The handling of dependencies depends on the '\n                 'upgrade-strategy used.'\n        )\n\n        self.cmd_opts.add_option(\n            '--upgrade-strategy',\n            dest='upgrade_strategy',\n            default='only-if-needed',\n            choices=['only-if-needed', 'eager'],\n            help='Determines how dependency upgrading should be handled '\n                 '[default: %default]. '\n                 '\"eager\" - dependencies are upgraded regardless of '\n                 'whether the currently installed version satisfies the '\n                 'requirements of the upgraded package(s). '\n                 '\"only-if-needed\" -  are upgraded only when they do not '\n                 'satisfy the requirements of the upgraded package(s).'\n        )\n\n        self.cmd_opts.add_option(\n            '--force-reinstall',\n            dest='force_reinstall',\n            action='store_true',\n            help='Reinstall all packages even if they are already '\n                 'up-to-date.')\n\n        self.cmd_opts.add_option(\n            '-I', '--ignore-installed',\n            dest='ignore_installed',\n            action='store_true',\n            help='Ignore the installed packages, overwriting them. '\n                 'This can break your system if the existing package '\n                 'is of a different version or was installed '\n                 'with a different package manager!'\n        )\n\n        self.cmd_opts.add_option(cmdoptions.ignore_requires_python())\n        self.cmd_opts.add_option(cmdoptions.no_build_isolation())\n        self.cmd_opts.add_option(cmdoptions.use_pep517())\n        self.cmd_opts.add_option(cmdoptions.no_use_pep517())\n\n        self.cmd_opts.add_option(cmdoptions.install_options())\n        self.cmd_opts.add_option(cmdoptions.global_options())\n\n        self.cmd_opts.add_option(\n            \"--compile\",\n            action=\"store_true\",\n            dest=\"compile\",\n            default=True,\n            help=\"Compile Python source files to bytecode\",\n        )\n\n        self.cmd_opts.add_option(\n            \"--no-compile\",\n            action=\"store_false\",\n            dest=\"compile\",\n            help=\"Do not compile Python source files to bytecode\",\n        )\n\n        self.cmd_opts.add_option(\n            \"--no-warn-script-location\",\n            action=\"store_false\",\n            dest=\"warn_script_location\",\n            default=True,\n            help=\"Do not warn when installing scripts outside PATH\",\n        )\n        self.cmd_opts.add_option(\n            \"--no-warn-conflicts\",\n            action=\"store_false\",\n            dest=\"warn_about_conflicts\",\n            default=True,\n            help=\"Do not warn about broken dependencies\",\n        )\n\n        self.cmd_opts.add_option(cmdoptions.no_binary())\n        self.cmd_opts.add_option(cmdoptions.only_binary())\n        self.cmd_opts.add_option(cmdoptions.prefer_binary())\n        self.cmd_opts.add_option(cmdoptions.require_hashes())\n        self.cmd_opts.add_option(cmdoptions.progress_bar())\n\n        index_opts = cmdoptions.make_option_group(\n            cmdoptions.index_group,\n            self.parser,\n        )\n\n        self.parser.insert_option_group(0, index_opts)\n        self.parser.insert_option_group(0, self.cmd_opts)\n\n    @with_cleanup\n    def run(self, options, args):\n        # type: (Values, List[str]) -> int\n        if options.use_user_site and options.target_dir is not None:\n            raise CommandError(\"Can not combine '--user' and '--target'\")\n\n        cmdoptions.check_install_build_global(options)\n        upgrade_strategy = \"to-satisfy-only\"\n        if options.upgrade:\n            upgrade_strategy = options.upgrade_strategy\n\n        cmdoptions.check_dist_restriction(options, check_target=True)\n\n        install_options = options.install_options or []\n\n        logger.debug(\"Using %s\", get_pip_version())\n        options.use_user_site = decide_user_install(\n            options.use_user_site,\n            prefix_path=options.prefix_path,\n            target_dir=options.target_dir,\n            root_path=options.root_path,\n            isolated_mode=options.isolated_mode,\n        )\n\n        target_temp_dir = None  # type: Optional[TempDirectory]\n        target_temp_dir_path = None  # type: Optional[str]\n        if options.target_dir:\n            options.ignore_installed = True\n            options.target_dir = os.path.abspath(options.target_dir)\n            if (os.path.exists(options.target_dir) and not\n                    os.path.isdir(options.target_dir)):\n                raise CommandError(\n                    \"Target path exists but is not a directory, will not \"\n                    \"continue.\"\n                )\n\n            # Create a target directory for using with the target option\n            target_temp_dir = TempDirectory(kind=\"target\")\n            target_temp_dir_path = target_temp_dir.path\n            self.enter_context(target_temp_dir)\n\n        global_options = options.global_options or []\n\n        session = self.get_default_session(options)\n\n        target_python = make_target_python(options)\n        finder = self._build_package_finder(\n            options=options,\n            session=session,\n            target_python=target_python,\n            ignore_requires_python=options.ignore_requires_python,\n        )\n        build_delete = (not (options.no_clean or options.build_dir))\n        wheel_cache = WheelCache(options.cache_dir, options.format_control)\n\n        req_tracker = self.enter_context(get_requirement_tracker())\n\n        directory = TempDirectory(\n            options.build_dir,\n            delete=build_delete,\n            kind=\"install\",\n            globally_managed=True,\n        )\n\n        try:\n            reqs = self.get_requirements(args, options, finder, session)\n\n            reject_location_related_install_options(\n                reqs, options.install_options\n            )\n\n            preparer = self.make_requirement_preparer(\n                temp_build_dir=directory,\n                options=options,\n                req_tracker=req_tracker,\n                session=session,\n                finder=finder,\n                use_user_site=options.use_user_site,\n            )\n            resolver = self.make_resolver(\n                preparer=preparer,\n                finder=finder,\n                options=options,\n                wheel_cache=wheel_cache,\n                use_user_site=options.use_user_site,\n                ignore_installed=options.ignore_installed,\n                ignore_requires_python=options.ignore_requires_python,\n                force_reinstall=options.force_reinstall,\n                upgrade_strategy=upgrade_strategy,\n                use_pep517=options.use_pep517,\n            )\n\n            self.trace_basic_info(finder)\n\n            requirement_set = resolver.resolve(\n                reqs, check_supported_wheels=not options.target_dir\n            )\n\n            try:\n                pip_req = requirement_set.get_requirement(\"pip\")\n            except KeyError:\n                modifying_pip = False\n            else:\n                # If we're not replacing an already installed pip,\n                # we're not modifying it.\n                modifying_pip = pip_req.satisfied_by is None\n            protect_pip_from_modification_on_windows(\n                modifying_pip=modifying_pip\n            )\n\n            check_binary_allowed = get_check_binary_allowed(\n                finder.format_control\n            )\n\n            reqs_to_build = [\n                r for r in requirement_set.requirements.values()\n                if should_build_for_install_command(\n                    r, check_binary_allowed\n                )\n            ]\n\n            _, build_failures = build(\n                reqs_to_build,\n                wheel_cache=wheel_cache,\n                build_options=[],\n                global_options=[],\n            )\n\n            # If we're using PEP 517, we cannot do a direct install\n            # so we fail here.\n            pep517_build_failure_names = [\n                r.name   # type: ignore\n                for r in build_failures if r.use_pep517\n            ]  # type: List[str]\n            if pep517_build_failure_names:\n                raise InstallationError(\n                    \"Could not build wheels for {} which use\"\n                    \" PEP 517 and cannot be installed directly\".format(\n                        \", \".join(pep517_build_failure_names)\n                    )\n                )\n\n            # For now, we just warn about failures building legacy\n            # requirements, as we'll fall through to a direct\n            # install for those.\n            legacy_build_failure_names = [\n                r.name  # type: ignore\n                for r in build_failures if not r.use_pep517\n            ]  # type: List[str]\n            if legacy_build_failure_names:\n                deprecated(\n                    reason=(\n                        \"Could not build wheels for {} which do not use \"\n                        \"PEP 517. pip will fall back to legacy 'setup.py \"\n                        \"install' for these.\".format(\n                            \", \".join(legacy_build_failure_names)\n                        )\n                    ),\n                    replacement=\"to fix the wheel build issue reported above\",\n                    gone_in=\"21.0\",\n                    issue=8368,\n                )\n\n            to_install = resolver.get_installation_order(\n                requirement_set\n            )\n\n            # Check for conflicts in the package set we're installing.\n            conflicts = None  # type: Optional[ConflictDetails]\n            should_warn_about_conflicts = (\n                not options.ignore_dependencies and\n                options.warn_about_conflicts\n            )\n            if should_warn_about_conflicts:\n                conflicts = self._determine_conflicts(to_install)\n\n            # Don't warn about script install locations if\n            # --target has been specified\n            warn_script_location = options.warn_script_location\n            if options.target_dir:\n                warn_script_location = False\n\n            installed = install_given_reqs(\n                to_install,\n                install_options,\n                global_options,\n                root=options.root_path,\n                home=target_temp_dir_path,\n                prefix=options.prefix_path,\n                warn_script_location=warn_script_location,\n                use_user_site=options.use_user_site,\n                pycompile=options.compile,\n            )\n\n            lib_locations = get_lib_location_guesses(\n                user=options.use_user_site,\n                home=target_temp_dir_path,\n                root=options.root_path,\n                prefix=options.prefix_path,\n                isolated=options.isolated_mode,\n            )\n            working_set = pkg_resources.WorkingSet(lib_locations)\n\n            installed.sort(key=operator.attrgetter('name'))\n            items = []\n            for result in installed:\n                item = result.name\n                try:\n                    installed_version = get_installed_version(\n                        result.name, working_set=working_set\n                    )\n                    if installed_version:\n                        item += '-' + installed_version\n                except Exception:\n                    pass\n                items.append(item)\n\n            if conflicts is not None:\n                self._warn_about_conflicts(\n                    conflicts,\n                    new_resolver='2020-resolver' in options.features_enabled,\n                )\n\n            installed_desc = ' '.join(items)\n            if installed_desc:\n                write_output(\n                    'Successfully installed %s', installed_desc,\n                )\n        except EnvironmentError as error:\n            show_traceback = (self.verbosity >= 1)\n\n            message = create_env_error_message(\n                error, show_traceback, options.use_user_site,\n            )\n            logger.error(message, exc_info=show_traceback)  # noqa\n\n            return ERROR\n\n        if options.target_dir:\n            assert target_temp_dir\n            self._handle_target_dir(\n                options.target_dir, target_temp_dir, options.upgrade\n            )\n\n        return SUCCESS\n\n    def _handle_target_dir(self, target_dir, target_temp_dir, upgrade):\n        # type: (str, TempDirectory, bool) -> None\n        ensure_dir(target_dir)\n\n        # Checking both purelib and platlib directories for installed\n        # packages to be moved to target directory\n        lib_dir_list = []\n\n        # Checking both purelib and platlib directories for installed\n        # packages to be moved to target directory\n        scheme = distutils_scheme('', home=target_temp_dir.path)\n        purelib_dir = scheme['purelib']\n        platlib_dir = scheme['platlib']\n        data_dir = scheme['data']\n\n        if os.path.exists(purelib_dir):\n            lib_dir_list.append(purelib_dir)\n        if os.path.exists(platlib_dir) and platlib_dir != purelib_dir:\n            lib_dir_list.append(platlib_dir)\n        if os.path.exists(data_dir):\n            lib_dir_list.append(data_dir)\n\n        for lib_dir in lib_dir_list:\n            for item in os.listdir(lib_dir):\n                if lib_dir == data_dir:\n                    ddir = os.path.join(data_dir, item)\n                    if any(s.startswith(ddir) for s in lib_dir_list[:-1]):\n                        continue\n                target_item_dir = os.path.join(target_dir, item)\n                if os.path.exists(target_item_dir):\n                    if not upgrade:\n                        logger.warning(\n                            'Target directory %s already exists. Specify '\n                            '--upgrade to force replacement.',\n                            target_item_dir\n                        )\n                        continue\n                    if os.path.islink(target_item_dir):\n                        logger.warning(\n                            'Target directory %s already exists and is '\n                            'a link. pip will not automatically replace '\n                            'links, please remove if replacement is '\n                            'desired.',\n                            target_item_dir\n                        )\n                        continue\n                    if os.path.isdir(target_item_dir):\n                        shutil.rmtree(target_item_dir)\n                    else:\n                        os.remove(target_item_dir)\n\n                shutil.move(\n                    os.path.join(lib_dir, item),\n                    target_item_dir\n                )\n\n    def _determine_conflicts(self, to_install):\n        # type: (List[InstallRequirement]) -> Optional[ConflictDetails]\n        try:\n            return check_install_conflicts(to_install)\n        except Exception:\n            logger.exception(\n                \"Error while checking for conflicts. Please file an issue on \"\n                \"pip's issue tracker: https://github.com/pypa/pip/issues/new\"\n            )\n            return None\n\n    def _warn_about_conflicts(self, conflict_details, new_resolver):\n        # type: (ConflictDetails, bool) -> None\n        package_set, (missing, conflicting) = conflict_details\n        if not missing and not conflicting:\n            return\n\n        parts = []  # type: List[str]\n        if not new_resolver:\n            parts.append(\n                \"After October 2020 you may experience errors when installing \"\n                \"or updating packages. This is because pip will change the \"\n                \"way that it resolves dependency conflicts.\\n\"\n            )\n            parts.append(\n                \"We recommend you use --use-feature=2020-resolver to test \"\n                \"your packages with the new resolver before it becomes the \"\n                \"default.\\n\"\n            )\n        elif not today_is_later_than(year=2020, month=7, day=31):\n            # NOTE: trailing newlines here are intentional\n            parts.append(\n                \"Pip will install or upgrade your package(s) and its \"\n                \"dependencies without taking into account other packages you \"\n                \"already have installed. This may cause an uncaught \"\n                \"dependency conflict.\\n\"\n            )\n            form_link = \"https://forms.gle/cWKMoDs8sUVE29hz9\"\n            parts.append(\n                \"If you would like pip to take your other packages into \"\n                \"account, please tell us here: {}\\n\".format(form_link)\n            )\n\n        # NOTE: There is some duplication here, with commands/check.py\n        for project_name in missing:\n            version = package_set[project_name][0]\n            for dependency in missing[project_name]:\n                message = (\n                    \"{name} {version} requires {requirement}, \"\n                    \"which is not installed.\"\n                ).format(\n                    name=project_name,\n                    version=version,\n                    requirement=dependency[1],\n                )\n                parts.append(message)\n\n        for project_name in conflicting:\n            version = package_set[project_name][0]\n            for dep_name, dep_version, req in conflicting[project_name]:\n                message = (\n                    \"{name} {version} requires {requirement}, but you'll have \"\n                    \"{dep_name} {dep_version} which is incompatible.\"\n                ).format(\n                    name=project_name,\n                    version=version,\n                    requirement=req,\n                    dep_name=dep_name,\n                    dep_version=dep_version,\n                )\n                parts.append(message)\n\n        logger.critical(\"\\n\".join(parts))\n\n\ndef get_lib_location_guesses(\n        user=False,  # type: bool\n        home=None,  # type: Optional[str]\n        root=None,  # type: Optional[str]\n        isolated=False,  # type: bool\n        prefix=None  # type: Optional[str]\n):\n    # type:(...) -> List[str]\n    scheme = distutils_scheme('', user=user, home=home, root=root,\n                              isolated=isolated, prefix=prefix)\n    return [scheme['purelib'], scheme['platlib']]\n\n\ndef site_packages_writable(root, isolated):\n    # type: (Optional[str], bool) -> bool\n    return all(\n        test_writable_dir(d) for d in set(\n            get_lib_location_guesses(root=root, isolated=isolated))\n    )\n\n\ndef decide_user_install(\n    use_user_site,  # type: Optional[bool]\n    prefix_path=None,  # type: Optional[str]\n    target_dir=None,  # type: Optional[str]\n    root_path=None,  # type: Optional[str]\n    isolated_mode=False,  # type: bool\n):\n    # type: (...) -> bool\n    \"\"\"Determine whether to do a user install based on the input options.\n\n    If use_user_site is False, no additional checks are done.\n    If use_user_site is True, it is checked for compatibility with other\n    options.\n    If use_user_site is None, the default behaviour depends on the environment,\n    which is provided by the other arguments.\n    \"\"\"\n    # In some cases (config from tox), use_user_site can be set to an integer\n    # rather than a bool, which 'use_user_site is False' wouldn't catch.\n    if (use_user_site is not None) and (not use_user_site):\n        logger.debug(\"Non-user install by explicit request\")\n        return False\n\n    if use_user_site:\n        if prefix_path:\n            raise CommandError(\n                \"Can not combine '--user' and '--prefix' as they imply \"\n                \"different installation locations\"\n            )\n        if virtualenv_no_global():\n            raise InstallationError(\n                \"Can not perform a '--user' install. User site-packages \"\n                \"are not visible in this virtualenv.\"\n            )\n        logger.debug(\"User install by explicit request\")\n        return True\n\n    # If we are here, user installs have not been explicitly requested/avoided\n    assert use_user_site is None\n\n    # user install incompatible with --prefix/--target\n    if prefix_path or target_dir:\n        logger.debug(\"Non-user install due to --prefix or --target option\")\n        return False\n\n    # If user installs are not enabled, choose a non-user install\n    if not site.ENABLE_USER_SITE:\n        logger.debug(\"Non-user install because user site-packages disabled\")\n        return False\n\n    # If we have permission for a non-user install, do that,\n    # otherwise do a user install.\n    if site_packages_writable(root=root_path, isolated=isolated_mode):\n        logger.debug(\"Non-user install because site-packages writeable\")\n        return False\n\n    logger.info(\"Defaulting to user installation because normal site-packages \"\n                \"is not writeable\")\n    return True\n\n\ndef reject_location_related_install_options(requirements, options):\n    # type: (List[InstallRequirement], Optional[List[str]]) -> None\n    \"\"\"If any location-changing --install-option arguments were passed for\n    requirements or on the command-line, then show a deprecation warning.\n    \"\"\"\n    def format_options(option_names):\n        # type: (Iterable[str]) -> List[str]\n        return [\"--{}\".format(name.replace(\"_\", \"-\")) for name in option_names]\n\n    offenders = []\n\n    for requirement in requirements:\n        install_options = requirement.install_options\n        location_options = parse_distutils_args(install_options)\n        if location_options:\n            offenders.append(\n                \"{!r} from {}\".format(\n                    format_options(location_options.keys()), requirement\n                )\n            )\n\n    if options:\n        location_options = parse_distutils_args(options)\n        if location_options:\n            offenders.append(\n                \"{!r} from command line\".format(\n                    format_options(location_options.keys())\n                )\n            )\n\n    if not offenders:\n        return\n\n    raise CommandError(\n        \"Location-changing options found in --install-option: {}.\"\n        \" This is unsupported, use pip-level options like --user,\"\n        \" --prefix, --root, and --target instead.\".format(\n            \"; \".join(offenders)\n        )\n    )\n\n\ndef create_env_error_message(error, show_traceback, using_user_site):\n    # type: (EnvironmentError, bool, bool) -> str\n    \"\"\"Format an error message for an EnvironmentError\n\n    It may occur anytime during the execution of the install command.\n    \"\"\"\n    parts = []\n\n    # Mention the error if we are not going to show a traceback\n    parts.append(\"Could not install packages due to an EnvironmentError\")\n    if not show_traceback:\n        parts.append(\": \")\n        parts.append(str(error))\n    else:\n        parts.append(\".\")\n\n    # Spilt the error indication from a helper message (if any)\n    parts[-1] += \"\\n\"\n\n    # Suggest useful actions to the user:\n    #  (1) using user site-packages or (2) verifying the permissions\n    if error.errno == errno.EACCES:\n        user_option_part = \"Consider using the `--user` option\"\n        permissions_part = \"Check the permissions\"\n\n        if not using_user_site:\n            parts.extend([\n                user_option_part, \" or \",\n                permissions_part.lower(),\n            ])\n        else:\n            parts.append(permissions_part)\n        parts.append(\".\\n\")\n\n    return \"\".join(parts).strip() + \"\\n\"\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pip/_internal/commands/install.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pip/_internal/commands/install.py	(date 1602088700393)
@@ -22,7 +22,6 @@
 from pip._internal.req import install_given_reqs
 from pip._internal.req.req_tracker import get_requirement_tracker
 from pip._internal.utils.datetime import today_is_later_than
-from pip._internal.utils.deprecation import deprecated
 from pip._internal.utils.distutils_args import parse_distutils_args
 from pip._internal.utils.filesystem import test_writable_dir
 from pip._internal.utils.misc import (
@@ -372,23 +371,9 @@
             # For now, we just warn about failures building legacy
             # requirements, as we'll fall through to a direct
             # install for those.
-            legacy_build_failure_names = [
-                r.name  # type: ignore
-                for r in build_failures if not r.use_pep517
-            ]  # type: List[str]
-            if legacy_build_failure_names:
-                deprecated(
-                    reason=(
-                        "Could not build wheels for {} which do not use "
-                        "PEP 517. pip will fall back to legacy 'setup.py "
-                        "install' for these.".format(
-                            ", ".join(legacy_build_failure_names)
-                        )
-                    ),
-                    replacement="to fix the wheel build issue reported above",
-                    gone_in="21.0",
-                    issue=8368,
-                )
+            for r in build_failures:
+                if not r.use_pep517:
+                    r.legacy_install_reason = 8368
 
             to_install = resolver.get_installation_order(
                 requirement_set
Index: env/pyvenv.cfg
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>home = /usr/local/bin\ninclude-system-site-packages = false\nversion = 3.8.6\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/pyvenv.cfg	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/pyvenv.cfg	(date 1602090679717)
@@ -1,3 +1,3 @@
 home = /usr/local/bin
-include-system-site-packages = false
+include-system-site-packages = true
 version = 3.8.6
Index: env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/utils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import absolute_import, division, print_function\n\nimport re\n\nfrom .version import InvalidVersion, Version\n\n\n_canonicalize_regex = re.compile(r\"[-_.]+\")\n\n\ndef canonicalize_name(name):\n    # This is taken from PEP 503.\n    return _canonicalize_regex.sub(\"-\", name).lower()\n\n\ndef canonicalize_version(version):\n    \"\"\"\n    This is very similar to Version.__str__, but has one subtle differences\n    with the way it handles the release segment.\n    \"\"\"\n\n    try:\n        version = Version(version)\n    except InvalidVersion:\n        # Legacy versions cannot be normalized\n        return version\n\n    parts = []\n\n    # Epoch\n    if version.epoch != 0:\n        parts.append(\"{0}!\".format(version.epoch))\n\n    # Release segment\n    # NB: This strips trailing '.0's to normalize\n    parts.append(re.sub(r\"(\\.0)+$\", \"\", \".\".join(str(x) for x in version.release)))\n\n    # Pre-release\n    if version.pre is not None:\n        parts.append(\"\".join(str(x) for x in version.pre))\n\n    # Post-release\n    if version.post is not None:\n        parts.append(\".post{0}\".format(version.post))\n\n    # Development release\n    if version.dev is not None:\n        parts.append(\".dev{0}\".format(version.dev))\n\n    # Local version segment\n    if version.local is not None:\n        parts.append(\"+{0}\".format(version.local))\n\n    return \"\".join(parts)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/utils.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/utils.py	(date 1602088701593)
@@ -5,28 +5,36 @@
 
 import re
 
+from ._typing import TYPE_CHECKING, cast
 from .version import InvalidVersion, Version
 
+if TYPE_CHECKING:  # pragma: no cover
+    from typing import NewType, Union
+
+    NormalizedName = NewType("NormalizedName", str)
 
 _canonicalize_regex = re.compile(r"[-_.]+")
 
 
 def canonicalize_name(name):
+    # type: (str) -> NormalizedName
     # This is taken from PEP 503.
-    return _canonicalize_regex.sub("-", name).lower()
+    value = _canonicalize_regex.sub("-", name).lower()
+    return cast("NormalizedName", value)
 
 
-def canonicalize_version(version):
+def canonicalize_version(_version):
+    # type: (str) -> Union[Version, str]
     """
-    This is very similar to Version.__str__, but has one subtle differences
+    This is very similar to Version.__str__, but has one subtle difference
     with the way it handles the release segment.
     """
 
     try:
-        version = Version(version)
+        version = Version(_version)
     except InvalidVersion:
         # Legacy versions cannot be normalized
-        return version
+        return _version
 
     parts = []
 
Index: env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/__about__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import absolute_import, division, print_function\n\n__all__ = [\n    \"__title__\",\n    \"__summary__\",\n    \"__uri__\",\n    \"__version__\",\n    \"__author__\",\n    \"__email__\",\n    \"__license__\",\n    \"__copyright__\",\n]\n\n__title__ = \"packaging\"\n__summary__ = \"Core utilities for Python packages\"\n__uri__ = \"https://github.com/pypa/packaging\"\n\n__version__ = \"19.2\"\n\n__author__ = \"Donald Stufft and individual contributors\"\n__email__ = \"donald@stufft.io\"\n\n__license__ = \"BSD or Apache License, Version 2.0\"\n__copyright__ = \"Copyright 2014-2019 %s\" % __author__\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/__about__.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/__about__.py	(date 1602088701593)
@@ -18,10 +18,10 @@
 __summary__ = "Core utilities for Python packages"
 __uri__ = "https://github.com/pypa/packaging"
 
-__version__ = "19.2"
+__version__ = "20.4"
 
 __author__ = "Donald Stufft and individual contributors"
 __email__ = "donald@stufft.io"
 
-__license__ = "BSD or Apache License, Version 2.0"
+__license__ = "BSD-2-Clause or Apache-2.0"
 __copyright__ = "Copyright 2014-2019 %s" % __author__
Index: env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/tags.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\n\nfrom __future__ import absolute_import\n\nimport distutils.util\n\ntry:\n    from importlib.machinery import EXTENSION_SUFFIXES\nexcept ImportError:  # pragma: no cover\n    import imp\n\n    EXTENSION_SUFFIXES = [x[0] for x in imp.get_suffixes()]\n    del imp\nimport platform\nimport re\nimport sys\nimport sysconfig\nimport warnings\n\n\nINTERPRETER_SHORT_NAMES = {\n    \"python\": \"py\",  # Generic.\n    \"cpython\": \"cp\",\n    \"pypy\": \"pp\",\n    \"ironpython\": \"ip\",\n    \"jython\": \"jy\",\n}\n\n\n_32_BIT_INTERPRETER = sys.maxsize <= 2 ** 32\n\n\nclass Tag(object):\n\n    __slots__ = [\"_interpreter\", \"_abi\", \"_platform\"]\n\n    def __init__(self, interpreter, abi, platform):\n        self._interpreter = interpreter.lower()\n        self._abi = abi.lower()\n        self._platform = platform.lower()\n\n    @property\n    def interpreter(self):\n        return self._interpreter\n\n    @property\n    def abi(self):\n        return self._abi\n\n    @property\n    def platform(self):\n        return self._platform\n\n    def __eq__(self, other):\n        return (\n            (self.platform == other.platform)\n            and (self.abi == other.abi)\n            and (self.interpreter == other.interpreter)\n        )\n\n    def __hash__(self):\n        return hash((self._interpreter, self._abi, self._platform))\n\n    def __str__(self):\n        return \"{}-{}-{}\".format(self._interpreter, self._abi, self._platform)\n\n    def __repr__(self):\n        return \"<{self} @ {self_id}>\".format(self=self, self_id=id(self))\n\n\ndef parse_tag(tag):\n    tags = set()\n    interpreters, abis, platforms = tag.split(\"-\")\n    for interpreter in interpreters.split(\".\"):\n        for abi in abis.split(\".\"):\n            for platform_ in platforms.split(\".\"):\n                tags.add(Tag(interpreter, abi, platform_))\n    return frozenset(tags)\n\n\ndef _normalize_string(string):\n    return string.replace(\".\", \"_\").replace(\"-\", \"_\")\n\n\ndef _cpython_interpreter(py_version):\n    # TODO: Is using py_version_nodot for interpreter version critical?\n    return \"cp{major}{minor}\".format(major=py_version[0], minor=py_version[1])\n\n\ndef _cpython_abis(py_version):\n    abis = []\n    version = \"{}{}\".format(*py_version[:2])\n    debug = pymalloc = ucs4 = \"\"\n    with_debug = sysconfig.get_config_var(\"Py_DEBUG\")\n    has_refcount = hasattr(sys, \"gettotalrefcount\")\n    # Windows doesn't set Py_DEBUG, so checking for support of debug-compiled\n    # extension modules is the best option.\n    # https://github.com/pypa/pip/issues/3383#issuecomment-173267692\n    has_ext = \"_d.pyd\" in EXTENSION_SUFFIXES\n    if with_debug or (with_debug is None and (has_refcount or has_ext)):\n        debug = \"d\"\n    if py_version < (3, 8):\n        with_pymalloc = sysconfig.get_config_var(\"WITH_PYMALLOC\")\n        if with_pymalloc or with_pymalloc is None:\n            pymalloc = \"m\"\n        if py_version < (3, 3):\n            unicode_size = sysconfig.get_config_var(\"Py_UNICODE_SIZE\")\n            if unicode_size == 4 or (\n                unicode_size is None and sys.maxunicode == 0x10FFFF\n            ):\n                ucs4 = \"u\"\n    elif debug:\n        # Debug builds can also load \"normal\" extension modules.\n        # We can also assume no UCS-4 or pymalloc requirement.\n        abis.append(\"cp{version}\".format(version=version))\n    abis.insert(\n        0,\n        \"cp{version}{debug}{pymalloc}{ucs4}\".format(\n            version=version, debug=debug, pymalloc=pymalloc, ucs4=ucs4\n        ),\n    )\n    return abis\n\n\ndef _cpython_tags(py_version, interpreter, abis, platforms):\n    for abi in abis:\n        for platform_ in platforms:\n            yield Tag(interpreter, abi, platform_)\n    for tag in (Tag(interpreter, \"abi3\", platform_) for platform_ in platforms):\n        yield tag\n    for tag in (Tag(interpreter, \"none\", platform_) for platform_ in platforms):\n        yield tag\n    # PEP 384 was first implemented in Python 3.2.\n    for minor_version in range(py_version[1] - 1, 1, -1):\n        for platform_ in platforms:\n            interpreter = \"cp{major}{minor}\".format(\n                major=py_version[0], minor=minor_version\n            )\n            yield Tag(interpreter, \"abi3\", platform_)\n\n\ndef _pypy_interpreter():\n    return \"pp{py_major}{pypy_major}{pypy_minor}\".format(\n        py_major=sys.version_info[0],\n        pypy_major=sys.pypy_version_info.major,\n        pypy_minor=sys.pypy_version_info.minor,\n    )\n\n\ndef _generic_abi():\n    abi = sysconfig.get_config_var(\"SOABI\")\n    if abi:\n        return _normalize_string(abi)\n    else:\n        return \"none\"\n\n\ndef _pypy_tags(py_version, interpreter, abi, platforms):\n    for tag in (Tag(interpreter, abi, platform) for platform in platforms):\n        yield tag\n    for tag in (Tag(interpreter, \"none\", platform) for platform in platforms):\n        yield tag\n\n\ndef _generic_tags(interpreter, py_version, abi, platforms):\n    for tag in (Tag(interpreter, abi, platform) for platform in platforms):\n        yield tag\n    if abi != \"none\":\n        tags = (Tag(interpreter, \"none\", platform_) for platform_ in platforms)\n        for tag in tags:\n            yield tag\n\n\ndef _py_interpreter_range(py_version):\n    \"\"\"\n    Yield Python versions in descending order.\n\n    After the latest version, the major-only version will be yielded, and then\n    all following versions up to 'end'.\n    \"\"\"\n    yield \"py{major}{minor}\".format(major=py_version[0], minor=py_version[1])\n    yield \"py{major}\".format(major=py_version[0])\n    for minor in range(py_version[1] - 1, -1, -1):\n        yield \"py{major}{minor}\".format(major=py_version[0], minor=minor)\n\n\ndef _independent_tags(interpreter, py_version, platforms):\n    \"\"\"\n    Return the sequence of tags that are consistent across implementations.\n\n    The tags consist of:\n    - py*-none-<platform>\n    - <interpreter>-none-any\n    - py*-none-any\n    \"\"\"\n    for version in _py_interpreter_range(py_version):\n        for platform_ in platforms:\n            yield Tag(version, \"none\", platform_)\n    yield Tag(interpreter, \"none\", \"any\")\n    for version in _py_interpreter_range(py_version):\n        yield Tag(version, \"none\", \"any\")\n\n\ndef _mac_arch(arch, is_32bit=_32_BIT_INTERPRETER):\n    if not is_32bit:\n        return arch\n\n    if arch.startswith(\"ppc\"):\n        return \"ppc\"\n\n    return \"i386\"\n\n\ndef _mac_binary_formats(version, cpu_arch):\n    formats = [cpu_arch]\n    if cpu_arch == \"x86_64\":\n        if version < (10, 4):\n            return []\n        formats.extend([\"intel\", \"fat64\", \"fat32\"])\n\n    elif cpu_arch == \"i386\":\n        if version < (10, 4):\n            return []\n        formats.extend([\"intel\", \"fat32\", \"fat\"])\n\n    elif cpu_arch == \"ppc64\":\n        # TODO: Need to care about 32-bit PPC for ppc64 through 10.2?\n        if version > (10, 5) or version < (10, 4):\n            return []\n        formats.append(\"fat64\")\n\n    elif cpu_arch == \"ppc\":\n        if version > (10, 6):\n            return []\n        formats.extend([\"fat32\", \"fat\"])\n\n    formats.append(\"universal\")\n    return formats\n\n\ndef _mac_platforms(version=None, arch=None):\n    version_str, _, cpu_arch = platform.mac_ver()\n    if version is None:\n        version = tuple(map(int, version_str.split(\".\")[:2]))\n    if arch is None:\n        arch = _mac_arch(cpu_arch)\n    platforms = []\n    for minor_version in range(version[1], -1, -1):\n        compat_version = version[0], minor_version\n        binary_formats = _mac_binary_formats(compat_version, arch)\n        for binary_format in binary_formats:\n            platforms.append(\n                \"macosx_{major}_{minor}_{binary_format}\".format(\n                    major=compat_version[0],\n                    minor=compat_version[1],\n                    binary_format=binary_format,\n                )\n            )\n    return platforms\n\n\n# From PEP 513.\ndef _is_manylinux_compatible(name, glibc_version):\n    # Check for presence of _manylinux module.\n    try:\n        import _manylinux\n\n        return bool(getattr(_manylinux, name + \"_compatible\"))\n    except (ImportError, AttributeError):\n        # Fall through to heuristic check below.\n        pass\n\n    return _have_compatible_glibc(*glibc_version)\n\n\ndef _glibc_version_string():\n    # Returns glibc version string, or None if not using glibc.\n    import ctypes\n\n    # ctypes.CDLL(None) internally calls dlopen(NULL), and as the dlopen\n    # manpage says, \"If filename is NULL, then the returned handle is for the\n    # main program\". This way we can let the linker do the work to figure out\n    # which libc our process is actually using.\n    process_namespace = ctypes.CDLL(None)\n    try:\n        gnu_get_libc_version = process_namespace.gnu_get_libc_version\n    except AttributeError:\n        # Symbol doesn't exist -> therefore, we are not linked to\n        # glibc.\n        return None\n\n    # Call gnu_get_libc_version, which returns a string like \"2.5\"\n    gnu_get_libc_version.restype = ctypes.c_char_p\n    version_str = gnu_get_libc_version()\n    # py2 / py3 compatibility:\n    if not isinstance(version_str, str):\n        version_str = version_str.decode(\"ascii\")\n\n    return version_str\n\n\n# Separated out from have_compatible_glibc for easier unit testing.\ndef _check_glibc_version(version_str, required_major, minimum_minor):\n    # Parse string and check against requested version.\n    #\n    # We use a regexp instead of str.split because we want to discard any\n    # random junk that might come after the minor version -- this might happen\n    # in patched/forked versions of glibc (e.g. Linaro's version of glibc\n    # uses version strings like \"2.20-2014.11\"). See gh-3588.\n    m = re.match(r\"(?P<major>[0-9]+)\\.(?P<minor>[0-9]+)\", version_str)\n    if not m:\n        warnings.warn(\n            \"Expected glibc version with 2 components major.minor,\"\n            \" got: %s\" % version_str,\n            RuntimeWarning,\n        )\n        return False\n    return (\n        int(m.group(\"major\")) == required_major\n        and int(m.group(\"minor\")) >= minimum_minor\n    )\n\n\ndef _have_compatible_glibc(required_major, minimum_minor):\n    version_str = _glibc_version_string()\n    if version_str is None:\n        return False\n    return _check_glibc_version(version_str, required_major, minimum_minor)\n\n\ndef _linux_platforms(is_32bit=_32_BIT_INTERPRETER):\n    linux = _normalize_string(distutils.util.get_platform())\n    if linux == \"linux_x86_64\" and is_32bit:\n        linux = \"linux_i686\"\n    manylinux_support = (\n        (\"manylinux2014\", (2, 17)),  # CentOS 7 w/ glibc 2.17 (PEP 599)\n        (\"manylinux2010\", (2, 12)),  # CentOS 6 w/ glibc 2.12 (PEP 571)\n        (\"manylinux1\", (2, 5)),  # CentOS 5 w/ glibc 2.5 (PEP 513)\n    )\n    manylinux_support_iter = iter(manylinux_support)\n    for name, glibc_version in manylinux_support_iter:\n        if _is_manylinux_compatible(name, glibc_version):\n            platforms = [linux.replace(\"linux\", name)]\n            break\n    else:\n        platforms = []\n    # Support for a later manylinux implies support for an earlier version.\n    platforms += [linux.replace(\"linux\", name) for name, _ in manylinux_support_iter]\n    platforms.append(linux)\n    return platforms\n\n\ndef _generic_platforms():\n    platform = _normalize_string(distutils.util.get_platform())\n    return [platform]\n\n\ndef _interpreter_name():\n    name = platform.python_implementation().lower()\n    return INTERPRETER_SHORT_NAMES.get(name) or name\n\n\ndef _generic_interpreter(name, py_version):\n    version = sysconfig.get_config_var(\"py_version_nodot\")\n    if not version:\n        version = \"\".join(map(str, py_version[:2]))\n    return \"{name}{version}\".format(name=name, version=version)\n\n\ndef sys_tags():\n    \"\"\"\n    Returns the sequence of tag triples for the running interpreter.\n\n    The order of the sequence corresponds to priority order for the\n    interpreter, from most to least important.\n    \"\"\"\n    py_version = sys.version_info[:2]\n    interpreter_name = _interpreter_name()\n    if platform.system() == \"Darwin\":\n        platforms = _mac_platforms()\n    elif platform.system() == \"Linux\":\n        platforms = _linux_platforms()\n    else:\n        platforms = _generic_platforms()\n\n    if interpreter_name == \"cp\":\n        interpreter = _cpython_interpreter(py_version)\n        abis = _cpython_abis(py_version)\n        for tag in _cpython_tags(py_version, interpreter, abis, platforms):\n            yield tag\n    elif interpreter_name == \"pp\":\n        interpreter = _pypy_interpreter()\n        abi = _generic_abi()\n        for tag in _pypy_tags(py_version, interpreter, abi, platforms):\n            yield tag\n    else:\n        interpreter = _generic_interpreter(interpreter_name, py_version)\n        abi = _generic_abi()\n        for tag in _generic_tags(interpreter, py_version, abi, platforms):\n            yield tag\n    for tag in _independent_tags(interpreter, py_version, platforms):\n        yield tag\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/tags.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/tags.py	(date 1602088701593)
@@ -13,12 +13,37 @@
 
     EXTENSION_SUFFIXES = [x[0] for x in imp.get_suffixes()]
     del imp
+import logging
+import os
 import platform
 import re
+import struct
 import sys
 import sysconfig
 import warnings
 
+from ._typing import TYPE_CHECKING, cast
+
+if TYPE_CHECKING:  # pragma: no cover
+    from typing import (
+        Dict,
+        FrozenSet,
+        IO,
+        Iterable,
+        Iterator,
+        List,
+        Optional,
+        Sequence,
+        Tuple,
+        Union,
+    )
+
+    PythonVersion = Sequence[int]
+    MacVersion = Tuple[int, int]
+    GlibcVersion = Tuple[int, int]
+
+
+logger = logging.getLogger(__name__)
 
 INTERPRETER_SHORT_NAMES = {
     "python": "py",  # Generic.
@@ -26,34 +51,48 @@
     "pypy": "pp",
     "ironpython": "ip",
     "jython": "jy",
-}
+}  # type: Dict[str, str]
 
 
 _32_BIT_INTERPRETER = sys.maxsize <= 2 ** 32
 
 
 class Tag(object):
+    """
+    A representation of the tag triple for a wheel.
+
+    Instances are considered immutable and thus are hashable. Equality checking
+    is also supported.
+    """
 
     __slots__ = ["_interpreter", "_abi", "_platform"]
 
     def __init__(self, interpreter, abi, platform):
+        # type: (str, str, str) -> None
         self._interpreter = interpreter.lower()
         self._abi = abi.lower()
         self._platform = platform.lower()
 
     @property
     def interpreter(self):
+        # type: () -> str
         return self._interpreter
 
     @property
     def abi(self):
+        # type: () -> str
         return self._abi
 
     @property
     def platform(self):
+        # type: () -> str
         return self._platform
 
     def __eq__(self, other):
+        # type: (object) -> bool
+        if not isinstance(other, Tag):
+            return NotImplemented
+
         return (
             (self.platform == other.platform)
             and (self.abi == other.abi)
@@ -61,16 +100,26 @@
         )
 
     def __hash__(self):
+        # type: () -> int
         return hash((self._interpreter, self._abi, self._platform))
 
     def __str__(self):
+        # type: () -> str
         return "{}-{}-{}".format(self._interpreter, self._abi, self._platform)
 
     def __repr__(self):
+        # type: () -> str
         return "<{self} @ {self_id}>".format(self=self, self_id=id(self))
 
 
 def parse_tag(tag):
+    # type: (str) -> FrozenSet[Tag]
+    """
+    Parses the provided tag (e.g. `py3-none-any`) into a frozenset of Tag instances.
+
+    Returning a set is required due to the possibility that the tag is a
+    compressed tag set.
+    """
     tags = set()
     interpreters, abis, platforms = tag.split("-")
     for interpreter in interpreters.split("."):
@@ -80,20 +129,54 @@
     return frozenset(tags)
 
 
+def _warn_keyword_parameter(func_name, kwargs):
+    # type: (str, Dict[str, bool]) -> bool
+    """
+    Backwards-compatibility with Python 2.7 to allow treating 'warn' as keyword-only.
+    """
+    if not kwargs:
+        return False
+    elif len(kwargs) > 1 or "warn" not in kwargs:
+        kwargs.pop("warn", None)
+        arg = next(iter(kwargs.keys()))
+        raise TypeError(
+            "{}() got an unexpected keyword argument {!r}".format(func_name, arg)
+        )
+    return kwargs["warn"]
+
+
+def _get_config_var(name, warn=False):
+    # type: (str, bool) -> Union[int, str, None]
+    value = sysconfig.get_config_var(name)
+    if value is None and warn:
+        logger.debug(
+            "Config variable '%s' is unset, Python ABI tag may be incorrect", name
+        )
+    return value
+
+
 def _normalize_string(string):
+    # type: (str) -> str
     return string.replace(".", "_").replace("-", "_")
 
 
-def _cpython_interpreter(py_version):
-    # TODO: Is using py_version_nodot for interpreter version critical?
-    return "cp{major}{minor}".format(major=py_version[0], minor=py_version[1])
+def _abi3_applies(python_version):
+    # type: (PythonVersion) -> bool
+    """
+    Determine if the Python version supports abi3.
 
+    PEP 384 was first implemented in Python 3.2.
+    """
+    return len(python_version) > 1 and tuple(python_version) >= (3, 2)
 
-def _cpython_abis(py_version):
+
+def _cpython_abis(py_version, warn=False):
+    # type: (PythonVersion, bool) -> List[str]
+    py_version = tuple(py_version)  # To allow for version comparison.
     abis = []
-    version = "{}{}".format(*py_version[:2])
+    version = _version_nodot(py_version[:2])
     debug = pymalloc = ucs4 = ""
-    with_debug = sysconfig.get_config_var("Py_DEBUG")
+    with_debug = _get_config_var("Py_DEBUG", warn)
     has_refcount = hasattr(sys, "gettotalrefcount")
     # Windows doesn't set Py_DEBUG, so checking for support of debug-compiled
     # extension modules is the best option.
@@ -102,11 +185,11 @@
     if with_debug or (with_debug is None and (has_refcount or has_ext)):
         debug = "d"
     if py_version < (3, 8):
-        with_pymalloc = sysconfig.get_config_var("WITH_PYMALLOC")
+        with_pymalloc = _get_config_var("WITH_PYMALLOC", warn)
         if with_pymalloc or with_pymalloc is None:
             pymalloc = "m"
         if py_version < (3, 3):
-            unicode_size = sysconfig.get_config_var("Py_UNICODE_SIZE")
+            unicode_size = _get_config_var("Py_UNICODE_SIZE", warn)
             if unicode_size == 4 or (
                 unicode_size is None and sys.maxunicode == 0x10FFFF
             ):
@@ -124,86 +207,148 @@
     return abis
 
 
-def _cpython_tags(py_version, interpreter, abis, platforms):
+def cpython_tags(
+    python_version=None,  # type: Optional[PythonVersion]
+    abis=None,  # type: Optional[Iterable[str]]
+    platforms=None,  # type: Optional[Iterable[str]]
+    **kwargs  # type: bool
+):
+    # type: (...) -> Iterator[Tag]
+    """
+    Yields the tags for a CPython interpreter.
+
+    The tags consist of:
+    - cp<python_version>-<abi>-<platform>
+    - cp<python_version>-abi3-<platform>
+    - cp<python_version>-none-<platform>
+    - cp<less than python_version>-abi3-<platform>  # Older Python versions down to 3.2.
+
+    If python_version only specifies a major version then user-provided ABIs and
+    the 'none' ABItag will be used.
+
+    If 'abi3' or 'none' are specified in 'abis' then they will be yielded at
+    their normal position and not at the beginning.
+    """
+    warn = _warn_keyword_parameter("cpython_tags", kwargs)
+    if not python_version:
+        python_version = sys.version_info[:2]
+
+    interpreter = "cp{}".format(_version_nodot(python_version[:2]))
+
+    if abis is None:
+        if len(python_version) > 1:
+            abis = _cpython_abis(python_version, warn)
+        else:
+            abis = []
+    abis = list(abis)
+    # 'abi3' and 'none' are explicitly handled later.
+    for explicit_abi in ("abi3", "none"):
+        try:
+            abis.remove(explicit_abi)
+        except ValueError:
+            pass
+
+    platforms = list(platforms or _platform_tags())
     for abi in abis:
         for platform_ in platforms:
             yield Tag(interpreter, abi, platform_)
-    for tag in (Tag(interpreter, "abi3", platform_) for platform_ in platforms):
-        yield tag
+    if _abi3_applies(python_version):
+        for tag in (Tag(interpreter, "abi3", platform_) for platform_ in platforms):
+            yield tag
     for tag in (Tag(interpreter, "none", platform_) for platform_ in platforms):
         yield tag
-    # PEP 384 was first implemented in Python 3.2.
-    for minor_version in range(py_version[1] - 1, 1, -1):
-        for platform_ in platforms:
-            interpreter = "cp{major}{minor}".format(
-                major=py_version[0], minor=minor_version
-            )
-            yield Tag(interpreter, "abi3", platform_)
+
+    if _abi3_applies(python_version):
+        for minor_version in range(python_version[1] - 1, 1, -1):
+            for platform_ in platforms:
+                interpreter = "cp{version}".format(
+                    version=_version_nodot((python_version[0], minor_version))
+                )
+                yield Tag(interpreter, "abi3", platform_)
 
 
-def _pypy_interpreter():
-    return "pp{py_major}{pypy_major}{pypy_minor}".format(
-        py_major=sys.version_info[0],
-        pypy_major=sys.pypy_version_info.major,
-        pypy_minor=sys.pypy_version_info.minor,
-    )
-
-
 def _generic_abi():
+    # type: () -> Iterator[str]
     abi = sysconfig.get_config_var("SOABI")
     if abi:
-        return _normalize_string(abi)
-    else:
-        return "none"
+        yield _normalize_string(abi)
 
 
-def _pypy_tags(py_version, interpreter, abi, platforms):
-    for tag in (Tag(interpreter, abi, platform) for platform in platforms):
-        yield tag
-    for tag in (Tag(interpreter, "none", platform) for platform in platforms):
-        yield tag
+def generic_tags(
+    interpreter=None,  # type: Optional[str]
+    abis=None,  # type: Optional[Iterable[str]]
+    platforms=None,  # type: Optional[Iterable[str]]
+    **kwargs  # type: bool
+):
+    # type: (...) -> Iterator[Tag]
+    """
+    Yields the tags for a generic interpreter.
 
+    The tags consist of:
+    - <interpreter>-<abi>-<platform>
 
-def _generic_tags(interpreter, py_version, abi, platforms):
-    for tag in (Tag(interpreter, abi, platform) for platform in platforms):
-        yield tag
-    if abi != "none":
-        tags = (Tag(interpreter, "none", platform_) for platform_ in platforms)
-        for tag in tags:
-            yield tag
+    The "none" ABI will be added if it was not explicitly provided.
+    """
+    warn = _warn_keyword_parameter("generic_tags", kwargs)
+    if not interpreter:
+        interp_name = interpreter_name()
+        interp_version = interpreter_version(warn=warn)
+        interpreter = "".join([interp_name, interp_version])
+    if abis is None:
+        abis = _generic_abi()
+    platforms = list(platforms or _platform_tags())
+    abis = list(abis)
+    if "none" not in abis:
+        abis.append("none")
+    for abi in abis:
+        for platform_ in platforms:
+            yield Tag(interpreter, abi, platform_)
 
 
 def _py_interpreter_range(py_version):
+    # type: (PythonVersion) -> Iterator[str]
     """
-    Yield Python versions in descending order.
+    Yields Python versions in descending order.
 
     After the latest version, the major-only version will be yielded, and then
-    all following versions up to 'end'.
+    all previous versions of that major version.
     """
-    yield "py{major}{minor}".format(major=py_version[0], minor=py_version[1])
+    if len(py_version) > 1:
+        yield "py{version}".format(version=_version_nodot(py_version[:2]))
     yield "py{major}".format(major=py_version[0])
-    for minor in range(py_version[1] - 1, -1, -1):
-        yield "py{major}{minor}".format(major=py_version[0], minor=minor)
+    if len(py_version) > 1:
+        for minor in range(py_version[1] - 1, -1, -1):
+            yield "py{version}".format(version=_version_nodot((py_version[0], minor)))
 
 
-def _independent_tags(interpreter, py_version, platforms):
+def compatible_tags(
+    python_version=None,  # type: Optional[PythonVersion]
+    interpreter=None,  # type: Optional[str]
+    platforms=None,  # type: Optional[Iterable[str]]
+):
+    # type: (...) -> Iterator[Tag]
     """
-    Return the sequence of tags that are consistent across implementations.
+    Yields the sequence of tags that are compatible with a specific version of Python.
 
     The tags consist of:
     - py*-none-<platform>
-    - <interpreter>-none-any
+    - <interpreter>-none-any  # ... if `interpreter` is provided.
     - py*-none-any
     """
-    for version in _py_interpreter_range(py_version):
+    if not python_version:
+        python_version = sys.version_info[:2]
+    platforms = list(platforms or _platform_tags())
+    for version in _py_interpreter_range(python_version):
         for platform_ in platforms:
             yield Tag(version, "none", platform_)
-    yield Tag(interpreter, "none", "any")
-    for version in _py_interpreter_range(py_version):
+    if interpreter:
+        yield Tag(interpreter, "none", "any")
+    for version in _py_interpreter_range(python_version):
         yield Tag(version, "none", "any")
 
 
 def _mac_arch(arch, is_32bit=_32_BIT_INTERPRETER):
+    # type: (str, bool) -> str
     if not is_32bit:
         return arch
 
@@ -214,6 +359,7 @@
 
 
 def _mac_binary_formats(version, cpu_arch):
+    # type: (MacVersion, str) -> List[str]
     formats = [cpu_arch]
     if cpu_arch == "x86_64":
         if version < (10, 4):
@@ -240,32 +386,42 @@
     return formats
 
 
-def _mac_platforms(version=None, arch=None):
-    version_str, _, cpu_arch = platform.mac_ver()
+def mac_platforms(version=None, arch=None):
+    # type: (Optional[MacVersion], Optional[str]) -> Iterator[str]
+    """
+    Yields the platform tags for a macOS system.
+
+    The `version` parameter is a two-item tuple specifying the macOS version to
+    generate platform tags for. The `arch` parameter is the CPU architecture to
+    generate platform tags for. Both parameters default to the appropriate value
+    for the current system.
+    """
+    version_str, _, cpu_arch = platform.mac_ver()  # type: ignore
     if version is None:
-        version = tuple(map(int, version_str.split(".")[:2]))
+        version = cast("MacVersion", tuple(map(int, version_str.split(".")[:2])))
+    else:
+        version = version
     if arch is None:
         arch = _mac_arch(cpu_arch)
-    platforms = []
+    else:
+        arch = arch
     for minor_version in range(version[1], -1, -1):
         compat_version = version[0], minor_version
         binary_formats = _mac_binary_formats(compat_version, arch)
         for binary_format in binary_formats:
-            platforms.append(
-                "macosx_{major}_{minor}_{binary_format}".format(
-                    major=compat_version[0],
-                    minor=compat_version[1],
-                    binary_format=binary_format,
-                )
+            yield "macosx_{major}_{minor}_{binary_format}".format(
+                major=compat_version[0],
+                minor=compat_version[1],
+                binary_format=binary_format,
             )
-    return platforms
 
 
 # From PEP 513.
 def _is_manylinux_compatible(name, glibc_version):
+    # type: (str, GlibcVersion) -> bool
     # Check for presence of _manylinux module.
     try:
-        import _manylinux
+        import _manylinux  # noqa
 
         return bool(getattr(_manylinux, name + "_compatible"))
     except (ImportError, AttributeError):
@@ -276,14 +432,50 @@
 
 
 def _glibc_version_string():
+    # type: () -> Optional[str]
     # Returns glibc version string, or None if not using glibc.
-    import ctypes
+    return _glibc_version_string_confstr() or _glibc_version_string_ctypes()
+
+
+def _glibc_version_string_confstr():
+    # type: () -> Optional[str]
+    """
+    Primary implementation of glibc_version_string using os.confstr.
+    """
+    # os.confstr is quite a bit faster than ctypes.DLL. It's also less likely
+    # to be broken or missing. This strategy is used in the standard library
+    # platform module.
+    # https://github.com/python/cpython/blob/fcf1d003bf4f0100c9d0921ff3d70e1127ca1b71/Lib/platform.py#L175-L183
+    try:
+        # os.confstr("CS_GNU_LIBC_VERSION") returns a string like "glibc 2.17".
+        version_string = os.confstr(  # type: ignore[attr-defined] # noqa: F821
+            "CS_GNU_LIBC_VERSION"
+        )
+        assert version_string is not None
+        _, version = version_string.split()  # type: Tuple[str, str]
+    except (AssertionError, AttributeError, OSError, ValueError):
+        # os.confstr() or CS_GNU_LIBC_VERSION not available (or a bad value)...
+        return None
+    return version
+
+
+def _glibc_version_string_ctypes():
+    # type: () -> Optional[str]
+    """
+    Fallback implementation of glibc_version_string using ctypes.
+    """
+    try:
+        import ctypes
+    except ImportError:
+        return None
 
     # ctypes.CDLL(None) internally calls dlopen(NULL), and as the dlopen
     # manpage says, "If filename is NULL, then the returned handle is for the
     # main program". This way we can let the linker do the work to figure out
     # which libc our process is actually using.
-    process_namespace = ctypes.CDLL(None)
+    #
+    # Note: typeshed is wrong here so we are ignoring this line.
+    process_namespace = ctypes.CDLL(None)  # type: ignore
     try:
         gnu_get_libc_version = process_namespace.gnu_get_libc_version
     except AttributeError:
@@ -293,7 +485,7 @@
 
     # Call gnu_get_libc_version, which returns a string like "2.5"
     gnu_get_libc_version.restype = ctypes.c_char_p
-    version_str = gnu_get_libc_version()
+    version_str = gnu_get_libc_version()  # type: str
     # py2 / py3 compatibility:
     if not isinstance(version_str, str):
         version_str = version_str.decode("ascii")
@@ -303,6 +495,7 @@
 
 # Separated out from have_compatible_glibc for easier unit testing.
 def _check_glibc_version(version_str, required_major, minimum_minor):
+    # type: (str, int, int) -> bool
     # Parse string and check against requested version.
     #
     # We use a regexp instead of str.split because we want to discard any
@@ -324,81 +517,235 @@
 
 
 def _have_compatible_glibc(required_major, minimum_minor):
+    # type: (int, int) -> bool
     version_str = _glibc_version_string()
     if version_str is None:
         return False
     return _check_glibc_version(version_str, required_major, minimum_minor)
 
 
+# Python does not provide platform information at sufficient granularity to
+# identify the architecture of the running executable in some cases, so we
+# determine it dynamically by reading the information from the running
+# process. This only applies on Linux, which uses the ELF format.
+class _ELFFileHeader(object):
+    # https://en.wikipedia.org/wiki/Executable_and_Linkable_Format#File_header
+    class _InvalidELFFileHeader(ValueError):
+        """
+        An invalid ELF file header was found.
+        """
+
+    ELF_MAGIC_NUMBER = 0x7F454C46
+    ELFCLASS32 = 1
+    ELFCLASS64 = 2
+    ELFDATA2LSB = 1
+    ELFDATA2MSB = 2
+    EM_386 = 3
+    EM_S390 = 22
+    EM_ARM = 40
+    EM_X86_64 = 62
+    EF_ARM_ABIMASK = 0xFF000000
+    EF_ARM_ABI_VER5 = 0x05000000
+    EF_ARM_ABI_FLOAT_HARD = 0x00000400
+
+    def __init__(self, file):
+        # type: (IO[bytes]) -> None
+        def unpack(fmt):
+            # type: (str) -> int
+            try:
+                (result,) = struct.unpack(
+                    fmt, file.read(struct.calcsize(fmt))
+                )  # type: (int, )
+            except struct.error:
+                raise _ELFFileHeader._InvalidELFFileHeader()
+            return result
+
+        self.e_ident_magic = unpack(">I")
+        if self.e_ident_magic != self.ELF_MAGIC_NUMBER:
+            raise _ELFFileHeader._InvalidELFFileHeader()
+        self.e_ident_class = unpack("B")
+        if self.e_ident_class not in {self.ELFCLASS32, self.ELFCLASS64}:
+            raise _ELFFileHeader._InvalidELFFileHeader()
+        self.e_ident_data = unpack("B")
+        if self.e_ident_data not in {self.ELFDATA2LSB, self.ELFDATA2MSB}:
+            raise _ELFFileHeader._InvalidELFFileHeader()
+        self.e_ident_version = unpack("B")
+        self.e_ident_osabi = unpack("B")
+        self.e_ident_abiversion = unpack("B")
+        self.e_ident_pad = file.read(7)
+        format_h = "<H" if self.e_ident_data == self.ELFDATA2LSB else ">H"
+        format_i = "<I" if self.e_ident_data == self.ELFDATA2LSB else ">I"
+        format_q = "<Q" if self.e_ident_data == self.ELFDATA2LSB else ">Q"
+        format_p = format_i if self.e_ident_class == self.ELFCLASS32 else format_q
+        self.e_type = unpack(format_h)
+        self.e_machine = unpack(format_h)
+        self.e_version = unpack(format_i)
+        self.e_entry = unpack(format_p)
+        self.e_phoff = unpack(format_p)
+        self.e_shoff = unpack(format_p)
+        self.e_flags = unpack(format_i)
+        self.e_ehsize = unpack(format_h)
+        self.e_phentsize = unpack(format_h)
+        self.e_phnum = unpack(format_h)
+        self.e_shentsize = unpack(format_h)
+        self.e_shnum = unpack(format_h)
+        self.e_shstrndx = unpack(format_h)
+
+
+def _get_elf_header():
+    # type: () -> Optional[_ELFFileHeader]
+    try:
+        with open(sys.executable, "rb") as f:
+            elf_header = _ELFFileHeader(f)
+    except (IOError, OSError, TypeError, _ELFFileHeader._InvalidELFFileHeader):
+        return None
+    return elf_header
+
+
+def _is_linux_armhf():
+    # type: () -> bool
+    # hard-float ABI can be detected from the ELF header of the running
+    # process
+    # https://static.docs.arm.com/ihi0044/g/aaelf32.pdf
+    elf_header = _get_elf_header()
+    if elf_header is None:
+        return False
+    result = elf_header.e_ident_class == elf_header.ELFCLASS32
+    result &= elf_header.e_ident_data == elf_header.ELFDATA2LSB
+    result &= elf_header.e_machine == elf_header.EM_ARM
+    result &= (
+        elf_header.e_flags & elf_header.EF_ARM_ABIMASK
+    ) == elf_header.EF_ARM_ABI_VER5
+    result &= (
+        elf_header.e_flags & elf_header.EF_ARM_ABI_FLOAT_HARD
+    ) == elf_header.EF_ARM_ABI_FLOAT_HARD
+    return result
+
+
+def _is_linux_i686():
+    # type: () -> bool
+    elf_header = _get_elf_header()
+    if elf_header is None:
+        return False
+    result = elf_header.e_ident_class == elf_header.ELFCLASS32
+    result &= elf_header.e_ident_data == elf_header.ELFDATA2LSB
+    result &= elf_header.e_machine == elf_header.EM_386
+    return result
+
+
+def _have_compatible_manylinux_abi(arch):
+    # type: (str) -> bool
+    if arch == "armv7l":
+        return _is_linux_armhf()
+    if arch == "i686":
+        return _is_linux_i686()
+    return True
+
+
 def _linux_platforms(is_32bit=_32_BIT_INTERPRETER):
+    # type: (bool) -> Iterator[str]
     linux = _normalize_string(distutils.util.get_platform())
-    if linux == "linux_x86_64" and is_32bit:
-        linux = "linux_i686"
-    manylinux_support = (
-        ("manylinux2014", (2, 17)),  # CentOS 7 w/ glibc 2.17 (PEP 599)
-        ("manylinux2010", (2, 12)),  # CentOS 6 w/ glibc 2.12 (PEP 571)
-        ("manylinux1", (2, 5)),  # CentOS 5 w/ glibc 2.5 (PEP 513)
-    )
+    if is_32bit:
+        if linux == "linux_x86_64":
+            linux = "linux_i686"
+        elif linux == "linux_aarch64":
+            linux = "linux_armv7l"
+    manylinux_support = []
+    _, arch = linux.split("_", 1)
+    if _have_compatible_manylinux_abi(arch):
+        if arch in {"x86_64", "i686", "aarch64", "armv7l", "ppc64", "ppc64le", "s390x"}:
+            manylinux_support.append(
+                ("manylinux2014", (2, 17))
+            )  # CentOS 7 w/ glibc 2.17 (PEP 599)
+        if arch in {"x86_64", "i686"}:
+            manylinux_support.append(
+                ("manylinux2010", (2, 12))
+            )  # CentOS 6 w/ glibc 2.12 (PEP 571)
+            manylinux_support.append(
+                ("manylinux1", (2, 5))
+            )  # CentOS 5 w/ glibc 2.5 (PEP 513)
     manylinux_support_iter = iter(manylinux_support)
     for name, glibc_version in manylinux_support_iter:
         if _is_manylinux_compatible(name, glibc_version):
-            platforms = [linux.replace("linux", name)]
+            yield linux.replace("linux", name)
             break
-    else:
-        platforms = []
     # Support for a later manylinux implies support for an earlier version.
-    platforms += [linux.replace("linux", name) for name, _ in manylinux_support_iter]
-    platforms.append(linux)
-    return platforms
+    for name, _ in manylinux_support_iter:
+        yield linux.replace("linux", name)
+    yield linux
 
 
 def _generic_platforms():
-    platform = _normalize_string(distutils.util.get_platform())
-    return [platform]
+    # type: () -> Iterator[str]
+    yield _normalize_string(distutils.util.get_platform())
+
 
+def _platform_tags():
+    # type: () -> Iterator[str]
+    """
+    Provides the platform tags for this installation.
+    """
+    if platform.system() == "Darwin":
+        return mac_platforms()
+    elif platform.system() == "Linux":
+        return _linux_platforms()
+    else:
+        return _generic_platforms()
 
-def _interpreter_name():
-    name = platform.python_implementation().lower()
+
+def interpreter_name():
+    # type: () -> str
+    """
+    Returns the name of the running interpreter.
+    """
+    try:
+        name = sys.implementation.name  # type: ignore
+    except AttributeError:  # pragma: no cover
+        # Python 2.7 compatibility.
+        name = platform.python_implementation().lower()
     return INTERPRETER_SHORT_NAMES.get(name) or name
 
 
-def _generic_interpreter(name, py_version):
-    version = sysconfig.get_config_var("py_version_nodot")
-    if not version:
-        version = "".join(map(str, py_version[:2]))
-    return "{name}{version}".format(name=name, version=version)
+def interpreter_version(**kwargs):
+    # type: (bool) -> str
+    """
+    Returns the version of the running interpreter.
+    """
+    warn = _warn_keyword_parameter("interpreter_version", kwargs)
+    version = _get_config_var("py_version_nodot", warn=warn)
+    if version:
+        version = str(version)
+    else:
+        version = _version_nodot(sys.version_info[:2])
+    return version
+
 
+def _version_nodot(version):
+    # type: (PythonVersion) -> str
+    if any(v >= 10 for v in version):
+        sep = "_"
+    else:
+        sep = ""
+    return sep.join(map(str, version))
 
-def sys_tags():
+
+def sys_tags(**kwargs):
+    # type: (bool) -> Iterator[Tag]
     """
     Returns the sequence of tag triples for the running interpreter.
 
     The order of the sequence corresponds to priority order for the
     interpreter, from most to least important.
     """
-    py_version = sys.version_info[:2]
-    interpreter_name = _interpreter_name()
-    if platform.system() == "Darwin":
-        platforms = _mac_platforms()
-    elif platform.system() == "Linux":
-        platforms = _linux_platforms()
-    else:
-        platforms = _generic_platforms()
+    warn = _warn_keyword_parameter("sys_tags", kwargs)
 
-    if interpreter_name == "cp":
-        interpreter = _cpython_interpreter(py_version)
-        abis = _cpython_abis(py_version)
-        for tag in _cpython_tags(py_version, interpreter, abis, platforms):
-            yield tag
-    elif interpreter_name == "pp":
-        interpreter = _pypy_interpreter()
-        abi = _generic_abi()
-        for tag in _pypy_tags(py_version, interpreter, abi, platforms):
+    interp_name = interpreter_name()
+    if interp_name == "cp":
+        for tag in cpython_tags(warn=warn):
             yield tag
     else:
-        interpreter = _generic_interpreter(interpreter_name, py_version)
-        abi = _generic_abi()
-        for tag in _generic_tags(interpreter, py_version, abi, platforms):
+        for tag in generic_tags():
             yield tag
-    for tag in _independent_tags(interpreter, py_version, platforms):
+
+    for tag in compatible_tags():
         yield tag
Index: env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/_structures.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import absolute_import, division, print_function\n\n\nclass Infinity(object):\n    def __repr__(self):\n        return \"Infinity\"\n\n    def __hash__(self):\n        return hash(repr(self))\n\n    def __lt__(self, other):\n        return False\n\n    def __le__(self, other):\n        return False\n\n    def __eq__(self, other):\n        return isinstance(other, self.__class__)\n\n    def __ne__(self, other):\n        return not isinstance(other, self.__class__)\n\n    def __gt__(self, other):\n        return True\n\n    def __ge__(self, other):\n        return True\n\n    def __neg__(self):\n        return NegativeInfinity\n\n\nInfinity = Infinity()\n\n\nclass NegativeInfinity(object):\n    def __repr__(self):\n        return \"-Infinity\"\n\n    def __hash__(self):\n        return hash(repr(self))\n\n    def __lt__(self, other):\n        return True\n\n    def __le__(self, other):\n        return True\n\n    def __eq__(self, other):\n        return isinstance(other, self.__class__)\n\n    def __ne__(self, other):\n        return not isinstance(other, self.__class__)\n\n    def __gt__(self, other):\n        return False\n\n    def __ge__(self, other):\n        return False\n\n    def __neg__(self):\n        return Infinity\n\n\nNegativeInfinity = NegativeInfinity()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/_structures.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/_structures.py	(date 1602088701593)
@@ -4,65 +4,83 @@
 from __future__ import absolute_import, division, print_function
 
 
-class Infinity(object):
+class InfinityType(object):
     def __repr__(self):
+        # type: () -> str
         return "Infinity"
 
     def __hash__(self):
+        # type: () -> int
         return hash(repr(self))
 
     def __lt__(self, other):
+        # type: (object) -> bool
         return False
 
     def __le__(self, other):
+        # type: (object) -> bool
         return False
 
     def __eq__(self, other):
+        # type: (object) -> bool
         return isinstance(other, self.__class__)
 
     def __ne__(self, other):
+        # type: (object) -> bool
         return not isinstance(other, self.__class__)
 
     def __gt__(self, other):
+        # type: (object) -> bool
         return True
 
     def __ge__(self, other):
+        # type: (object) -> bool
         return True
 
     def __neg__(self):
+        # type: (object) -> NegativeInfinityType
         return NegativeInfinity
 
 
-Infinity = Infinity()
+Infinity = InfinityType()
 
 
-class NegativeInfinity(object):
+class NegativeInfinityType(object):
     def __repr__(self):
+        # type: () -> str
         return "-Infinity"
 
     def __hash__(self):
+        # type: () -> int
         return hash(repr(self))
 
     def __lt__(self, other):
+        # type: (object) -> bool
         return True
 
     def __le__(self, other):
+        # type: (object) -> bool
         return True
 
     def __eq__(self, other):
+        # type: (object) -> bool
         return isinstance(other, self.__class__)
 
     def __ne__(self, other):
+        # type: (object) -> bool
         return not isinstance(other, self.__class__)
 
     def __gt__(self, other):
+        # type: (object) -> bool
         return False
 
     def __ge__(self, other):
+        # type: (object) -> bool
         return False
 
     def __neg__(self):
+        # type: (object) -> InfinityType
         return Infinity
 
 
-NegativeInfinity = NegativeInfinity()
+NegativeInfinity = NegativeInfinityType()
Index: env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/specifiers.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\nfrom __future__ import absolute_import, division, print_function\n\nimport abc\nimport functools\nimport itertools\nimport re\n\nfrom ._compat import string_types, with_metaclass\nfrom .version import Version, LegacyVersion, parse\n\n\nclass InvalidSpecifier(ValueError):\n    \"\"\"\n    An invalid specifier was found, users should refer to PEP 440.\n    \"\"\"\n\n\nclass BaseSpecifier(with_metaclass(abc.ABCMeta, object)):\n    @abc.abstractmethod\n    def __str__(self):\n        \"\"\"\n        Returns the str representation of this Specifier like object. This\n        should be representative of the Specifier itself.\n        \"\"\"\n\n    @abc.abstractmethod\n    def __hash__(self):\n        \"\"\"\n        Returns a hash value for this Specifier like object.\n        \"\"\"\n\n    @abc.abstractmethod\n    def __eq__(self, other):\n        \"\"\"\n        Returns a boolean representing whether or not the two Specifier like\n        objects are equal.\n        \"\"\"\n\n    @abc.abstractmethod\n    def __ne__(self, other):\n        \"\"\"\n        Returns a boolean representing whether or not the two Specifier like\n        objects are not equal.\n        \"\"\"\n\n    @abc.abstractproperty\n    def prereleases(self):\n        \"\"\"\n        Returns whether or not pre-releases as a whole are allowed by this\n        specifier.\n        \"\"\"\n\n    @prereleases.setter\n    def prereleases(self, value):\n        \"\"\"\n        Sets whether or not pre-releases as a whole are allowed by this\n        specifier.\n        \"\"\"\n\n    @abc.abstractmethod\n    def contains(self, item, prereleases=None):\n        \"\"\"\n        Determines if the given item is contained within this specifier.\n        \"\"\"\n\n    @abc.abstractmethod\n    def filter(self, iterable, prereleases=None):\n        \"\"\"\n        Takes an iterable of items and filters them so that only items which\n        are contained within this specifier are allowed in it.\n        \"\"\"\n\n\nclass _IndividualSpecifier(BaseSpecifier):\n\n    _operators = {}\n\n    def __init__(self, spec=\"\", prereleases=None):\n        match = self._regex.search(spec)\n        if not match:\n            raise InvalidSpecifier(\"Invalid specifier: '{0}'\".format(spec))\n\n        self._spec = (match.group(\"operator\").strip(), match.group(\"version\").strip())\n\n        # Store whether or not this Specifier should accept prereleases\n        self._prereleases = prereleases\n\n    def __repr__(self):\n        pre = (\n            \", prereleases={0!r}\".format(self.prereleases)\n            if self._prereleases is not None\n            else \"\"\n        )\n\n        return \"<{0}({1!r}{2})>\".format(self.__class__.__name__, str(self), pre)\n\n    def __str__(self):\n        return \"{0}{1}\".format(*self._spec)\n\n    def __hash__(self):\n        return hash(self._spec)\n\n    def __eq__(self, other):\n        if isinstance(other, string_types):\n            try:\n                other = self.__class__(other)\n            except InvalidSpecifier:\n                return NotImplemented\n        elif not isinstance(other, self.__class__):\n            return NotImplemented\n\n        return self._spec == other._spec\n\n    def __ne__(self, other):\n        if isinstance(other, string_types):\n            try:\n                other = self.__class__(other)\n            except InvalidSpecifier:\n                return NotImplemented\n        elif not isinstance(other, self.__class__):\n            return NotImplemented\n\n        return self._spec != other._spec\n\n    def _get_operator(self, op):\n        return getattr(self, \"_compare_{0}\".format(self._operators[op]))\n\n    def _coerce_version(self, version):\n        if not isinstance(version, (LegacyVersion, Version)):\n            version = parse(version)\n        return version\n\n    @property\n    def operator(self):\n        return self._spec[0]\n\n    @property\n    def version(self):\n        return self._spec[1]\n\n    @property\n    def prereleases(self):\n        return self._prereleases\n\n    @prereleases.setter\n    def prereleases(self, value):\n        self._prereleases = value\n\n    def __contains__(self, item):\n        return self.contains(item)\n\n    def contains(self, item, prereleases=None):\n        # Determine if prereleases are to be allowed or not.\n        if prereleases is None:\n            prereleases = self.prereleases\n\n        # Normalize item to a Version or LegacyVersion, this allows us to have\n        # a shortcut for ``\"2.0\" in Specifier(\">=2\")\n        item = self._coerce_version(item)\n\n        # Determine if we should be supporting prereleases in this specifier\n        # or not, if we do not support prereleases than we can short circuit\n        # logic if this version is a prereleases.\n        if item.is_prerelease and not prereleases:\n            return False\n\n        # Actually do the comparison to determine if this item is contained\n        # within this Specifier or not.\n        return self._get_operator(self.operator)(item, self.version)\n\n    def filter(self, iterable, prereleases=None):\n        yielded = False\n        found_prereleases = []\n\n        kw = {\"prereleases\": prereleases if prereleases is not None else True}\n\n        # Attempt to iterate over all the values in the iterable and if any of\n        # them match, yield them.\n        for version in iterable:\n            parsed_version = self._coerce_version(version)\n\n            if self.contains(parsed_version, **kw):\n                # If our version is a prerelease, and we were not set to allow\n                # prereleases, then we'll store it for later incase nothing\n                # else matches this specifier.\n                if parsed_version.is_prerelease and not (\n                    prereleases or self.prereleases\n                ):\n                    found_prereleases.append(version)\n                # Either this is not a prerelease, or we should have been\n                # accepting prereleases from the beginning.\n                else:\n                    yielded = True\n                    yield version\n\n        # Now that we've iterated over everything, determine if we've yielded\n        # any values, and if we have not and we have any prereleases stored up\n        # then we will go ahead and yield the prereleases.\n        if not yielded and found_prereleases:\n            for version in found_prereleases:\n                yield version\n\n\nclass LegacySpecifier(_IndividualSpecifier):\n\n    _regex_str = r\"\"\"\n        (?P<operator>(==|!=|<=|>=|<|>))\n        \\s*\n        (?P<version>\n            [^,;\\s)]* # Since this is a \"legacy\" specifier, and the version\n                      # string can be just about anything, we match everything\n                      # except for whitespace, a semi-colon for marker support,\n                      # a closing paren since versions can be enclosed in\n                      # them, and a comma since it's a version separator.\n        )\n        \"\"\"\n\n    _regex = re.compile(r\"^\\s*\" + _regex_str + r\"\\s*$\", re.VERBOSE | re.IGNORECASE)\n\n    _operators = {\n        \"==\": \"equal\",\n        \"!=\": \"not_equal\",\n        \"<=\": \"less_than_equal\",\n        \">=\": \"greater_than_equal\",\n        \"<\": \"less_than\",\n        \">\": \"greater_than\",\n    }\n\n    def _coerce_version(self, version):\n        if not isinstance(version, LegacyVersion):\n            version = LegacyVersion(str(version))\n        return version\n\n    def _compare_equal(self, prospective, spec):\n        return prospective == self._coerce_version(spec)\n\n    def _compare_not_equal(self, prospective, spec):\n        return prospective != self._coerce_version(spec)\n\n    def _compare_less_than_equal(self, prospective, spec):\n        return prospective <= self._coerce_version(spec)\n\n    def _compare_greater_than_equal(self, prospective, spec):\n        return prospective >= self._coerce_version(spec)\n\n    def _compare_less_than(self, prospective, spec):\n        return prospective < self._coerce_version(spec)\n\n    def _compare_greater_than(self, prospective, spec):\n        return prospective > self._coerce_version(spec)\n\n\ndef _require_version_compare(fn):\n    @functools.wraps(fn)\n    def wrapped(self, prospective, spec):\n        if not isinstance(prospective, Version):\n            return False\n        return fn(self, prospective, spec)\n\n    return wrapped\n\n\nclass Specifier(_IndividualSpecifier):\n\n    _regex_str = r\"\"\"\n        (?P<operator>(~=|==|!=|<=|>=|<|>|===))\n        (?P<version>\n            (?:\n                # The identity operators allow for an escape hatch that will\n                # do an exact string match of the version you wish to install.\n                # This will not be parsed by PEP 440 and we cannot determine\n                # any semantic meaning from it. This operator is discouraged\n                # but included entirely as an escape hatch.\n                (?<====)  # Only match for the identity operator\n                \\s*\n                [^\\s]*    # We just match everything, except for whitespace\n                          # since we are only testing for strict identity.\n            )\n            |\n            (?:\n                # The (non)equality operators allow for wild card and local\n                # versions to be specified so we have to define these two\n                # operators separately to enable that.\n                (?<===|!=)            # Only match for equals and not equals\n\n                \\s*\n                v?\n                (?:[0-9]+!)?          # epoch\n                [0-9]+(?:\\.[0-9]+)*   # release\n                (?:                   # pre release\n                    [-_\\.]?\n                    (a|b|c|rc|alpha|beta|pre|preview)\n                    [-_\\.]?\n                    [0-9]*\n                )?\n                (?:                   # post release\n                    (?:-[0-9]+)|(?:[-_\\.]?(post|rev|r)[-_\\.]?[0-9]*)\n                )?\n\n                # You cannot use a wild card and a dev or local version\n                # together so group them with a | and make them optional.\n                (?:\n                    (?:[-_\\.]?dev[-_\\.]?[0-9]*)?         # dev release\n                    (?:\\+[a-z0-9]+(?:[-_\\.][a-z0-9]+)*)? # local\n                    |\n                    \\.\\*  # Wild card syntax of .*\n                )?\n            )\n            |\n            (?:\n                # The compatible operator requires at least two digits in the\n                # release segment.\n                (?<=~=)               # Only match for the compatible operator\n\n                \\s*\n                v?\n                (?:[0-9]+!)?          # epoch\n                [0-9]+(?:\\.[0-9]+)+   # release  (We have a + instead of a *)\n                (?:                   # pre release\n                    [-_\\.]?\n                    (a|b|c|rc|alpha|beta|pre|preview)\n                    [-_\\.]?\n                    [0-9]*\n                )?\n                (?:                                   # post release\n                    (?:-[0-9]+)|(?:[-_\\.]?(post|rev|r)[-_\\.]?[0-9]*)\n                )?\n                (?:[-_\\.]?dev[-_\\.]?[0-9]*)?          # dev release\n            )\n            |\n            (?:\n                # All other operators only allow a sub set of what the\n                # (non)equality operators do. Specifically they do not allow\n                # local versions to be specified nor do they allow the prefix\n                # matching wild cards.\n                (?<!==|!=|~=)         # We have special cases for these\n                                      # operators so we want to make sure they\n                                      # don't match here.\n\n                \\s*\n                v?\n                (?:[0-9]+!)?          # epoch\n                [0-9]+(?:\\.[0-9]+)*   # release\n                (?:                   # pre release\n                    [-_\\.]?\n                    (a|b|c|rc|alpha|beta|pre|preview)\n                    [-_\\.]?\n                    [0-9]*\n                )?\n                (?:                                   # post release\n                    (?:-[0-9]+)|(?:[-_\\.]?(post|rev|r)[-_\\.]?[0-9]*)\n                )?\n                (?:[-_\\.]?dev[-_\\.]?[0-9]*)?          # dev release\n            )\n        )\n        \"\"\"\n\n    _regex = re.compile(r\"^\\s*\" + _regex_str + r\"\\s*$\", re.VERBOSE | re.IGNORECASE)\n\n    _operators = {\n        \"~=\": \"compatible\",\n        \"==\": \"equal\",\n        \"!=\": \"not_equal\",\n        \"<=\": \"less_than_equal\",\n        \">=\": \"greater_than_equal\",\n        \"<\": \"less_than\",\n        \">\": \"greater_than\",\n        \"===\": \"arbitrary\",\n    }\n\n    @_require_version_compare\n    def _compare_compatible(self, prospective, spec):\n        # Compatible releases have an equivalent combination of >= and ==. That\n        # is that ~=2.2 is equivalent to >=2.2,==2.*. This allows us to\n        # implement this in terms of the other specifiers instead of\n        # implementing it ourselves. The only thing we need to do is construct\n        # the other specifiers.\n\n        # We want everything but the last item in the version, but we want to\n        # ignore post and dev releases and we want to treat the pre-release as\n        # it's own separate segment.\n        prefix = \".\".join(\n            list(\n                itertools.takewhile(\n                    lambda x: (not x.startswith(\"post\") and not x.startswith(\"dev\")),\n                    _version_split(spec),\n                )\n            )[:-1]\n        )\n\n        # Add the prefix notation to the end of our string\n        prefix += \".*\"\n\n        return self._get_operator(\">=\")(prospective, spec) and self._get_operator(\"==\")(\n            prospective, prefix\n        )\n\n    @_require_version_compare\n    def _compare_equal(self, prospective, spec):\n        # We need special logic to handle prefix matching\n        if spec.endswith(\".*\"):\n            # In the case of prefix matching we want to ignore local segment.\n            prospective = Version(prospective.public)\n            # Split the spec out by dots, and pretend that there is an implicit\n            # dot in between a release segment and a pre-release segment.\n            spec = _version_split(spec[:-2])  # Remove the trailing .*\n\n            # Split the prospective version out by dots, and pretend that there\n            # is an implicit dot in between a release segment and a pre-release\n            # segment.\n            prospective = _version_split(str(prospective))\n\n            # Shorten the prospective version to be the same length as the spec\n            # so that we can determine if the specifier is a prefix of the\n            # prospective version or not.\n            prospective = prospective[: len(spec)]\n\n            # Pad out our two sides with zeros so that they both equal the same\n            # length.\n            spec, prospective = _pad_version(spec, prospective)\n        else:\n            # Convert our spec string into a Version\n            spec = Version(spec)\n\n            # If the specifier does not have a local segment, then we want to\n            # act as if the prospective version also does not have a local\n            # segment.\n            if not spec.local:\n                prospective = Version(prospective.public)\n\n        return prospective == spec\n\n    @_require_version_compare\n    def _compare_not_equal(self, prospective, spec):\n        return not self._compare_equal(prospective, spec)\n\n    @_require_version_compare\n    def _compare_less_than_equal(self, prospective, spec):\n        return prospective <= Version(spec)\n\n    @_require_version_compare\n    def _compare_greater_than_equal(self, prospective, spec):\n        return prospective >= Version(spec)\n\n    @_require_version_compare\n    def _compare_less_than(self, prospective, spec):\n        # Convert our spec to a Version instance, since we'll want to work with\n        # it as a version.\n        spec = Version(spec)\n\n        # Check to see if the prospective version is less than the spec\n        # version. If it's not we can short circuit and just return False now\n        # instead of doing extra unneeded work.\n        if not prospective < spec:\n            return False\n\n        # This special case is here so that, unless the specifier itself\n        # includes is a pre-release version, that we do not accept pre-release\n        # versions for the version mentioned in the specifier (e.g. <3.1 should\n        # not match 3.1.dev0, but should match 3.0.dev0).\n        if not spec.is_prerelease and prospective.is_prerelease:\n            if Version(prospective.base_version) == Version(spec.base_version):\n                return False\n\n        # If we've gotten to here, it means that prospective version is both\n        # less than the spec version *and* it's not a pre-release of the same\n        # version in the spec.\n        return True\n\n    @_require_version_compare\n    def _compare_greater_than(self, prospective, spec):\n        # Convert our spec to a Version instance, since we'll want to work with\n        # it as a version.\n        spec = Version(spec)\n\n        # Check to see if the prospective version is greater than the spec\n        # version. If it's not we can short circuit and just return False now\n        # instead of doing extra unneeded work.\n        if not prospective > spec:\n            return False\n\n        # This special case is here so that, unless the specifier itself\n        # includes is a post-release version, that we do not accept\n        # post-release versions for the version mentioned in the specifier\n        # (e.g. >3.1 should not match 3.0.post0, but should match 3.2.post0).\n        if not spec.is_postrelease and prospective.is_postrelease:\n            if Version(prospective.base_version) == Version(spec.base_version):\n                return False\n\n        # Ensure that we do not allow a local version of the version mentioned\n        # in the specifier, which is technically greater than, to match.\n        if prospective.local is not None:\n            if Version(prospective.base_version) == Version(spec.base_version):\n                return False\n\n        # If we've gotten to here, it means that prospective version is both\n        # greater than the spec version *and* it's not a pre-release of the\n        # same version in the spec.\n        return True\n\n    def _compare_arbitrary(self, prospective, spec):\n        return str(prospective).lower() == str(spec).lower()\n\n    @property\n    def prereleases(self):\n        # If there is an explicit prereleases set for this, then we'll just\n        # blindly use that.\n        if self._prereleases is not None:\n            return self._prereleases\n\n        # Look at all of our specifiers and determine if they are inclusive\n        # operators, and if they are if they are including an explicit\n        # prerelease.\n        operator, version = self._spec\n        if operator in [\"==\", \">=\", \"<=\", \"~=\", \"===\"]:\n            # The == specifier can include a trailing .*, if it does we\n            # want to remove before parsing.\n            if operator == \"==\" and version.endswith(\".*\"):\n                version = version[:-2]\n\n            # Parse the version, and if it is a pre-release than this\n            # specifier allows pre-releases.\n            if parse(version).is_prerelease:\n                return True\n\n        return False\n\n    @prereleases.setter\n    def prereleases(self, value):\n        self._prereleases = value\n\n\n_prefix_regex = re.compile(r\"^([0-9]+)((?:a|b|c|rc)[0-9]+)$\")\n\n\ndef _version_split(version):\n    result = []\n    for item in version.split(\".\"):\n        match = _prefix_regex.search(item)\n        if match:\n            result.extend(match.groups())\n        else:\n            result.append(item)\n    return result\n\n\ndef _pad_version(left, right):\n    left_split, right_split = [], []\n\n    # Get the release segment of our versions\n    left_split.append(list(itertools.takewhile(lambda x: x.isdigit(), left)))\n    right_split.append(list(itertools.takewhile(lambda x: x.isdigit(), right)))\n\n    # Get the rest of our versions\n    left_split.append(left[len(left_split[0]) :])\n    right_split.append(right[len(right_split[0]) :])\n\n    # Insert our padding\n    left_split.insert(1, [\"0\"] * max(0, len(right_split[0]) - len(left_split[0])))\n    right_split.insert(1, [\"0\"] * max(0, len(left_split[0]) - len(right_split[0])))\n\n    return (list(itertools.chain(*left_split)), list(itertools.chain(*right_split)))\n\n\nclass SpecifierSet(BaseSpecifier):\n    def __init__(self, specifiers=\"\", prereleases=None):\n        # Split on , to break each indidivual specifier into it's own item, and\n        # strip each item to remove leading/trailing whitespace.\n        specifiers = [s.strip() for s in specifiers.split(\",\") if s.strip()]\n\n        # Parsed each individual specifier, attempting first to make it a\n        # Specifier and falling back to a LegacySpecifier.\n        parsed = set()\n        for specifier in specifiers:\n            try:\n                parsed.add(Specifier(specifier))\n            except InvalidSpecifier:\n                parsed.add(LegacySpecifier(specifier))\n\n        # Turn our parsed specifiers into a frozen set and save them for later.\n        self._specs = frozenset(parsed)\n\n        # Store our prereleases value so we can use it later to determine if\n        # we accept prereleases or not.\n        self._prereleases = prereleases\n\n    def __repr__(self):\n        pre = (\n            \", prereleases={0!r}\".format(self.prereleases)\n            if self._prereleases is not None\n            else \"\"\n        )\n\n        return \"<SpecifierSet({0!r}{1})>\".format(str(self), pre)\n\n    def __str__(self):\n        return \",\".join(sorted(str(s) for s in self._specs))\n\n    def __hash__(self):\n        return hash(self._specs)\n\n    def __and__(self, other):\n        if isinstance(other, string_types):\n            other = SpecifierSet(other)\n        elif not isinstance(other, SpecifierSet):\n            return NotImplemented\n\n        specifier = SpecifierSet()\n        specifier._specs = frozenset(self._specs | other._specs)\n\n        if self._prereleases is None and other._prereleases is not None:\n            specifier._prereleases = other._prereleases\n        elif self._prereleases is not None and other._prereleases is None:\n            specifier._prereleases = self._prereleases\n        elif self._prereleases == other._prereleases:\n            specifier._prereleases = self._prereleases\n        else:\n            raise ValueError(\n                \"Cannot combine SpecifierSets with True and False prerelease \"\n                \"overrides.\"\n            )\n\n        return specifier\n\n    def __eq__(self, other):\n        if isinstance(other, string_types):\n            other = SpecifierSet(other)\n        elif isinstance(other, _IndividualSpecifier):\n            other = SpecifierSet(str(other))\n        elif not isinstance(other, SpecifierSet):\n            return NotImplemented\n\n        return self._specs == other._specs\n\n    def __ne__(self, other):\n        if isinstance(other, string_types):\n            other = SpecifierSet(other)\n        elif isinstance(other, _IndividualSpecifier):\n            other = SpecifierSet(str(other))\n        elif not isinstance(other, SpecifierSet):\n            return NotImplemented\n\n        return self._specs != other._specs\n\n    def __len__(self):\n        return len(self._specs)\n\n    def __iter__(self):\n        return iter(self._specs)\n\n    @property\n    def prereleases(self):\n        # If we have been given an explicit prerelease modifier, then we'll\n        # pass that through here.\n        if self._prereleases is not None:\n            return self._prereleases\n\n        # If we don't have any specifiers, and we don't have a forced value,\n        # then we'll just return None since we don't know if this should have\n        # pre-releases or not.\n        if not self._specs:\n            return None\n\n        # Otherwise we'll see if any of the given specifiers accept\n        # prereleases, if any of them do we'll return True, otherwise False.\n        return any(s.prereleases for s in self._specs)\n\n    @prereleases.setter\n    def prereleases(self, value):\n        self._prereleases = value\n\n    def __contains__(self, item):\n        return self.contains(item)\n\n    def contains(self, item, prereleases=None):\n        # Ensure that our item is a Version or LegacyVersion instance.\n        if not isinstance(item, (LegacyVersion, Version)):\n            item = parse(item)\n\n        # Determine if we're forcing a prerelease or not, if we're not forcing\n        # one for this particular filter call, then we'll use whatever the\n        # SpecifierSet thinks for whether or not we should support prereleases.\n        if prereleases is None:\n            prereleases = self.prereleases\n\n        # We can determine if we're going to allow pre-releases by looking to\n        # see if any of the underlying items supports them. If none of them do\n        # and this item is a pre-release then we do not allow it and we can\n        # short circuit that here.\n        # Note: This means that 1.0.dev1 would not be contained in something\n        #       like >=1.0.devabc however it would be in >=1.0.debabc,>0.0.dev0\n        if not prereleases and item.is_prerelease:\n            return False\n\n        # We simply dispatch to the underlying specs here to make sure that the\n        # given version is contained within all of them.\n        # Note: This use of all() here means that an empty set of specifiers\n        #       will always return True, this is an explicit design decision.\n        return all(s.contains(item, prereleases=prereleases) for s in self._specs)\n\n    def filter(self, iterable, prereleases=None):\n        # Determine if we're forcing a prerelease or not, if we're not forcing\n        # one for this particular filter call, then we'll use whatever the\n        # SpecifierSet thinks for whether or not we should support prereleases.\n        if prereleases is None:\n            prereleases = self.prereleases\n\n        # If we have any specifiers, then we want to wrap our iterable in the\n        # filter method for each one, this will act as a logical AND amongst\n        # each specifier.\n        if self._specs:\n            for spec in self._specs:\n                iterable = spec.filter(iterable, prereleases=bool(prereleases))\n            return iterable\n        # If we do not have any specifiers, then we need to have a rough filter\n        # which will filter out any pre-releases, unless there are no final\n        # releases, and which will filter out LegacyVersion in general.\n        else:\n            filtered = []\n            found_prereleases = []\n\n            for item in iterable:\n                # Ensure that we some kind of Version class for this item.\n                if not isinstance(item, (LegacyVersion, Version)):\n                    parsed_version = parse(item)\n                else:\n                    parsed_version = item\n\n                # Filter out any item which is parsed as a LegacyVersion\n                if isinstance(parsed_version, LegacyVersion):\n                    continue\n\n                # Store any item which is a pre-release for later unless we've\n                # already found a final version or we are accepting prereleases\n                if parsed_version.is_prerelease and not prereleases:\n                    if not filtered:\n                        found_prereleases.append(item)\n                else:\n                    filtered.append(item)\n\n            # If we've found no items except for pre-releases, then we'll go\n            # ahead and use the pre-releases\n            if not filtered and found_prereleases and prereleases is None:\n                return found_prereleases\n\n            return filtered\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/specifiers.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/specifiers.py	(date 1602088701593)
@@ -9,8 +9,27 @@
 import re
 
 from ._compat import string_types, with_metaclass
+from ._typing import TYPE_CHECKING
+from .utils import canonicalize_version
 from .version import Version, LegacyVersion, parse
 
+if TYPE_CHECKING:  # pragma: no cover
+    from typing import (
+        List,
+        Dict,
+        Union,
+        Iterable,
+        Iterator,
+        Optional,
+        Callable,
+        Tuple,
+        FrozenSet,
+    )
+
+    ParsedVersion = Union[Version, LegacyVersion]
+    UnparsedVersion = Union[Version, LegacyVersion, str]
+    CallableOperator = Callable[[ParsedVersion, str], bool]
+
 
 class InvalidSpecifier(ValueError):
     """
@@ -18,9 +37,10 @@
     """
 
 
-class BaseSpecifier(with_metaclass(abc.ABCMeta, object)):
+class BaseSpecifier(with_metaclass(abc.ABCMeta, object)):  # type: ignore
     @abc.abstractmethod
     def __str__(self):
+        # type: () -> str
         """
         Returns the str representation of this Specifier like object. This
         should be representative of the Specifier itself.
@@ -28,12 +48,14 @@
 
     @abc.abstractmethod
     def __hash__(self):
+        # type: () -> int
         """
         Returns a hash value for this Specifier like object.
         """
 
     @abc.abstractmethod
     def __eq__(self, other):
+        # type: (object) -> bool
         """
         Returns a boolean representing whether or not the two Specifier like
         objects are equal.
@@ -41,6 +63,7 @@
 
     @abc.abstractmethod
     def __ne__(self, other):
+        # type: (object) -> bool
         """
         Returns a boolean representing whether or not the two Specifier like
         objects are not equal.
@@ -48,6 +71,7 @@
 
     @abc.abstractproperty
     def prereleases(self):
+        # type: () -> Optional[bool]
         """
         Returns whether or not pre-releases as a whole are allowed by this
         specifier.
@@ -55,6 +79,7 @@
 
     @prereleases.setter
     def prereleases(self, value):
+        # type: (bool) -> None
         """
         Sets whether or not pre-releases as a whole are allowed by this
         specifier.
@@ -62,12 +87,14 @@
 
     @abc.abstractmethod
     def contains(self, item, prereleases=None):
+        # type: (str, Optional[bool]) -> bool
         """
         Determines if the given item is contained within this specifier.
         """
 
     @abc.abstractmethod
     def filter(self, iterable, prereleases=None):
+        # type: (Iterable[UnparsedVersion], Optional[bool]) -> Iterable[UnparsedVersion]
         """
         Takes an iterable of items and filters them so that only items which
         are contained within this specifier are allowed in it.
@@ -76,19 +103,24 @@
 
 class _IndividualSpecifier(BaseSpecifier):
 
-    _operators = {}
+    _operators = {}  # type: Dict[str, str]
 
     def __init__(self, spec="", prereleases=None):
+        # type: (str, Optional[bool]) -> None
         match = self._regex.search(spec)
         if not match:
             raise InvalidSpecifier("Invalid specifier: '{0}'".format(spec))
 
-        self._spec = (match.group("operator").strip(), match.group("version").strip())
+        self._spec = (
+            match.group("operator").strip(),
+            match.group("version").strip(),
+        )  # type: Tuple[str, str]
 
         # Store whether or not this Specifier should accept prereleases
         self._prereleases = prereleases
 
     def __repr__(self):
+        # type: () -> str
         pre = (
             ", prereleases={0!r}".format(self.prereleases)
             if self._prereleases is not None
@@ -98,26 +130,35 @@
         return "<{0}({1!r}{2})>".format(self.__class__.__name__, str(self), pre)
 
     def __str__(self):
+        # type: () -> str
         return "{0}{1}".format(*self._spec)
 
+    @property
+    def _canonical_spec(self):
+        # type: () -> Tuple[str, Union[Version, str]]
+        return self._spec[0], canonicalize_version(self._spec[1])
+
     def __hash__(self):
-        return hash(self._spec)
+        # type: () -> int
+        return hash(self._canonical_spec)
 
     def __eq__(self, other):
+        # type: (object) -> bool
         if isinstance(other, string_types):
             try:
-                other = self.__class__(other)
+                other = self.__class__(str(other))
             except InvalidSpecifier:
                 return NotImplemented
         elif not isinstance(other, self.__class__):
             return NotImplemented
 
-        return self._spec == other._spec
+        return self._canonical_spec == other._canonical_spec
 
     def __ne__(self, other):
+        # type: (object) -> bool
         if isinstance(other, string_types):
             try:
-                other = self.__class__(other)
+                other = self.__class__(str(other))
             except InvalidSpecifier:
                 return NotImplemented
         elif not isinstance(other, self.__class__):
@@ -126,52 +167,67 @@
         return self._spec != other._spec
 
     def _get_operator(self, op):
-        return getattr(self, "_compare_{0}".format(self._operators[op]))
+        # type: (str) -> CallableOperator
+        operator_callable = getattr(
+            self, "_compare_{0}".format(self._operators[op])
+        )  # type: CallableOperator
+        return operator_callable
 
     def _coerce_version(self, version):
+        # type: (UnparsedVersion) -> ParsedVersion
         if not isinstance(version, (LegacyVersion, Version)):
             version = parse(version)
         return version
 
     @property
     def operator(self):
+        # type: () -> str
         return self._spec[0]
 
     @property
     def version(self):
+        # type: () -> str
         return self._spec[1]
 
     @property
     def prereleases(self):
+        # type: () -> Optional[bool]
         return self._prereleases
 
     @prereleases.setter
     def prereleases(self, value):
+        # type: (bool) -> None
         self._prereleases = value
 
     def __contains__(self, item):
+        # type: (str) -> bool
         return self.contains(item)
 
     def contains(self, item, prereleases=None):
+        # type: (UnparsedVersion, Optional[bool]) -> bool
+
         # Determine if prereleases are to be allowed or not.
         if prereleases is None:
             prereleases = self.prereleases
 
         # Normalize item to a Version or LegacyVersion, this allows us to have
         # a shortcut for ``"2.0" in Specifier(">=2")
-        item = self._coerce_version(item)
+        normalized_item = self._coerce_version(item)
 
         # Determine if we should be supporting prereleases in this specifier
         # or not, if we do not support prereleases than we can short circuit
         # logic if this version is a prereleases.
-        if item.is_prerelease and not prereleases:
+        if normalized_item.is_prerelease and not prereleases:
             return False
 
         # Actually do the comparison to determine if this item is contained
         # within this Specifier or not.
-        return self._get_operator(self.operator)(item, self.version)
+        operator_callable = self._get_operator(self.operator)  # type: CallableOperator
+        return operator_callable(normalized_item, self.version)
 
     def filter(self, iterable, prereleases=None):
+        # type: (Iterable[UnparsedVersion], Optional[bool]) -> Iterable[UnparsedVersion]
+
         yielded = False
         found_prereleases = []
 
@@ -230,32 +286,43 @@
     }
 
     def _coerce_version(self, version):
+        # type: (Union[ParsedVersion, str]) -> LegacyVersion
         if not isinstance(version, LegacyVersion):
             version = LegacyVersion(str(version))
         return version
 
     def _compare_equal(self, prospective, spec):
+        # type: (LegacyVersion, str) -> bool
         return prospective == self._coerce_version(spec)
 
     def _compare_not_equal(self, prospective, spec):
+        # type: (LegacyVersion, str) -> bool
         return prospective != self._coerce_version(spec)
 
     def _compare_less_than_equal(self, prospective, spec):
+        # type: (LegacyVersion, str) -> bool
         return prospective <= self._coerce_version(spec)
 
     def _compare_greater_than_equal(self, prospective, spec):
+        # type: (LegacyVersion, str) -> bool
         return prospective >= self._coerce_version(spec)
 
     def _compare_less_than(self, prospective, spec):
+        # type: (LegacyVersion, str) -> bool
         return prospective < self._coerce_version(spec)
 
     def _compare_greater_than(self, prospective, spec):
+        # type: (LegacyVersion, str) -> bool
         return prospective > self._coerce_version(spec)
 
 
-def _require_version_compare(fn):
+def _require_version_compare(
+    fn  # type: (Callable[[Specifier, ParsedVersion, str], bool])
+):
+    # type: (...) -> Callable[[Specifier, ParsedVersion, str], bool]
     @functools.wraps(fn)
     def wrapped(self, prospective, spec):
+        # type: (Specifier, ParsedVersion, str) -> bool
         if not isinstance(prospective, Version):
             return False
         return fn(self, prospective, spec)
@@ -373,6 +440,8 @@
 
     @_require_version_compare
     def _compare_compatible(self, prospective, spec):
+        # type: (ParsedVersion, str) -> bool
+
         # Compatible releases have an equivalent combination of >= and ==. That
         # is that ~=2.2 is equivalent to >=2.2,==2.*. This allows us to
         # implement this in terms of the other specifiers instead of
@@ -400,56 +469,75 @@
 
     @_require_version_compare
     def _compare_equal(self, prospective, spec):
+        # type: (ParsedVersion, str) -> bool
+
         # We need special logic to handle prefix matching
         if spec.endswith(".*"):
             # In the case of prefix matching we want to ignore local segment.
             prospective = Version(prospective.public)
             # Split the spec out by dots, and pretend that there is an implicit
             # dot in between a release segment and a pre-release segment.
-            spec = _version_split(spec[:-2])  # Remove the trailing .*
+            split_spec = _version_split(spec[:-2])  # Remove the trailing .*
 
             # Split the prospective version out by dots, and pretend that there
             # is an implicit dot in between a release segment and a pre-release
             # segment.
-            prospective = _version_split(str(prospective))
+            split_prospective = _version_split(str(prospective))
 
             # Shorten the prospective version to be the same length as the spec
             # so that we can determine if the specifier is a prefix of the
             # prospective version or not.
-            prospective = prospective[: len(spec)]
+            shortened_prospective = split_prospective[: len(split_spec)]
 
             # Pad out our two sides with zeros so that they both equal the same
             # length.
-            spec, prospective = _pad_version(spec, prospective)
+            padded_spec, padded_prospective = _pad_version(
+                split_spec, shortened_prospective
+            )
+
+            return padded_prospective == padded_spec
         else:
             # Convert our spec string into a Version
-            spec = Version(spec)
+            spec_version = Version(spec)
 
             # If the specifier does not have a local segment, then we want to
             # act as if the prospective version also does not have a local
             # segment.
-            if not spec.local:
+            if not spec_version.local:
                 prospective = Version(prospective.public)
 
-        return prospective == spec
+            return prospective == spec_version
 
     @_require_version_compare
     def _compare_not_equal(self, prospective, spec):
+        # type: (ParsedVersion, str) -> bool
         return not self._compare_equal(prospective, spec)
 
     @_require_version_compare
     def _compare_less_than_equal(self, prospective, spec):
-        return prospective <= Version(spec)
+        # type: (ParsedVersion, str) -> bool
+
+        # NB: Local version identifiers are NOT permitted in the version
+        # specifier, so local version labels can be universally removed from
+        # the prospective version.
+        return Version(prospective.public) <= Version(spec)
 
     @_require_version_compare
     def _compare_greater_than_equal(self, prospective, spec):
-        return prospective >= Version(spec)
+        # type: (ParsedVersion, str) -> bool
+
+        # NB: Local version identifiers are NOT permitted in the version
+        # specifier, so local version labels can be universally removed from
+        # the prospective version.
+        return Version(prospective.public) >= Version(spec)
 
     @_require_version_compare
-    def _compare_less_than(self, prospective, spec):
+    def _compare_less_than(self, prospective, spec_str):
+        # type: (ParsedVersion, str) -> bool
+
         # Convert our spec to a Version instance, since we'll want to work with
         # it as a version.
-        spec = Version(spec)
+        spec = Version(spec_str)
 
         # Check to see if the prospective version is less than the spec
         # version. If it's not we can short circuit and just return False now
@@ -471,10 +559,12 @@
         return True
 
     @_require_version_compare
-    def _compare_greater_than(self, prospective, spec):
+    def _compare_greater_than(self, prospective, spec_str):
+        # type: (ParsedVersion, str) -> bool
+
         # Convert our spec to a Version instance, since we'll want to work with
         # it as a version.
-        spec = Version(spec)
+        spec = Version(spec_str)
 
         # Check to see if the prospective version is greater than the spec
         # version. If it's not we can short circuit and just return False now
@@ -502,10 +592,13 @@
         return True
 
     def _compare_arbitrary(self, prospective, spec):
+        # type: (Version, str) -> bool
         return str(prospective).lower() == str(spec).lower()
 
     @property
     def prereleases(self):
+        # type: () -> bool
+
         # If there is an explicit prereleases set for this, then we'll just
         # blindly use that.
         if self._prereleases is not None:
@@ -530,6 +623,7 @@
 
     @prereleases.setter
     def prereleases(self, value):
+        # type: (bool) -> None
         self._prereleases = value
 
 
@@ -537,7 +631,8 @@
 
 
 def _version_split(version):
-    result = []
+    # type: (str) -> List[str]
+    result = []  # type: List[str]
     for item in version.split("."):
         match = _prefix_regex.search(item)
         if match:
@@ -548,6 +643,7 @@
 
 
 def _pad_version(left, right):
+    # type: (List[str], List[str]) -> Tuple[List[str], List[str]]
     left_split, right_split = [], []
 
     # Get the release segment of our versions
@@ -567,14 +663,16 @@
 
 class SpecifierSet(BaseSpecifier):
     def __init__(self, specifiers="", prereleases=None):
-        # Split on , to break each indidivual specifier into it's own item, and
+        # type: (str, Optional[bool]) -> None
+
+        # Split on , to break each individual specifier into it's own item, and
         # strip each item to remove leading/trailing whitespace.
-        specifiers = [s.strip() for s in specifiers.split(",") if s.strip()]
+        split_specifiers = [s.strip() for s in specifiers.split(",") if s.strip()]
 
         # Parsed each individual specifier, attempting first to make it a
         # Specifier and falling back to a LegacySpecifier.
         parsed = set()
-        for specifier in specifiers:
+        for specifier in split_specifiers:
             try:
                 parsed.add(Specifier(specifier))
             except InvalidSpecifier:
@@ -588,6 +686,7 @@
         self._prereleases = prereleases
 
     def __repr__(self):
+        # type: () -> str
         pre = (
             ", prereleases={0!r}".format(self.prereleases)
             if self._prereleases is not None
@@ -597,12 +696,15 @@
         return "<SpecifierSet({0!r}{1})>".format(str(self), pre)
 
     def __str__(self):
+        # type: () -> str
         return ",".join(sorted(str(s) for s in self._specs))
 
     def __hash__(self):
+        # type: () -> int
         return hash(self._specs)
 
     def __and__(self, other):
+        # type: (Union[SpecifierSet, str]) -> SpecifierSet
         if isinstance(other, string_types):
             other = SpecifierSet(other)
         elif not isinstance(other, SpecifierSet):
@@ -626,9 +728,8 @@
         return specifier
 
     def __eq__(self, other):
-        if isinstance(other, string_types):
-            other = SpecifierSet(other)
-        elif isinstance(other, _IndividualSpecifier):
+        # type: (object) -> bool
+        if isinstance(other, (string_types, _IndividualSpecifier)):
             other = SpecifierSet(str(other))
         elif not isinstance(other, SpecifierSet):
             return NotImplemented
@@ -636,9 +737,8 @@
         return self._specs == other._specs
 
     def __ne__(self, other):
-        if isinstance(other, string_types):
-            other = SpecifierSet(other)
-        elif isinstance(other, _IndividualSpecifier):
+        # type: (object) -> bool
+        if isinstance(other, (string_types, _IndividualSpecifier)):
             other = SpecifierSet(str(other))
         elif not isinstance(other, SpecifierSet):
             return NotImplemented
@@ -646,13 +746,17 @@
         return self._specs != other._specs
 
     def __len__(self):
+        # type: () -> int
         return len(self._specs)
 
     def __iter__(self):
+        # type: () -> Iterator[FrozenSet[_IndividualSpecifier]]
         return iter(self._specs)
 
     @property
     def prereleases(self):
+        # type: () -> Optional[bool]
+
         # If we have been given an explicit prerelease modifier, then we'll
         # pass that through here.
         if self._prereleases is not None:
@@ -670,12 +774,16 @@
 
     @prereleases.setter
     def prereleases(self, value):
+        # type: (bool) -> None
         self._prereleases = value
 
     def __contains__(self, item):
+        # type: (Union[ParsedVersion, str]) -> bool
         return self.contains(item)
 
     def contains(self, item, prereleases=None):
+        # type: (Union[ParsedVersion, str], Optional[bool]) -> bool
+
         # Ensure that our item is a Version or LegacyVersion instance.
         if not isinstance(item, (LegacyVersion, Version)):
             item = parse(item)
@@ -701,7 +809,13 @@
         #       will always return True, this is an explicit design decision.
         return all(s.contains(item, prereleases=prereleases) for s in self._specs)
 
-    def filter(self, iterable, prereleases=None):
+    def filter(
+        self,
+        iterable,  # type: Iterable[Union[ParsedVersion, str]]
+        prereleases=None,  # type: Optional[bool]
+    ):
+        # type: (...) -> Iterable[Union[ParsedVersion, str]]
+
         # Determine if we're forcing a prerelease or not, if we're not forcing
         # one for this particular filter call, then we'll use whatever the
         # SpecifierSet thinks for whether or not we should support prereleases.
@@ -719,8 +833,8 @@
         # which will filter out any pre-releases, unless there are no final
         # releases, and which will filter out LegacyVersion in general.
         else:
-            filtered = []
-            found_prereleases = []
+            filtered = []  # type: List[Union[ParsedVersion, str]]
+            found_prereleases = []  # type: List[Union[ParsedVersion, str]]
 
             for item in iterable:
                 # Ensure that we some kind of Version class for this item.
Index: env/lib/python3.8/site-packages/pip/_internal/network/lazy_wheel.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"Lazy ZIP over HTTP\"\"\"\n\n__all__ = ['HTTPRangeRequestUnsupported', 'dist_from_wheel_url']\n\nfrom bisect import bisect_left, bisect_right\nfrom contextlib import contextmanager\nfrom tempfile import NamedTemporaryFile\nfrom zipfile import BadZipfile, ZipFile\n\nfrom pip._vendor.requests.models import CONTENT_CHUNK_SIZE\nfrom pip._vendor.six.moves import range\n\nfrom pip._internal.network.utils import (\n    HEADERS,\n    raise_for_status,\n    response_chunks,\n)\nfrom pip._internal.utils.typing import MYPY_CHECK_RUNNING\nfrom pip._internal.utils.wheel import pkg_resources_distribution_for_wheel\n\nif MYPY_CHECK_RUNNING:\n    from typing import Any, Dict, Iterator, List, Optional, Tuple\n\n    from pip._vendor.pkg_resources import Distribution\n    from pip._vendor.requests.models import Response\n\n    from pip._internal.network.session import PipSession\n\n\nclass HTTPRangeRequestUnsupported(Exception):\n    pass\n\n\ndef dist_from_wheel_url(name, url, session):\n    # type: (str, str, PipSession) -> Distribution\n    \"\"\"Return a pkg_resources.Distribution from the given wheel URL.\n\n    This uses HTTP range requests to only fetch the potion of the wheel\n    containing metadata, just enough for the object to be constructed.\n    If such requests are not supported, HTTPRangeRequestUnsupported\n    is raised.\n    \"\"\"\n    with LazyZipOverHTTP(url, session) as wheel:\n        # For read-only ZIP files, ZipFile only needs methods read,\n        # seek, seekable and tell, not the whole IO protocol.\n        zip_file = ZipFile(wheel)  # type: ignore\n        # After context manager exit, wheel.name\n        # is an invalid file by intention.\n        return pkg_resources_distribution_for_wheel(zip_file, name, wheel.name)\n\n\nclass LazyZipOverHTTP(object):\n    \"\"\"File-like object mapped to a ZIP file over HTTP.\n\n    This uses HTTP range requests to lazily fetch the file's content,\n    which is supposed to be fed to ZipFile.  If such requests are not\n    supported by the server, raise HTTPRangeRequestUnsupported\n    during initialization.\n    \"\"\"\n\n    def __init__(self, url, session, chunk_size=CONTENT_CHUNK_SIZE):\n        # type: (str, PipSession, int) -> None\n        head = session.head(url, headers=HEADERS)\n        raise_for_status(head)\n        assert head.status_code == 200\n        self._session, self._url, self._chunk_size = session, url, chunk_size\n        self._length = int(head.headers['Content-Length'])\n        self._file = NamedTemporaryFile()\n        self.truncate(self._length)\n        self._left = []  # type: List[int]\n        self._right = []  # type: List[int]\n        if 'bytes' not in head.headers.get('Accept-Ranges', 'none'):\n            raise HTTPRangeRequestUnsupported('range request is not supported')\n        self._check_zip()\n\n    @property\n    def mode(self):\n        # type: () -> str\n        \"\"\"Opening mode, which is always rb.\"\"\"\n        return 'rb'\n\n    @property\n    def name(self):\n        # type: () -> str\n        \"\"\"Path to the underlying file.\"\"\"\n        return self._file.name\n\n    def seekable(self):\n        # type: () -> bool\n        \"\"\"Return whether random access is supported, which is True.\"\"\"\n        return True\n\n    def close(self):\n        # type: () -> None\n        \"\"\"Close the file.\"\"\"\n        self._file.close()\n\n    @property\n    def closed(self):\n        # type: () -> bool\n        \"\"\"Whether the file is closed.\"\"\"\n        return self._file.closed\n\n    def read(self, size=-1):\n        # type: (int) -> bytes\n        \"\"\"Read up to size bytes from the object and return them.\n\n        As a convenience, if size is unspecified or -1,\n        all bytes until EOF are returned.  Fewer than\n        size bytes may be returned if EOF is reached.\n        \"\"\"\n        download_size = max(size, self._chunk_size)\n        start, length = self.tell(), self._length\n        stop = length if size < 0 else min(start+download_size, length)\n        start = max(0, stop-download_size)\n        self._download(start, stop-1)\n        return self._file.read(size)\n\n    def readable(self):\n        # type: () -> bool\n        \"\"\"Return whether the file is readable, which is True.\"\"\"\n        return True\n\n    def seek(self, offset, whence=0):\n        # type: (int, int) -> int\n        \"\"\"Change stream position and return the new absolute position.\n\n        Seek to offset relative position indicated by whence:\n        * 0: Start of stream (the default).  pos should be >= 0;\n        * 1: Current position - pos may be negative;\n        * 2: End of stream - pos usually negative.\n        \"\"\"\n        return self._file.seek(offset, whence)\n\n    def tell(self):\n        # type: () -> int\n        \"\"\"Return the current possition.\"\"\"\n        return self._file.tell()\n\n    def truncate(self, size=None):\n        # type: (Optional[int]) -> int\n        \"\"\"Resize the stream to the given size in bytes.\n\n        If size is unspecified resize to the current position.\n        The current stream position isn't changed.\n\n        Return the new file size.\n        \"\"\"\n        return self._file.truncate(size)\n\n    def writable(self):\n        # type: () -> bool\n        \"\"\"Return False.\"\"\"\n        return False\n\n    def __enter__(self):\n        # type: () -> LazyZipOverHTTP\n        self._file.__enter__()\n        return self\n\n    def __exit__(self, *exc):\n        # type: (*Any) -> Optional[bool]\n        return self._file.__exit__(*exc)\n\n    @contextmanager\n    def _stay(self):\n        # type: ()-> Iterator[None]\n        \"\"\"Return a context manager keeping the position.\n\n        At the end of the block, seek back to original position.\n        \"\"\"\n        pos = self.tell()\n        try:\n            yield\n        finally:\n            self.seek(pos)\n\n    def _check_zip(self):\n        # type: () -> None\n        \"\"\"Check and download until the file is a valid ZIP.\"\"\"\n        end = self._length - 1\n        for start in reversed(range(0, end, self._chunk_size)):\n            self._download(start, end)\n            with self._stay():\n                try:\n                    # For read-only ZIP files, ZipFile only needs\n                    # methods read, seek, seekable and tell.\n                    ZipFile(self)  # type: ignore\n                except BadZipfile:\n                    pass\n                else:\n                    break\n\n    def _stream_response(self, start, end, base_headers=HEADERS):\n        # type: (int, int, Dict[str, str]) -> Response\n        \"\"\"Return HTTP response to a range request from start to end.\"\"\"\n        headers = {'Range': 'bytes={}-{}'.format(start, end)}\n        headers.update(base_headers)\n        return self._session.get(self._url, headers=headers, stream=True)\n\n    def _merge(self, start, end, left, right):\n        # type: (int, int, int, int) -> Iterator[Tuple[int, int]]\n        \"\"\"Return an iterator of intervals to be fetched.\n\n        Args:\n            start (int): Start of needed interval\n            end (int): End of needed interval\n            left (int): Index of first overlapping downloaded data\n            right (int): Index after last overlapping downloaded data\n        \"\"\"\n        lslice, rslice = self._left[left:right], self._right[left:right]\n        i = start = min([start]+lslice[:1])\n        end = max([end]+rslice[-1:])\n        for j, k in zip(lslice, rslice):\n            if j > i:\n                yield i, j-1\n            i = k + 1\n        if i <= end:\n            yield i, end\n        self._left[left:right], self._right[left:right] = [start], [end]\n\n    def _download(self, start, end):\n        # type: (int, int) -> None\n        \"\"\"Download bytes from start to end inclusively.\"\"\"\n        with self._stay():\n            left = bisect_left(self._right, start)\n            right = bisect_right(self._left, end)\n            for start, end in self._merge(start, end, left, right):\n                response = self._stream_response(start, end)\n                response.raise_for_status()\n                self.seek(start)\n                for chunk in response_chunks(response, self._chunk_size):\n                    self._file.write(chunk)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pip/_internal/network/lazy_wheel.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pip/_internal/network/lazy_wheel.py	(date 1602088700397)
@@ -194,8 +194,10 @@
     def _stream_response(self, start, end, base_headers=HEADERS):
         # type: (int, int, Dict[str, str]) -> Response
         """Return HTTP response to a range request from start to end."""
-        headers = {'Range': 'bytes={}-{}'.format(start, end)}
-        headers.update(base_headers)
+        headers = base_headers.copy()
+        headers['Range'] = 'bytes={}-{}'.format(start, end)
+        # TODO: Get range requests to be correctly cached
+        headers['Cache-Control'] = 'no-cache'
         return self._session.get(self._url, headers=headers, stream=True)
 
     def _merge(self, start, end, left, right):
Index: env/lib/python3.8/site-packages/pip/_internal/network/auth.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"Network Authentication Helpers\n\nContains interface (MultiDomainBasicAuth) and associated glue code for\nproviding credentials in the context of network requests.\n\"\"\"\n\nimport logging\n\nfrom pip._vendor.requests.auth import AuthBase, HTTPBasicAuth\nfrom pip._vendor.requests.utils import get_netrc_auth\nfrom pip._vendor.six.moves.urllib import parse as urllib_parse\n\nfrom pip._internal.utils.misc import (\n    ask,\n    ask_input,\n    ask_password,\n    remove_auth_from_url,\n    split_auth_netloc_from_url,\n)\nfrom pip._internal.utils.typing import MYPY_CHECK_RUNNING\n\nif MYPY_CHECK_RUNNING:\n    from typing import Dict, Optional, Tuple, List, Any\n\n    from pip._internal.vcs.versioncontrol import AuthInfo\n\n    from pip._vendor.requests.models import Response, Request\n\n    Credentials = Tuple[str, str, str]\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import keyring  # noqa\nexcept ImportError:\n    keyring = None\nexcept Exception as exc:\n    logger.warning(\n        \"Keyring is skipped due to an exception: %s\", str(exc),\n    )\n    keyring = None\n\n\ndef get_keyring_auth(url, username):\n    # type: (str, str) -> Optional[AuthInfo]\n    \"\"\"Return the tuple auth for a given url from keyring.\"\"\"\n    if not url or not keyring:\n        return None\n\n    try:\n        try:\n            get_credential = keyring.get_credential\n        except AttributeError:\n            pass\n        else:\n            logger.debug(\"Getting credentials from keyring for %s\", url)\n            cred = get_credential(url, username)\n            if cred is not None:\n                return cred.username, cred.password\n            return None\n\n        if username:\n            logger.debug(\"Getting password from keyring for %s\", url)\n            password = keyring.get_password(url, username)\n            if password:\n                return username, password\n\n    except Exception as exc:\n        logger.warning(\n            \"Keyring is skipped due to an exception: %s\", str(exc),\n        )\n    return None\n\n\nclass MultiDomainBasicAuth(AuthBase):\n\n    def __init__(self, prompting=True, index_urls=None):\n        # type: (bool, Optional[List[str]]) -> None\n        self.prompting = prompting\n        self.index_urls = index_urls\n        self.passwords = {}  # type: Dict[str, AuthInfo]\n        # When the user is prompted to enter credentials and keyring is\n        # available, we will offer to save them. If the user accepts,\n        # this value is set to the credentials they entered. After the\n        # request authenticates, the caller should call\n        # ``save_credentials`` to save these.\n        self._credentials_to_save = None  # type: Optional[Credentials]\n\n    def _get_index_url(self, url):\n        # type: (str) -> Optional[str]\n        \"\"\"Return the original index URL matching the requested URL.\n\n        Cached or dynamically generated credentials may work against\n        the original index URL rather than just the netloc.\n\n        The provided url should have had its username and password\n        removed already. If the original index url had credentials then\n        they will be included in the return value.\n\n        Returns None if no matching index was found, or if --no-index\n        was specified by the user.\n        \"\"\"\n        if not url or not self.index_urls:\n            return None\n\n        for u in self.index_urls:\n            prefix = remove_auth_from_url(u).rstrip(\"/\") + \"/\"\n            if url.startswith(prefix):\n                return u\n        return None\n\n    def _get_new_credentials(self, original_url, allow_netrc=True,\n                             allow_keyring=True):\n        # type: (str, bool, bool) -> AuthInfo\n        \"\"\"Find and return credentials for the specified URL.\"\"\"\n        # Split the credentials and netloc from the url.\n        url, netloc, url_user_password = split_auth_netloc_from_url(\n            original_url,\n        )\n\n        # Start with the credentials embedded in the url\n        username, password = url_user_password\n        if username is not None and password is not None:\n            logger.debug(\"Found credentials in url for %s\", netloc)\n            return url_user_password\n\n        # Find a matching index url for this request\n        index_url = self._get_index_url(url)\n        if index_url:\n            # Split the credentials from the url.\n            index_info = split_auth_netloc_from_url(index_url)\n            if index_info:\n                index_url, _, index_url_user_password = index_info\n                logger.debug(\"Found index url %s\", index_url)\n\n        # If an index URL was found, try its embedded credentials\n        if index_url and index_url_user_password[0] is not None:\n            username, password = index_url_user_password\n            if username is not None and password is not None:\n                logger.debug(\"Found credentials in index url for %s\", netloc)\n                return index_url_user_password\n\n        # Get creds from netrc if we still don't have them\n        if allow_netrc:\n            netrc_auth = get_netrc_auth(original_url)\n            if netrc_auth:\n                logger.debug(\"Found credentials in netrc for %s\", netloc)\n                return netrc_auth\n\n        # If we don't have a password and keyring is available, use it.\n        if allow_keyring:\n            # The index url is more specific than the netloc, so try it first\n            kr_auth = (\n                get_keyring_auth(index_url, username) or\n                get_keyring_auth(netloc, username)\n            )\n            if kr_auth:\n                logger.debug(\"Found credentials in keyring for %s\", netloc)\n                return kr_auth\n\n        return username, password\n\n    def _get_url_and_credentials(self, original_url):\n        # type: (str) -> Tuple[str, Optional[str], Optional[str]]\n        \"\"\"Return the credentials to use for the provided URL.\n\n        If allowed, netrc and keyring may be used to obtain the\n        correct credentials.\n\n        Returns (url_without_credentials, username, password). Note\n        that even if the original URL contains credentials, this\n        function may return a different username and password.\n        \"\"\"\n        url, netloc, _ = split_auth_netloc_from_url(original_url)\n\n        # Use any stored credentials that we have for this netloc\n        username, password = self.passwords.get(netloc, (None, None))\n\n        if username is None and password is None:\n            # No stored credentials. Acquire new credentials without prompting\n            # the user. (e.g. from netrc, keyring, or the URL itself)\n            username, password = self._get_new_credentials(original_url)\n\n        if username is not None or password is not None:\n            # Convert the username and password if they're None, so that\n            # this netloc will show up as \"cached\" in the conditional above.\n            # Further, HTTPBasicAuth doesn't accept None, so it makes sense to\n            # cache the value that is going to be used.\n            username = username or \"\"\n            password = password or \"\"\n\n            # Store any acquired credentials.\n            self.passwords[netloc] = (username, password)\n\n        assert (\n            # Credentials were found\n            (username is not None and password is not None) or\n            # Credentials were not found\n            (username is None and password is None)\n        ), \"Could not load credentials from url: {}\".format(original_url)\n\n        return url, username, password\n\n    def __call__(self, req):\n        # type: (Request) -> Request\n        # Get credentials for this request\n        url, username, password = self._get_url_and_credentials(req.url)\n\n        # Set the url of the request to the url without any credentials\n        req.url = url\n\n        if username is not None and password is not None:\n            # Send the basic auth with this request\n            req = HTTPBasicAuth(username, password)(req)\n\n        # Attach a hook to handle 401 responses\n        req.register_hook(\"response\", self.handle_401)\n\n        return req\n\n    # Factored out to allow for easy patching in tests\n    def _prompt_for_password(self, netloc):\n        # type: (str) -> Tuple[Optional[str], Optional[str], bool]\n        username = ask_input(\"User for {}: \".format(netloc))\n        if not username:\n            return None, None, False\n        auth = get_keyring_auth(netloc, username)\n        if auth and auth[0] is not None and auth[1] is not None:\n            return auth[0], auth[1], False\n        password = ask_password(\"Password: \")\n        return username, password, True\n\n    # Factored out to allow for easy patching in tests\n    def _should_save_password_to_keyring(self):\n        # type: () -> bool\n        if not keyring:\n            return False\n        return ask(\"Save credentials to keyring [y/N]: \", [\"y\", \"n\"]) == \"y\"\n\n    def handle_401(self, resp, **kwargs):\n        # type: (Response, **Any) -> Response\n        # We only care about 401 responses, anything else we want to just\n        #   pass through the actual response\n        if resp.status_code != 401:\n            return resp\n\n        # We are not able to prompt the user so simply return the response\n        if not self.prompting:\n            return resp\n\n        parsed = urllib_parse.urlparse(resp.url)\n\n        # Prompt the user for a new username and password\n        username, password, save = self._prompt_for_password(parsed.netloc)\n\n        # Store the new username and password to use for future requests\n        self._credentials_to_save = None\n        if username is not None and password is not None:\n            self.passwords[parsed.netloc] = (username, password)\n\n            # Prompt to save the password to keyring\n            if save and self._should_save_password_to_keyring():\n                self._credentials_to_save = (parsed.netloc, username, password)\n\n        # Consume content and release the original connection to allow our new\n        #   request to reuse the same one.\n        resp.content\n        resp.raw.release_conn()\n\n        # Add our new username and password to the request\n        req = HTTPBasicAuth(username or \"\", password or \"\")(resp.request)\n        req.register_hook(\"response\", self.warn_on_401)\n\n        # On successful request, save the credentials that were used to\n        # keyring. (Note that if the user responded \"no\" above, this member\n        # is not set and nothing will be saved.)\n        if self._credentials_to_save:\n            req.register_hook(\"response\", self.save_credentials)\n\n        # Send our new request\n        new_resp = resp.connection.send(req, **kwargs)\n        new_resp.history.append(resp)\n\n        return new_resp\n\n    def warn_on_401(self, resp, **kwargs):\n        # type: (Response, **Any) -> None\n        \"\"\"Response callback to warn about incorrect credentials.\"\"\"\n        if resp.status_code == 401:\n            logger.warning(\n                '401 Error, Credentials not correct for %s', resp.request.url,\n            )\n\n    def save_credentials(self, resp, **kwargs):\n        # type: (Response, **Any) -> None\n        \"\"\"Response callback to save credentials on success.\"\"\"\n        assert keyring is not None, \"should never reach here without keyring\"\n        if not keyring:\n            return\n\n        creds = self._credentials_to_save\n        self._credentials_to_save = None\n        if creds and resp.status_code < 400:\n            try:\n                logger.info('Saving credentials to keyring')\n                keyring.set_password(*creds)\n            except Exception:\n                logger.exception('Failed to save credentials')\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pip/_internal/network/auth.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pip/_internal/network/auth.py	(date 1602088700397)
@@ -44,6 +44,7 @@
 def get_keyring_auth(url, username):
     # type: (str, str) -> Optional[AuthInfo]
     """Return the tuple auth for a given url from keyring."""
+    global keyring
     if not url or not keyring:
         return None
 
@@ -69,6 +70,7 @@
         logger.warning(
             "Keyring is skipped due to an exception: %s", str(exc),
         )
+        keyring = None
     return None
 
 
Index: env/lib/python3.8/site-packages/pkg_resources/extern/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import sys\n\n\nclass VendorImporter:\n    \"\"\"\n    A PEP 302 meta path importer for finding optionally-vendored\n    or otherwise naturally-installed packages from root_name.\n    \"\"\"\n\n    def __init__(self, root_name, vendored_names=(), vendor_pkg=None):\n        self.root_name = root_name\n        self.vendored_names = set(vendored_names)\n        self.vendor_pkg = vendor_pkg or root_name.replace('extern', '_vendor')\n\n    @property\n    def search_path(self):\n        \"\"\"\n        Search first the vendor package then as a natural package.\n        \"\"\"\n        yield self.vendor_pkg + '.'\n        yield ''\n\n    def find_module(self, fullname, path=None):\n        \"\"\"\n        Return self when fullname starts with root_name and the\n        target module is one vendored through this importer.\n        \"\"\"\n        root, base, target = fullname.partition(self.root_name + '.')\n        if root:\n            return\n        if not any(map(target.startswith, self.vendored_names)):\n            return\n        return self\n\n    def load_module(self, fullname):\n        \"\"\"\n        Iterate over the search path to locate and load fullname.\n        \"\"\"\n        root, base, target = fullname.partition(self.root_name + '.')\n        for prefix in self.search_path:\n            try:\n                extant = prefix + target\n                __import__(extant)\n                mod = sys.modules[extant]\n                sys.modules[fullname] = mod\n                return mod\n            except ImportError:\n                pass\n        else:\n            raise ImportError(\n                \"The '{target}' package is required; \"\n                \"normally this is bundled with this package so if you get \"\n                \"this warning, consult the packager of your \"\n                \"distribution.\".format(**locals())\n            )\n\n    def install(self):\n        \"\"\"\n        Install this importer into sys.meta_path if not already present.\n        \"\"\"\n        if self not in sys.meta_path:\n            sys.meta_path.append(self)\n\n\nnames = 'packaging', 'pyparsing', 'six', 'appdirs'\nVendorImporter(__name__, names).install()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pkg_resources/extern/__init__.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pkg_resources/extern/__init__.py	(date 1602088701593)
@@ -62,5 +62,5 @@
             sys.meta_path.append(self)
 
 
-names = 'packaging', 'pyparsing', 'six', 'appdirs'
+names = 'packaging', 'pyparsing', 'appdirs'
 VendorImporter(__name__, names).install()
Index: env/lib/python3.8/site-packages/pkg_resources/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># coding: utf-8\n\"\"\"\nPackage resource API\n--------------------\n\nA resource is a logical file contained within a package, or a logical\nsubdirectory thereof.  The package resource API expects resource names\nto have their path parts separated with ``/``, *not* whatever the local\npath separator is.  Do not use os.path operations to manipulate resource\nnames being passed into the API.\n\nThe package resource API is designed to work with normal filesystem packages,\n.egg files, and unpacked .egg files.  It can also work in a limited way with\n.zip files and with custom PEP 302 loaders that support the ``get_data()``\nmethod.\n\"\"\"\n\nfrom __future__ import absolute_import\n\nimport sys\nimport os\nimport io\nimport time\nimport re\nimport types\nimport zipfile\nimport zipimport\nimport warnings\nimport stat\nimport functools\nimport pkgutil\nimport operator\nimport platform\nimport collections\nimport plistlib\nimport email.parser\nimport errno\nimport tempfile\nimport textwrap\nimport itertools\nimport inspect\nimport ntpath\nimport posixpath\nfrom pkgutil import get_importer\n\ntry:\n    import _imp\nexcept ImportError:\n    # Python 3.2 compatibility\n    import imp as _imp\n\ntry:\n    FileExistsError\nexcept NameError:\n    FileExistsError = OSError\n\nfrom pkg_resources.extern import six\nfrom pkg_resources.extern.six.moves import map, filter\n\n# capture these to bypass sandboxing\nfrom os import utime\ntry:\n    from os import mkdir, rename, unlink\n    WRITE_SUPPORT = True\nexcept ImportError:\n    # no write support, probably under GAE\n    WRITE_SUPPORT = False\n\nfrom os import open as os_open\nfrom os.path import isdir, split\n\ntry:\n    import importlib.machinery as importlib_machinery\n    # access attribute to force import under delayed import mechanisms.\n    importlib_machinery.__name__\nexcept ImportError:\n    importlib_machinery = None\n\nfrom pkg_resources.extern import appdirs\nfrom pkg_resources.extern import packaging\n__import__('pkg_resources.extern.packaging.version')\n__import__('pkg_resources.extern.packaging.specifiers')\n__import__('pkg_resources.extern.packaging.requirements')\n__import__('pkg_resources.extern.packaging.markers')\n\n\n__metaclass__ = type\n\n\nif (3, 0) < sys.version_info < (3, 5):\n    raise RuntimeError(\"Python 3.5 or later is required\")\n\nif six.PY2:\n    # Those builtin exceptions are only defined in Python 3\n    PermissionError = None\n    NotADirectoryError = None\n\n# declare some globals that will be defined later to\n# satisfy the linters.\nrequire = None\nworking_set = None\nadd_activation_listener = None\nresources_stream = None\ncleanup_resources = None\nresource_dir = None\nresource_stream = None\nset_extraction_path = None\nresource_isdir = None\nresource_string = None\niter_entry_points = None\nresource_listdir = None\nresource_filename = None\nresource_exists = None\n_distribution_finders = None\n_namespace_handlers = None\n_namespace_packages = None\n\n\nclass PEP440Warning(RuntimeWarning):\n    \"\"\"\n    Used when there is an issue with a version or specifier not complying with\n    PEP 440.\n    \"\"\"\n\n\ndef parse_version(v):\n    try:\n        return packaging.version.Version(v)\n    except packaging.version.InvalidVersion:\n        return packaging.version.LegacyVersion(v)\n\n\n_state_vars = {}\n\n\ndef _declare_state(vartype, **kw):\n    globals().update(kw)\n    _state_vars.update(dict.fromkeys(kw, vartype))\n\n\ndef __getstate__():\n    state = {}\n    g = globals()\n    for k, v in _state_vars.items():\n        state[k] = g['_sget_' + v](g[k])\n    return state\n\n\ndef __setstate__(state):\n    g = globals()\n    for k, v in state.items():\n        g['_sset_' + _state_vars[k]](k, g[k], v)\n    return state\n\n\ndef _sget_dict(val):\n    return val.copy()\n\n\ndef _sset_dict(key, ob, state):\n    ob.clear()\n    ob.update(state)\n\n\ndef _sget_object(val):\n    return val.__getstate__()\n\n\ndef _sset_object(key, ob, state):\n    ob.__setstate__(state)\n\n\n_sget_none = _sset_none = lambda *args: None\n\n\ndef get_supported_platform():\n    \"\"\"Return this platform's maximum compatible version.\n\n    distutils.util.get_platform() normally reports the minimum version\n    of macOS that would be required to *use* extensions produced by\n    distutils.  But what we want when checking compatibility is to know the\n    version of macOS that we are *running*.  To allow usage of packages that\n    explicitly require a newer version of macOS, we must also know the\n    current version of the OS.\n\n    If this condition occurs for any other platform with a version in its\n    platform strings, this function should be extended accordingly.\n    \"\"\"\n    plat = get_build_platform()\n    m = macosVersionString.match(plat)\n    if m is not None and sys.platform == \"darwin\":\n        try:\n            plat = 'macosx-%s-%s' % ('.'.join(_macos_vers()[:2]), m.group(3))\n        except ValueError:\n            # not macOS\n            pass\n    return plat\n\n\n__all__ = [\n    # Basic resource access and distribution/entry point discovery\n    'require', 'run_script', 'get_provider', 'get_distribution',\n    'load_entry_point', 'get_entry_map', 'get_entry_info',\n    'iter_entry_points',\n    'resource_string', 'resource_stream', 'resource_filename',\n    'resource_listdir', 'resource_exists', 'resource_isdir',\n\n    # Environmental control\n    'declare_namespace', 'working_set', 'add_activation_listener',\n    'find_distributions', 'set_extraction_path', 'cleanup_resources',\n    'get_default_cache',\n\n    # Primary implementation classes\n    'Environment', 'WorkingSet', 'ResourceManager',\n    'Distribution', 'Requirement', 'EntryPoint',\n\n    # Exceptions\n    'ResolutionError', 'VersionConflict', 'DistributionNotFound',\n    'UnknownExtra', 'ExtractionError',\n\n    # Warnings\n    'PEP440Warning',\n\n    # Parsing functions and string utilities\n    'parse_requirements', 'parse_version', 'safe_name', 'safe_version',\n    'get_platform', 'compatible_platforms', 'yield_lines', 'split_sections',\n    'safe_extra', 'to_filename', 'invalid_marker', 'evaluate_marker',\n\n    # filesystem utilities\n    'ensure_directory', 'normalize_path',\n\n    # Distribution \"precedence\" constants\n    'EGG_DIST', 'BINARY_DIST', 'SOURCE_DIST', 'CHECKOUT_DIST', 'DEVELOP_DIST',\n\n    # \"Provider\" interfaces, implementations, and registration/lookup APIs\n    'IMetadataProvider', 'IResourceProvider', 'FileMetadata',\n    'PathMetadata', 'EggMetadata', 'EmptyProvider', 'empty_provider',\n    'NullProvider', 'EggProvider', 'DefaultProvider', 'ZipProvider',\n    'register_finder', 'register_namespace_handler', 'register_loader_type',\n    'fixup_namespace_packages', 'get_importer',\n\n    # Warnings\n    'PkgResourcesDeprecationWarning',\n\n    # Deprecated/backward compatibility only\n    'run_main', 'AvailableDistributions',\n]\n\n\nclass ResolutionError(Exception):\n    \"\"\"Abstract base for dependency resolution errors\"\"\"\n\n    def __repr__(self):\n        return self.__class__.__name__ + repr(self.args)\n\n\nclass VersionConflict(ResolutionError):\n    \"\"\"\n    An already-installed version conflicts with the requested version.\n\n    Should be initialized with the installed Distribution and the requested\n    Requirement.\n    \"\"\"\n\n    _template = \"{self.dist} is installed but {self.req} is required\"\n\n    @property\n    def dist(self):\n        return self.args[0]\n\n    @property\n    def req(self):\n        return self.args[1]\n\n    def report(self):\n        return self._template.format(**locals())\n\n    def with_context(self, required_by):\n        \"\"\"\n        If required_by is non-empty, return a version of self that is a\n        ContextualVersionConflict.\n        \"\"\"\n        if not required_by:\n            return self\n        args = self.args + (required_by,)\n        return ContextualVersionConflict(*args)\n\n\nclass ContextualVersionConflict(VersionConflict):\n    \"\"\"\n    A VersionConflict that accepts a third parameter, the set of the\n    requirements that required the installed Distribution.\n    \"\"\"\n\n    _template = VersionConflict._template + ' by {self.required_by}'\n\n    @property\n    def required_by(self):\n        return self.args[2]\n\n\nclass DistributionNotFound(ResolutionError):\n    \"\"\"A requested distribution was not found\"\"\"\n\n    _template = (\"The '{self.req}' distribution was not found \"\n                 \"and is required by {self.requirers_str}\")\n\n    @property\n    def req(self):\n        return self.args[0]\n\n    @property\n    def requirers(self):\n        return self.args[1]\n\n    @property\n    def requirers_str(self):\n        if not self.requirers:\n            return 'the application'\n        return ', '.join(self.requirers)\n\n    def report(self):\n        return self._template.format(**locals())\n\n    def __str__(self):\n        return self.report()\n\n\nclass UnknownExtra(ResolutionError):\n    \"\"\"Distribution doesn't have an \"extra feature\" of the given name\"\"\"\n\n\n_provider_factories = {}\n\nPY_MAJOR = '{}.{}'.format(*sys.version_info)\nEGG_DIST = 3\nBINARY_DIST = 2\nSOURCE_DIST = 1\nCHECKOUT_DIST = 0\nDEVELOP_DIST = -1\n\n\ndef register_loader_type(loader_type, provider_factory):\n    \"\"\"Register `provider_factory` to make providers for `loader_type`\n\n    `loader_type` is the type or class of a PEP 302 ``module.__loader__``,\n    and `provider_factory` is a function that, passed a *module* object,\n    returns an ``IResourceProvider`` for that module.\n    \"\"\"\n    _provider_factories[loader_type] = provider_factory\n\n\ndef get_provider(moduleOrReq):\n    \"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\n    if isinstance(moduleOrReq, Requirement):\n        return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]\n    try:\n        module = sys.modules[moduleOrReq]\n    except KeyError:\n        __import__(moduleOrReq)\n        module = sys.modules[moduleOrReq]\n    loader = getattr(module, '__loader__', None)\n    return _find_adapter(_provider_factories, loader)(module)\n\n\ndef _macos_vers(_cache=[]):\n    if not _cache:\n        version = platform.mac_ver()[0]\n        # fallback for MacPorts\n        if version == '':\n            plist = '/System/Library/CoreServices/SystemVersion.plist'\n            if os.path.exists(plist):\n                if hasattr(plistlib, 'readPlist'):\n                    plist_content = plistlib.readPlist(plist)\n                    if 'ProductVersion' in plist_content:\n                        version = plist_content['ProductVersion']\n\n        _cache.append(version.split('.'))\n    return _cache[0]\n\n\ndef _macos_arch(machine):\n    return {'PowerPC': 'ppc', 'Power_Macintosh': 'ppc'}.get(machine, machine)\n\n\ndef get_build_platform():\n    \"\"\"Return this platform's string for platform-specific distributions\n\n    XXX Currently this is the same as ``distutils.util.get_platform()``, but it\n    needs some hacks for Linux and macOS.\n    \"\"\"\n    from sysconfig import get_platform\n\n    plat = get_platform()\n    if sys.platform == \"darwin\" and not plat.startswith('macosx-'):\n        try:\n            version = _macos_vers()\n            machine = os.uname()[4].replace(\" \", \"_\")\n            return \"macosx-%d.%d-%s\" % (\n                int(version[0]), int(version[1]),\n                _macos_arch(machine),\n            )\n        except ValueError:\n            # if someone is running a non-Mac darwin system, this will fall\n            # through to the default implementation\n            pass\n    return plat\n\n\nmacosVersionString = re.compile(r\"macosx-(\\d+)\\.(\\d+)-(.*)\")\ndarwinVersionString = re.compile(r\"darwin-(\\d+)\\.(\\d+)\\.(\\d+)-(.*)\")\n# XXX backward compat\nget_platform = get_build_platform\n\n\ndef compatible_platforms(provided, required):\n    \"\"\"Can code for the `provided` platform run on the `required` platform?\n\n    Returns true if either platform is ``None``, or the platforms are equal.\n\n    XXX Needs compatibility checks for Linux and other unixy OSes.\n    \"\"\"\n    if provided is None or required is None or provided == required:\n        # easy case\n        return True\n\n    # macOS special cases\n    reqMac = macosVersionString.match(required)\n    if reqMac:\n        provMac = macosVersionString.match(provided)\n\n        # is this a Mac package?\n        if not provMac:\n            # this is backwards compatibility for packages built before\n            # setuptools 0.6. All packages built after this point will\n            # use the new macOS designation.\n            provDarwin = darwinVersionString.match(provided)\n            if provDarwin:\n                dversion = int(provDarwin.group(1))\n                macosversion = \"%s.%s\" % (reqMac.group(1), reqMac.group(2))\n                if dversion == 7 and macosversion >= \"10.3\" or \\\n                        dversion == 8 and macosversion >= \"10.4\":\n                    return True\n            # egg isn't macOS or legacy darwin\n            return False\n\n        # are they the same major version and machine type?\n        if provMac.group(1) != reqMac.group(1) or \\\n                provMac.group(3) != reqMac.group(3):\n            return False\n\n        # is the required OS major update >= the provided one?\n        if int(provMac.group(2)) > int(reqMac.group(2)):\n            return False\n\n        return True\n\n    # XXX Linux and other platforms' special cases should go here\n    return False\n\n\ndef run_script(dist_spec, script_name):\n    \"\"\"Locate distribution `dist_spec` and run its `script_name` script\"\"\"\n    ns = sys._getframe(1).f_globals\n    name = ns['__name__']\n    ns.clear()\n    ns['__name__'] = name\n    require(dist_spec)[0].run_script(script_name, ns)\n\n\n# backward compatibility\nrun_main = run_script\n\n\ndef get_distribution(dist):\n    \"\"\"Return a current distribution object for a Requirement or string\"\"\"\n    if isinstance(dist, six.string_types):\n        dist = Requirement.parse(dist)\n    if isinstance(dist, Requirement):\n        dist = get_provider(dist)\n    if not isinstance(dist, Distribution):\n        raise TypeError(\"Expected string, Requirement, or Distribution\", dist)\n    return dist\n\n\ndef load_entry_point(dist, group, name):\n    \"\"\"Return `name` entry point of `group` for `dist` or raise ImportError\"\"\"\n    return get_distribution(dist).load_entry_point(group, name)\n\n\ndef get_entry_map(dist, group=None):\n    \"\"\"Return the entry point map for `group`, or the full entry map\"\"\"\n    return get_distribution(dist).get_entry_map(group)\n\n\ndef get_entry_info(dist, group, name):\n    \"\"\"Return the EntryPoint object for `group`+`name`, or ``None``\"\"\"\n    return get_distribution(dist).get_entry_info(group, name)\n\n\nclass IMetadataProvider:\n    def has_metadata(name):\n        \"\"\"Does the package's distribution contain the named metadata?\"\"\"\n\n    def get_metadata(name):\n        \"\"\"The named metadata resource as a string\"\"\"\n\n    def get_metadata_lines(name):\n        \"\"\"Yield named metadata resource as list of non-blank non-comment lines\n\n       Leading and trailing whitespace is stripped from each line, and lines\n       with ``#`` as the first non-blank character are omitted.\"\"\"\n\n    def metadata_isdir(name):\n        \"\"\"Is the named metadata a directory?  (like ``os.path.isdir()``)\"\"\"\n\n    def metadata_listdir(name):\n        \"\"\"List of metadata names in the directory (like ``os.listdir()``)\"\"\"\n\n    def run_script(script_name, namespace):\n        \"\"\"Execute the named script in the supplied namespace dictionary\"\"\"\n\n\nclass IResourceProvider(IMetadataProvider):\n    \"\"\"An object that provides access to package resources\"\"\"\n\n    def get_resource_filename(manager, resource_name):\n        \"\"\"Return a true filesystem path for `resource_name`\n\n        `manager` must be an ``IResourceManager``\"\"\"\n\n    def get_resource_stream(manager, resource_name):\n        \"\"\"Return a readable file-like object for `resource_name`\n\n        `manager` must be an ``IResourceManager``\"\"\"\n\n    def get_resource_string(manager, resource_name):\n        \"\"\"Return a string containing the contents of `resource_name`\n\n        `manager` must be an ``IResourceManager``\"\"\"\n\n    def has_resource(resource_name):\n        \"\"\"Does the package contain the named resource?\"\"\"\n\n    def resource_isdir(resource_name):\n        \"\"\"Is the named resource a directory?  (like ``os.path.isdir()``)\"\"\"\n\n    def resource_listdir(resource_name):\n        \"\"\"List of resource names in the directory (like ``os.listdir()``)\"\"\"\n\n\nclass WorkingSet:\n    \"\"\"A collection of active distributions on sys.path (or a similar list)\"\"\"\n\n    def __init__(self, entries=None):\n        \"\"\"Create working set from list of path entries (default=sys.path)\"\"\"\n        self.entries = []\n        self.entry_keys = {}\n        self.by_key = {}\n        self.callbacks = []\n\n        if entries is None:\n            entries = sys.path\n\n        for entry in entries:\n            self.add_entry(entry)\n\n    @classmethod\n    def _build_master(cls):\n        \"\"\"\n        Prepare the master working set.\n        \"\"\"\n        ws = cls()\n        try:\n            from __main__ import __requires__\n        except ImportError:\n            # The main program does not list any requirements\n            return ws\n\n        # ensure the requirements are met\n        try:\n            ws.require(__requires__)\n        except VersionConflict:\n            return cls._build_from_requirements(__requires__)\n\n        return ws\n\n    @classmethod\n    def _build_from_requirements(cls, req_spec):\n        \"\"\"\n        Build a working set from a requirement spec. Rewrites sys.path.\n        \"\"\"\n        # try it without defaults already on sys.path\n        # by starting with an empty path\n        ws = cls([])\n        reqs = parse_requirements(req_spec)\n        dists = ws.resolve(reqs, Environment())\n        for dist in dists:\n            ws.add(dist)\n\n        # add any missing entries from sys.path\n        for entry in sys.path:\n            if entry not in ws.entries:\n                ws.add_entry(entry)\n\n        # then copy back to sys.path\n        sys.path[:] = ws.entries\n        return ws\n\n    def add_entry(self, entry):\n        \"\"\"Add a path item to ``.entries``, finding any distributions on it\n\n        ``find_distributions(entry, True)`` is used to find distributions\n        corresponding to the path entry, and they are added.  `entry` is\n        always appended to ``.entries``, even if it is already present.\n        (This is because ``sys.path`` can contain the same value more than\n        once, and the ``.entries`` of the ``sys.path`` WorkingSet should always\n        equal ``sys.path``.)\n        \"\"\"\n        self.entry_keys.setdefault(entry, [])\n        self.entries.append(entry)\n        for dist in find_distributions(entry, True):\n            self.add(dist, entry, False)\n\n    def __contains__(self, dist):\n        \"\"\"True if `dist` is the active distribution for its project\"\"\"\n        return self.by_key.get(dist.key) == dist\n\n    def find(self, req):\n        \"\"\"Find a distribution matching requirement `req`\n\n        If there is an active distribution for the requested project, this\n        returns it as long as it meets the version requirement specified by\n        `req`.  But, if there is an active distribution for the project and it\n        does *not* meet the `req` requirement, ``VersionConflict`` is raised.\n        If there is no active distribution for the requested project, ``None``\n        is returned.\n        \"\"\"\n        dist = self.by_key.get(req.key)\n        if dist is not None and dist not in req:\n            # XXX add more info\n            raise VersionConflict(dist, req)\n        return dist\n\n    def iter_entry_points(self, group, name=None):\n        \"\"\"Yield entry point objects from `group` matching `name`\n\n        If `name` is None, yields all entry points in `group` from all\n        distributions in the working set, otherwise only ones matching\n        both `group` and `name` are yielded (in distribution order).\n        \"\"\"\n        return (\n            entry\n            for dist in self\n            for entry in dist.get_entry_map(group).values()\n            if name is None or name == entry.name\n        )\n\n    def run_script(self, requires, script_name):\n        \"\"\"Locate distribution for `requires` and run `script_name` script\"\"\"\n        ns = sys._getframe(1).f_globals\n        name = ns['__name__']\n        ns.clear()\n        ns['__name__'] = name\n        self.require(requires)[0].run_script(script_name, ns)\n\n    def __iter__(self):\n        \"\"\"Yield distributions for non-duplicate projects in the working set\n\n        The yield order is the order in which the items' path entries were\n        added to the working set.\n        \"\"\"\n        seen = {}\n        for item in self.entries:\n            if item not in self.entry_keys:\n                # workaround a cache issue\n                continue\n\n            for key in self.entry_keys[item]:\n                if key not in seen:\n                    seen[key] = 1\n                    yield self.by_key[key]\n\n    def add(self, dist, entry=None, insert=True, replace=False):\n        \"\"\"Add `dist` to working set, associated with `entry`\n\n        If `entry` is unspecified, it defaults to the ``.location`` of `dist`.\n        On exit from this routine, `entry` is added to the end of the working\n        set's ``.entries`` (if it wasn't already present).\n\n        `dist` is only added to the working set if it's for a project that\n        doesn't already have a distribution in the set, unless `replace=True`.\n        If it's added, any callbacks registered with the ``subscribe()`` method\n        will be called.\n        \"\"\"\n        if insert:\n            dist.insert_on(self.entries, entry, replace=replace)\n\n        if entry is None:\n            entry = dist.location\n        keys = self.entry_keys.setdefault(entry, [])\n        keys2 = self.entry_keys.setdefault(dist.location, [])\n        if not replace and dist.key in self.by_key:\n            # ignore hidden distros\n            return\n\n        self.by_key[dist.key] = dist\n        if dist.key not in keys:\n            keys.append(dist.key)\n        if dist.key not in keys2:\n            keys2.append(dist.key)\n        self._added_new(dist)\n\n    def resolve(self, requirements, env=None, installer=None,\n                replace_conflicting=False, extras=None):\n        \"\"\"List all distributions needed to (recursively) meet `requirements`\n\n        `requirements` must be a sequence of ``Requirement`` objects.  `env`,\n        if supplied, should be an ``Environment`` instance.  If\n        not supplied, it defaults to all distributions available within any\n        entry or distribution in the working set.  `installer`, if supplied,\n        will be invoked with each requirement that cannot be met by an\n        already-installed distribution; it should return a ``Distribution`` or\n        ``None``.\n\n        Unless `replace_conflicting=True`, raises a VersionConflict exception\n        if\n        any requirements are found on the path that have the correct name but\n        the wrong version.  Otherwise, if an `installer` is supplied it will be\n        invoked to obtain the correct version of the requirement and activate\n        it.\n\n        `extras` is a list of the extras to be used with these requirements.\n        This is important because extra requirements may look like `my_req;\n        extra = \"my_extra\"`, which would otherwise be interpreted as a purely\n        optional requirement.  Instead, we want to be able to assert that these\n        requirements are truly required.\n        \"\"\"\n\n        # set up the stack\n        requirements = list(requirements)[::-1]\n        # set of processed requirements\n        processed = {}\n        # key -> dist\n        best = {}\n        to_activate = []\n\n        req_extras = _ReqExtras()\n\n        # Mapping of requirement to set of distributions that required it;\n        # useful for reporting info about conflicts.\n        required_by = collections.defaultdict(set)\n\n        while requirements:\n            # process dependencies breadth-first\n            req = requirements.pop(0)\n            if req in processed:\n                # Ignore cyclic or redundant dependencies\n                continue\n\n            if not req_extras.markers_pass(req, extras):\n                continue\n\n            dist = best.get(req.key)\n            if dist is None:\n                # Find the best distribution and add it to the map\n                dist = self.by_key.get(req.key)\n                if dist is None or (dist not in req and replace_conflicting):\n                    ws = self\n                    if env is None:\n                        if dist is None:\n                            env = Environment(self.entries)\n                        else:\n                            # Use an empty environment and workingset to avoid\n                            # any further conflicts with the conflicting\n                            # distribution\n                            env = Environment([])\n                            ws = WorkingSet([])\n                    dist = best[req.key] = env.best_match(\n                        req, ws, installer,\n                        replace_conflicting=replace_conflicting\n                    )\n                    if dist is None:\n                        requirers = required_by.get(req, None)\n                        raise DistributionNotFound(req, requirers)\n                to_activate.append(dist)\n            if dist not in req:\n                # Oops, the \"best\" so far conflicts with a dependency\n                dependent_req = required_by[req]\n                raise VersionConflict(dist, req).with_context(dependent_req)\n\n            # push the new requirements onto the stack\n            new_requirements = dist.requires(req.extras)[::-1]\n            requirements.extend(new_requirements)\n\n            # Register the new requirements needed by req\n            for new_requirement in new_requirements:\n                required_by[new_requirement].add(req.project_name)\n                req_extras[new_requirement] = req.extras\n\n            processed[req] = True\n\n        # return list of distros to activate\n        return to_activate\n\n    def find_plugins(\n            self, plugin_env, full_env=None, installer=None, fallback=True):\n        \"\"\"Find all activatable distributions in `plugin_env`\n\n        Example usage::\n\n            distributions, errors = working_set.find_plugins(\n                Environment(plugin_dirlist)\n            )\n            # add plugins+libs to sys.path\n            map(working_set.add, distributions)\n            # display errors\n            print('Could not load', errors)\n\n        The `plugin_env` should be an ``Environment`` instance that contains\n        only distributions that are in the project's \"plugin directory\" or\n        directories. The `full_env`, if supplied, should be an ``Environment``\n        contains all currently-available distributions.  If `full_env` is not\n        supplied, one is created automatically from the ``WorkingSet`` this\n        method is called on, which will typically mean that every directory on\n        ``sys.path`` will be scanned for distributions.\n\n        `installer` is a standard installer callback as used by the\n        ``resolve()`` method. The `fallback` flag indicates whether we should\n        attempt to resolve older versions of a plugin if the newest version\n        cannot be resolved.\n\n        This method returns a 2-tuple: (`distributions`, `error_info`), where\n        `distributions` is a list of the distributions found in `plugin_env`\n        that were loadable, along with any other distributions that are needed\n        to resolve their dependencies.  `error_info` is a dictionary mapping\n        unloadable plugin distributions to an exception instance describing the\n        error that occurred. Usually this will be a ``DistributionNotFound`` or\n        ``VersionConflict`` instance.\n        \"\"\"\n\n        plugin_projects = list(plugin_env)\n        # scan project names in alphabetic order\n        plugin_projects.sort()\n\n        error_info = {}\n        distributions = {}\n\n        if full_env is None:\n            env = Environment(self.entries)\n            env += plugin_env\n        else:\n            env = full_env + plugin_env\n\n        shadow_set = self.__class__([])\n        # put all our entries in shadow_set\n        list(map(shadow_set.add, self))\n\n        for project_name in plugin_projects:\n\n            for dist in plugin_env[project_name]:\n\n                req = [dist.as_requirement()]\n\n                try:\n                    resolvees = shadow_set.resolve(req, env, installer)\n\n                except ResolutionError as v:\n                    # save error info\n                    error_info[dist] = v\n                    if fallback:\n                        # try the next older version of project\n                        continue\n                    else:\n                        # give up on this project, keep going\n                        break\n\n                else:\n                    list(map(shadow_set.add, resolvees))\n                    distributions.update(dict.fromkeys(resolvees))\n\n                    # success, no need to try any more versions of this project\n                    break\n\n        distributions = list(distributions)\n        distributions.sort()\n\n        return distributions, error_info\n\n    def require(self, *requirements):\n        \"\"\"Ensure that distributions matching `requirements` are activated\n\n        `requirements` must be a string or a (possibly-nested) sequence\n        thereof, specifying the distributions and versions required.  The\n        return value is a sequence of the distributions that needed to be\n        activated to fulfill the requirements; all relevant distributions are\n        included, even if they were already activated in this working set.\n        \"\"\"\n        needed = self.resolve(parse_requirements(requirements))\n\n        for dist in needed:\n            self.add(dist)\n\n        return needed\n\n    def subscribe(self, callback, existing=True):\n        \"\"\"Invoke `callback` for all distributions\n\n        If `existing=True` (default),\n        call on all existing ones, as well.\n        \"\"\"\n        if callback in self.callbacks:\n            return\n        self.callbacks.append(callback)\n        if not existing:\n            return\n        for dist in self:\n            callback(dist)\n\n    def _added_new(self, dist):\n        for callback in self.callbacks:\n            callback(dist)\n\n    def __getstate__(self):\n        return (\n            self.entries[:], self.entry_keys.copy(), self.by_key.copy(),\n            self.callbacks[:]\n        )\n\n    def __setstate__(self, e_k_b_c):\n        entries, keys, by_key, callbacks = e_k_b_c\n        self.entries = entries[:]\n        self.entry_keys = keys.copy()\n        self.by_key = by_key.copy()\n        self.callbacks = callbacks[:]\n\n\nclass _ReqExtras(dict):\n    \"\"\"\n    Map each requirement to the extras that demanded it.\n    \"\"\"\n\n    def markers_pass(self, req, extras=None):\n        \"\"\"\n        Evaluate markers for req against each extra that\n        demanded it.\n\n        Return False if the req has a marker and fails\n        evaluation. Otherwise, return True.\n        \"\"\"\n        extra_evals = (\n            req.marker.evaluate({'extra': extra})\n            for extra in self.get(req, ()) + (extras or (None,))\n        )\n        return not req.marker or any(extra_evals)\n\n\nclass Environment:\n    \"\"\"Searchable snapshot of distributions on a search path\"\"\"\n\n    def __init__(\n            self, search_path=None, platform=get_supported_platform(),\n            python=PY_MAJOR):\n        \"\"\"Snapshot distributions available on a search path\n\n        Any distributions found on `search_path` are added to the environment.\n        `search_path` should be a sequence of ``sys.path`` items.  If not\n        supplied, ``sys.path`` is used.\n\n        `platform` is an optional string specifying the name of the platform\n        that platform-specific distributions must be compatible with.  If\n        unspecified, it defaults to the current platform.  `python` is an\n        optional string naming the desired version of Python (e.g. ``'3.6'``);\n        it defaults to the current version.\n\n        You may explicitly set `platform` (and/or `python`) to ``None`` if you\n        wish to map *all* distributions, not just those compatible with the\n        running platform or Python version.\n        \"\"\"\n        self._distmap = {}\n        self.platform = platform\n        self.python = python\n        self.scan(search_path)\n\n    def can_add(self, dist):\n        \"\"\"Is distribution `dist` acceptable for this environment?\n\n        The distribution must match the platform and python version\n        requirements specified when this environment was created, or False\n        is returned.\n        \"\"\"\n        py_compat = (\n            self.python is None\n            or dist.py_version is None\n            or dist.py_version == self.python\n        )\n        return py_compat and compatible_platforms(dist.platform, self.platform)\n\n    def remove(self, dist):\n        \"\"\"Remove `dist` from the environment\"\"\"\n        self._distmap[dist.key].remove(dist)\n\n    def scan(self, search_path=None):\n        \"\"\"Scan `search_path` for distributions usable in this environment\n\n        Any distributions found are added to the environment.\n        `search_path` should be a sequence of ``sys.path`` items.  If not\n        supplied, ``sys.path`` is used.  Only distributions conforming to\n        the platform/python version defined at initialization are added.\n        \"\"\"\n        if search_path is None:\n            search_path = sys.path\n\n        for item in search_path:\n            for dist in find_distributions(item):\n                self.add(dist)\n\n    def __getitem__(self, project_name):\n        \"\"\"Return a newest-to-oldest list of distributions for `project_name`\n\n        Uses case-insensitive `project_name` comparison, assuming all the\n        project's distributions use their project's name converted to all\n        lowercase as their key.\n\n        \"\"\"\n        distribution_key = project_name.lower()\n        return self._distmap.get(distribution_key, [])\n\n    def add(self, dist):\n        \"\"\"Add `dist` if we ``can_add()`` it and it has not already been added\n        \"\"\"\n        if self.can_add(dist) and dist.has_version():\n            dists = self._distmap.setdefault(dist.key, [])\n            if dist not in dists:\n                dists.append(dist)\n                dists.sort(key=operator.attrgetter('hashcmp'), reverse=True)\n\n    def best_match(\n            self, req, working_set, installer=None, replace_conflicting=False):\n        \"\"\"Find distribution best matching `req` and usable on `working_set`\n\n        This calls the ``find(req)`` method of the `working_set` to see if a\n        suitable distribution is already active.  (This may raise\n        ``VersionConflict`` if an unsuitable version of the project is already\n        active in the specified `working_set`.)  If a suitable distribution\n        isn't active, this method returns the newest distribution in the\n        environment that meets the ``Requirement`` in `req`.  If no suitable\n        distribution is found, and `installer` is supplied, then the result of\n        calling the environment's ``obtain(req, installer)`` method will be\n        returned.\n        \"\"\"\n        try:\n            dist = working_set.find(req)\n        except VersionConflict:\n            if not replace_conflicting:\n                raise\n            dist = None\n        if dist is not None:\n            return dist\n        for dist in self[req.key]:\n            if dist in req:\n                return dist\n        # try to download/install\n        return self.obtain(req, installer)\n\n    def obtain(self, requirement, installer=None):\n        \"\"\"Obtain a distribution matching `requirement` (e.g. via download)\n\n        Obtain a distro that matches requirement (e.g. via download).  In the\n        base ``Environment`` class, this routine just returns\n        ``installer(requirement)``, unless `installer` is None, in which case\n        None is returned instead.  This method is a hook that allows subclasses\n        to attempt other ways of obtaining a distribution before falling back\n        to the `installer` argument.\"\"\"\n        if installer is not None:\n            return installer(requirement)\n\n    def __iter__(self):\n        \"\"\"Yield the unique project names of the available distributions\"\"\"\n        for key in self._distmap.keys():\n            if self[key]:\n                yield key\n\n    def __iadd__(self, other):\n        \"\"\"In-place addition of a distribution or environment\"\"\"\n        if isinstance(other, Distribution):\n            self.add(other)\n        elif isinstance(other, Environment):\n            for project in other:\n                for dist in other[project]:\n                    self.add(dist)\n        else:\n            raise TypeError(\"Can't add %r to environment\" % (other,))\n        return self\n\n    def __add__(self, other):\n        \"\"\"Add an environment or distribution to an environment\"\"\"\n        new = self.__class__([], platform=None, python=None)\n        for env in self, other:\n            new += env\n        return new\n\n\n# XXX backward compatibility\nAvailableDistributions = Environment\n\n\nclass ExtractionError(RuntimeError):\n    \"\"\"An error occurred extracting a resource\n\n    The following attributes are available from instances of this exception:\n\n    manager\n        The resource manager that raised this exception\n\n    cache_path\n        The base directory for resource extraction\n\n    original_error\n        The exception instance that caused extraction to fail\n    \"\"\"\n\n\nclass ResourceManager:\n    \"\"\"Manage resource extraction and packages\"\"\"\n    extraction_path = None\n\n    def __init__(self):\n        self.cached_files = {}\n\n    def resource_exists(self, package_or_requirement, resource_name):\n        \"\"\"Does the named resource exist?\"\"\"\n        return get_provider(package_or_requirement).has_resource(resource_name)\n\n    def resource_isdir(self, package_or_requirement, resource_name):\n        \"\"\"Is the named resource an existing directory?\"\"\"\n        return get_provider(package_or_requirement).resource_isdir(\n            resource_name\n        )\n\n    def resource_filename(self, package_or_requirement, resource_name):\n        \"\"\"Return a true filesystem path for specified resource\"\"\"\n        return get_provider(package_or_requirement).get_resource_filename(\n            self, resource_name\n        )\n\n    def resource_stream(self, package_or_requirement, resource_name):\n        \"\"\"Return a readable file-like object for specified resource\"\"\"\n        return get_provider(package_or_requirement).get_resource_stream(\n            self, resource_name\n        )\n\n    def resource_string(self, package_or_requirement, resource_name):\n        \"\"\"Return specified resource as a string\"\"\"\n        return get_provider(package_or_requirement).get_resource_string(\n            self, resource_name\n        )\n\n    def resource_listdir(self, package_or_requirement, resource_name):\n        \"\"\"List the contents of the named resource directory\"\"\"\n        return get_provider(package_or_requirement).resource_listdir(\n            resource_name\n        )\n\n    def extraction_error(self):\n        \"\"\"Give an error message for problems extracting file(s)\"\"\"\n\n        old_exc = sys.exc_info()[1]\n        cache_path = self.extraction_path or get_default_cache()\n\n        tmpl = textwrap.dedent(\"\"\"\n            Can't extract file(s) to egg cache\n\n            The following error occurred while trying to extract file(s)\n            to the Python egg cache:\n\n              {old_exc}\n\n            The Python egg cache directory is currently set to:\n\n              {cache_path}\n\n            Perhaps your account does not have write access to this directory?\n            You can change the cache directory by setting the PYTHON_EGG_CACHE\n            environment variable to point to an accessible directory.\n            \"\"\").lstrip()\n        err = ExtractionError(tmpl.format(**locals()))\n        err.manager = self\n        err.cache_path = cache_path\n        err.original_error = old_exc\n        raise err\n\n    def get_cache_path(self, archive_name, names=()):\n        \"\"\"Return absolute location in cache for `archive_name` and `names`\n\n        The parent directory of the resulting path will be created if it does\n        not already exist.  `archive_name` should be the base filename of the\n        enclosing egg (which may not be the name of the enclosing zipfile!),\n        including its \".egg\" extension.  `names`, if provided, should be a\n        sequence of path name parts \"under\" the egg's extraction location.\n\n        This method should only be called by resource providers that need to\n        obtain an extraction location, and only for names they intend to\n        extract, as it tracks the generated names for possible cleanup later.\n        \"\"\"\n        extract_path = self.extraction_path or get_default_cache()\n        target_path = os.path.join(extract_path, archive_name + '-tmp', *names)\n        try:\n            _bypass_ensure_directory(target_path)\n        except Exception:\n            self.extraction_error()\n\n        self._warn_unsafe_extraction_path(extract_path)\n\n        self.cached_files[target_path] = 1\n        return target_path\n\n    @staticmethod\n    def _warn_unsafe_extraction_path(path):\n        \"\"\"\n        If the default extraction path is overridden and set to an insecure\n        location, such as /tmp, it opens up an opportunity for an attacker to\n        replace an extracted file with an unauthorized payload. Warn the user\n        if a known insecure location is used.\n\n        See Distribute #375 for more details.\n        \"\"\"\n        if os.name == 'nt' and not path.startswith(os.environ['windir']):\n            # On Windows, permissions are generally restrictive by default\n            #  and temp directories are not writable by other users, so\n            #  bypass the warning.\n            return\n        mode = os.stat(path).st_mode\n        if mode & stat.S_IWOTH or mode & stat.S_IWGRP:\n            msg = (\n                \"Extraction path is writable by group/others \"\n                \"and vulnerable to attack when \"\n                \"used with get_resource_filename ({path}). \"\n                \"Consider a more secure \"\n                \"location (set with .set_extraction_path or the \"\n                \"PYTHON_EGG_CACHE environment variable).\"\n            ).format(**locals())\n            warnings.warn(msg, UserWarning)\n\n    def postprocess(self, tempname, filename):\n        \"\"\"Perform any platform-specific postprocessing of `tempname`\n\n        This is where Mac header rewrites should be done; other platforms don't\n        have anything special they should do.\n\n        Resource providers should call this method ONLY after successfully\n        extracting a compressed resource.  They must NOT call it on resources\n        that are already in the filesystem.\n\n        `tempname` is the current (temporary) name of the file, and `filename`\n        is the name it will be renamed to by the caller after this routine\n        returns.\n        \"\"\"\n\n        if os.name == 'posix':\n            # Make the resource executable\n            mode = ((os.stat(tempname).st_mode) | 0o555) & 0o7777\n            os.chmod(tempname, mode)\n\n    def set_extraction_path(self, path):\n        \"\"\"Set the base path where resources will be extracted to, if needed.\n\n        If you do not call this routine before any extractions take place, the\n        path defaults to the return value of ``get_default_cache()``.  (Which\n        is based on the ``PYTHON_EGG_CACHE`` environment variable, with various\n        platform-specific fallbacks.  See that routine's documentation for more\n        details.)\n\n        Resources are extracted to subdirectories of this path based upon\n        information given by the ``IResourceProvider``.  You may set this to a\n        temporary directory, but then you must call ``cleanup_resources()`` to\n        delete the extracted files when done.  There is no guarantee that\n        ``cleanup_resources()`` will be able to remove all extracted files.\n\n        (Note: you may not change the extraction path for a given resource\n        manager once resources have been extracted, unless you first call\n        ``cleanup_resources()``.)\n        \"\"\"\n        if self.cached_files:\n            raise ValueError(\n                \"Can't change extraction path, files already extracted\"\n            )\n\n        self.extraction_path = path\n\n    def cleanup_resources(self, force=False):\n        \"\"\"\n        Delete all extracted resource files and directories, returning a list\n        of the file and directory names that could not be successfully removed.\n        This function does not have any concurrency protection, so it should\n        generally only be called when the extraction path is a temporary\n        directory exclusive to a single process.  This method is not\n        automatically called; you must call it explicitly or register it as an\n        ``atexit`` function if you wish to ensure cleanup of a temporary\n        directory used for extractions.\n        \"\"\"\n        # XXX\n\n\ndef get_default_cache():\n    \"\"\"\n    Return the ``PYTHON_EGG_CACHE`` environment variable\n    or a platform-relevant user cache dir for an app\n    named \"Python-Eggs\".\n    \"\"\"\n    return (\n        os.environ.get('PYTHON_EGG_CACHE')\n        or appdirs.user_cache_dir(appname='Python-Eggs')\n    )\n\n\ndef safe_name(name):\n    \"\"\"Convert an arbitrary string to a standard distribution name\n\n    Any runs of non-alphanumeric/. characters are replaced with a single '-'.\n    \"\"\"\n    return re.sub('[^A-Za-z0-9.]+', '-', name)\n\n\ndef safe_version(version):\n    \"\"\"\n    Convert an arbitrary string to a standard version string\n    \"\"\"\n    try:\n        # normalize the version\n        return str(packaging.version.Version(version))\n    except packaging.version.InvalidVersion:\n        version = version.replace(' ', '.')\n        return re.sub('[^A-Za-z0-9.]+', '-', version)\n\n\ndef safe_extra(extra):\n    \"\"\"Convert an arbitrary string to a standard 'extra' name\n\n    Any runs of non-alphanumeric characters are replaced with a single '_',\n    and the result is always lowercased.\n    \"\"\"\n    return re.sub('[^A-Za-z0-9.-]+', '_', extra).lower()\n\n\ndef to_filename(name):\n    \"\"\"Convert a project or version name to its filename-escaped form\n\n    Any '-' characters are currently replaced with '_'.\n    \"\"\"\n    return name.replace('-', '_')\n\n\ndef invalid_marker(text):\n    \"\"\"\n    Validate text as a PEP 508 environment marker; return an exception\n    if invalid or False otherwise.\n    \"\"\"\n    try:\n        evaluate_marker(text)\n    except SyntaxError as e:\n        e.filename = None\n        e.lineno = None\n        return e\n    return False\n\n\ndef evaluate_marker(text, extra=None):\n    \"\"\"\n    Evaluate a PEP 508 environment marker.\n    Return a boolean indicating the marker result in this environment.\n    Raise SyntaxError if marker is invalid.\n\n    This implementation uses the 'pyparsing' module.\n    \"\"\"\n    try:\n        marker = packaging.markers.Marker(text)\n        return marker.evaluate()\n    except packaging.markers.InvalidMarker as e:\n        raise SyntaxError(e) from e\n\n\nclass NullProvider:\n    \"\"\"Try to implement resources and metadata for arbitrary PEP 302 loaders\"\"\"\n\n    egg_name = None\n    egg_info = None\n    loader = None\n\n    def __init__(self, module):\n        self.loader = getattr(module, '__loader__', None)\n        self.module_path = os.path.dirname(getattr(module, '__file__', ''))\n\n    def get_resource_filename(self, manager, resource_name):\n        return self._fn(self.module_path, resource_name)\n\n    def get_resource_stream(self, manager, resource_name):\n        return io.BytesIO(self.get_resource_string(manager, resource_name))\n\n    def get_resource_string(self, manager, resource_name):\n        return self._get(self._fn(self.module_path, resource_name))\n\n    def has_resource(self, resource_name):\n        return self._has(self._fn(self.module_path, resource_name))\n\n    def _get_metadata_path(self, name):\n        return self._fn(self.egg_info, name)\n\n    def has_metadata(self, name):\n        if not self.egg_info:\n            return self.egg_info\n\n        path = self._get_metadata_path(name)\n        return self._has(path)\n\n    def get_metadata(self, name):\n        if not self.egg_info:\n            return \"\"\n        path = self._get_metadata_path(name)\n        value = self._get(path)\n        if six.PY2:\n            return value\n        try:\n            return value.decode('utf-8')\n        except UnicodeDecodeError as exc:\n            # Include the path in the error message to simplify\n            # troubleshooting, and without changing the exception type.\n            exc.reason += ' in {} file at path: {}'.format(name, path)\n            raise\n\n    def get_metadata_lines(self, name):\n        return yield_lines(self.get_metadata(name))\n\n    def resource_isdir(self, resource_name):\n        return self._isdir(self._fn(self.module_path, resource_name))\n\n    def metadata_isdir(self, name):\n        return self.egg_info and self._isdir(self._fn(self.egg_info, name))\n\n    def resource_listdir(self, resource_name):\n        return self._listdir(self._fn(self.module_path, resource_name))\n\n    def metadata_listdir(self, name):\n        if self.egg_info:\n            return self._listdir(self._fn(self.egg_info, name))\n        return []\n\n    def run_script(self, script_name, namespace):\n        script = 'scripts/' + script_name\n        if not self.has_metadata(script):\n            raise ResolutionError(\n                \"Script {script!r} not found in metadata at {self.egg_info!r}\"\n                .format(**locals()),\n            )\n        script_text = self.get_metadata(script).replace('\\r\\n', '\\n')\n        script_text = script_text.replace('\\r', '\\n')\n        script_filename = self._fn(self.egg_info, script)\n        namespace['__file__'] = script_filename\n        if os.path.exists(script_filename):\n            with open(script_filename) as fid:\n                source = fid.read()\n            code = compile(source, script_filename, 'exec')\n            exec(code, namespace, namespace)\n        else:\n            from linecache import cache\n            cache[script_filename] = (\n                len(script_text), 0, script_text.split('\\n'), script_filename\n            )\n            script_code = compile(script_text, script_filename, 'exec')\n            exec(script_code, namespace, namespace)\n\n    def _has(self, path):\n        raise NotImplementedError(\n            \"Can't perform this operation for unregistered loader type\"\n        )\n\n    def _isdir(self, path):\n        raise NotImplementedError(\n            \"Can't perform this operation for unregistered loader type\"\n        )\n\n    def _listdir(self, path):\n        raise NotImplementedError(\n            \"Can't perform this operation for unregistered loader type\"\n        )\n\n    def _fn(self, base, resource_name):\n        self._validate_resource_path(resource_name)\n        if resource_name:\n            return os.path.join(base, *resource_name.split('/'))\n        return base\n\n    @staticmethod\n    def _validate_resource_path(path):\n        \"\"\"\n        Validate the resource paths according to the docs.\n        https://setuptools.readthedocs.io/en/latest/pkg_resources.html#basic-resource-access\n\n        >>> warned = getfixture('recwarn')\n        >>> warnings.simplefilter('always')\n        >>> vrp = NullProvider._validate_resource_path\n        >>> vrp('foo/bar.txt')\n        >>> bool(warned)\n        False\n        >>> vrp('../foo/bar.txt')\n        >>> bool(warned)\n        True\n        >>> warned.clear()\n        >>> vrp('/foo/bar.txt')\n        >>> bool(warned)\n        True\n        >>> vrp('foo/../../bar.txt')\n        >>> bool(warned)\n        True\n        >>> warned.clear()\n        >>> vrp('foo/f../bar.txt')\n        >>> bool(warned)\n        False\n\n        Windows path separators are straight-up disallowed.\n        >>> vrp(r'\\\\foo/bar.txt')\n        Traceback (most recent call last):\n        ...\n        ValueError: Use of .. or absolute path in a resource path \\\nis not allowed.\n\n        >>> vrp(r'C:\\\\foo/bar.txt')\n        Traceback (most recent call last):\n        ...\n        ValueError: Use of .. or absolute path in a resource path \\\nis not allowed.\n\n        Blank values are allowed\n\n        >>> vrp('')\n        >>> bool(warned)\n        False\n\n        Non-string values are not.\n\n        >>> vrp(None)\n        Traceback (most recent call last):\n        ...\n        AttributeError: ...\n        \"\"\"\n        invalid = (\n            os.path.pardir in path.split(posixpath.sep) or\n            posixpath.isabs(path) or\n            ntpath.isabs(path)\n        )\n        if not invalid:\n            return\n\n        msg = \"Use of .. or absolute path in a resource path is not allowed.\"\n\n        # Aggressively disallow Windows absolute paths\n        if ntpath.isabs(path) and not posixpath.isabs(path):\n            raise ValueError(msg)\n\n        # for compatibility, warn; in future\n        # raise ValueError(msg)\n        warnings.warn(\n            msg[:-1] + \" and will raise exceptions in a future release.\",\n            DeprecationWarning,\n            stacklevel=4,\n        )\n\n    def _get(self, path):\n        if hasattr(self.loader, 'get_data'):\n            return self.loader.get_data(path)\n        raise NotImplementedError(\n            \"Can't perform this operation for loaders without 'get_data()'\"\n        )\n\n\nregister_loader_type(object, NullProvider)\n\n\ndef _parents(path):\n    \"\"\"\n    yield all parents of path including path\n    \"\"\"\n    last = None\n    while path != last:\n        yield path\n        last = path\n        path, _ = os.path.split(path)\n\n\nclass EggProvider(NullProvider):\n    \"\"\"Provider based on a virtual filesystem\"\"\"\n\n    def __init__(self, module):\n        NullProvider.__init__(self, module)\n        self._setup_prefix()\n\n    def _setup_prefix(self):\n        # Assume that metadata may be nested inside a \"basket\"\n        # of multiple eggs and use module_path instead of .archive.\n        eggs = filter(_is_egg_path, _parents(self.module_path))\n        egg = next(eggs, None)\n        egg and self._set_egg(egg)\n\n    def _set_egg(self, path):\n        self.egg_name = os.path.basename(path)\n        self.egg_info = os.path.join(path, 'EGG-INFO')\n        self.egg_root = path\n\n\nclass DefaultProvider(EggProvider):\n    \"\"\"Provides access to package resources in the filesystem\"\"\"\n\n    def _has(self, path):\n        return os.path.exists(path)\n\n    def _isdir(self, path):\n        return os.path.isdir(path)\n\n    def _listdir(self, path):\n        return os.listdir(path)\n\n    def get_resource_stream(self, manager, resource_name):\n        return open(self._fn(self.module_path, resource_name), 'rb')\n\n    def _get(self, path):\n        with open(path, 'rb') as stream:\n            return stream.read()\n\n    @classmethod\n    def _register(cls):\n        loader_names = 'SourceFileLoader', 'SourcelessFileLoader',\n        for name in loader_names:\n            loader_cls = getattr(importlib_machinery, name, type(None))\n            register_loader_type(loader_cls, cls)\n\n\nDefaultProvider._register()\n\n\nclass EmptyProvider(NullProvider):\n    \"\"\"Provider that returns nothing for all requests\"\"\"\n\n    module_path = None\n\n    _isdir = _has = lambda self, path: False\n\n    def _get(self, path):\n        return ''\n\n    def _listdir(self, path):\n        return []\n\n    def __init__(self):\n        pass\n\n\nempty_provider = EmptyProvider()\n\n\nclass ZipManifests(dict):\n    \"\"\"\n    zip manifest builder\n    \"\"\"\n\n    @classmethod\n    def build(cls, path):\n        \"\"\"\n        Build a dictionary similar to the zipimport directory\n        caches, except instead of tuples, store ZipInfo objects.\n\n        Use a platform-specific path separator (os.sep) for the path keys\n        for compatibility with pypy on Windows.\n        \"\"\"\n        with zipfile.ZipFile(path) as zfile:\n            items = (\n                (\n                    name.replace('/', os.sep),\n                    zfile.getinfo(name),\n                )\n                for name in zfile.namelist()\n            )\n            return dict(items)\n\n    load = build\n\n\nclass MemoizedZipManifests(ZipManifests):\n    \"\"\"\n    Memoized zipfile manifests.\n    \"\"\"\n    manifest_mod = collections.namedtuple('manifest_mod', 'manifest mtime')\n\n    def load(self, path):\n        \"\"\"\n        Load a manifest at path or return a suitable manifest already loaded.\n        \"\"\"\n        path = os.path.normpath(path)\n        mtime = os.stat(path).st_mtime\n\n        if path not in self or self[path].mtime != mtime:\n            manifest = self.build(path)\n            self[path] = self.manifest_mod(manifest, mtime)\n\n        return self[path].manifest\n\n\nclass ZipProvider(EggProvider):\n    \"\"\"Resource support for zips and eggs\"\"\"\n\n    eagers = None\n    _zip_manifests = MemoizedZipManifests()\n\n    def __init__(self, module):\n        EggProvider.__init__(self, module)\n        self.zip_pre = self.loader.archive + os.sep\n\n    def _zipinfo_name(self, fspath):\n        # Convert a virtual filename (full path to file) into a zipfile subpath\n        # usable with the zipimport directory cache for our target archive\n        fspath = fspath.rstrip(os.sep)\n        if fspath == self.loader.archive:\n            return ''\n        if fspath.startswith(self.zip_pre):\n            return fspath[len(self.zip_pre):]\n        raise AssertionError(\n            \"%s is not a subpath of %s\" % (fspath, self.zip_pre)\n        )\n\n    def _parts(self, zip_path):\n        # Convert a zipfile subpath into an egg-relative path part list.\n        # pseudo-fs path\n        fspath = self.zip_pre + zip_path\n        if fspath.startswith(self.egg_root + os.sep):\n            return fspath[len(self.egg_root) + 1:].split(os.sep)\n        raise AssertionError(\n            \"%s is not a subpath of %s\" % (fspath, self.egg_root)\n        )\n\n    @property\n    def zipinfo(self):\n        return self._zip_manifests.load(self.loader.archive)\n\n    def get_resource_filename(self, manager, resource_name):\n        if not self.egg_name:\n            raise NotImplementedError(\n                \"resource_filename() only supported for .egg, not .zip\"\n            )\n        # no need to lock for extraction, since we use temp names\n        zip_path = self._resource_to_zip(resource_name)\n        eagers = self._get_eager_resources()\n        if '/'.join(self._parts(zip_path)) in eagers:\n            for name in eagers:\n                self._extract_resource(manager, self._eager_to_zip(name))\n        return self._extract_resource(manager, zip_path)\n\n    @staticmethod\n    def _get_date_and_size(zip_stat):\n        size = zip_stat.file_size\n        # ymdhms+wday, yday, dst\n        date_time = zip_stat.date_time + (0, 0, -1)\n        # 1980 offset already done\n        timestamp = time.mktime(date_time)\n        return timestamp, size\n\n    def _extract_resource(self, manager, zip_path):\n\n        if zip_path in self._index():\n            for name in self._index()[zip_path]:\n                last = self._extract_resource(\n                    manager, os.path.join(zip_path, name)\n                )\n            # return the extracted directory name\n            return os.path.dirname(last)\n\n        timestamp, size = self._get_date_and_size(self.zipinfo[zip_path])\n\n        if not WRITE_SUPPORT:\n            raise IOError('\"os.rename\" and \"os.unlink\" are not supported '\n                          'on this platform')\n        try:\n\n            real_path = manager.get_cache_path(\n                self.egg_name, self._parts(zip_path)\n            )\n\n            if self._is_current(real_path, zip_path):\n                return real_path\n\n            outf, tmpnam = _mkstemp(\n                \".$extract\",\n                dir=os.path.dirname(real_path),\n            )\n            os.write(outf, self.loader.get_data(zip_path))\n            os.close(outf)\n            utime(tmpnam, (timestamp, timestamp))\n            manager.postprocess(tmpnam, real_path)\n\n            try:\n                rename(tmpnam, real_path)\n\n            except os.error:\n                if os.path.isfile(real_path):\n                    if self._is_current(real_path, zip_path):\n                        # the file became current since it was checked above,\n                        #  so proceed.\n                        return real_path\n                    # Windows, del old file and retry\n                    elif os.name == 'nt':\n                        unlink(real_path)\n                        rename(tmpnam, real_path)\n                        return real_path\n                raise\n\n        except os.error:\n            # report a user-friendly error\n            manager.extraction_error()\n\n        return real_path\n\n    def _is_current(self, file_path, zip_path):\n        \"\"\"\n        Return True if the file_path is current for this zip_path\n        \"\"\"\n        timestamp, size = self._get_date_and_size(self.zipinfo[zip_path])\n        if not os.path.isfile(file_path):\n            return False\n        stat = os.stat(file_path)\n        if stat.st_size != size or stat.st_mtime != timestamp:\n            return False\n        # check that the contents match\n        zip_contents = self.loader.get_data(zip_path)\n        with open(file_path, 'rb') as f:\n            file_contents = f.read()\n        return zip_contents == file_contents\n\n    def _get_eager_resources(self):\n        if self.eagers is None:\n            eagers = []\n            for name in ('native_libs.txt', 'eager_resources.txt'):\n                if self.has_metadata(name):\n                    eagers.extend(self.get_metadata_lines(name))\n            self.eagers = eagers\n        return self.eagers\n\n    def _index(self):\n        try:\n            return self._dirindex\n        except AttributeError:\n            ind = {}\n            for path in self.zipinfo:\n                parts = path.split(os.sep)\n                while parts:\n                    parent = os.sep.join(parts[:-1])\n                    if parent in ind:\n                        ind[parent].append(parts[-1])\n                        break\n                    else:\n                        ind[parent] = [parts.pop()]\n            self._dirindex = ind\n            return ind\n\n    def _has(self, fspath):\n        zip_path = self._zipinfo_name(fspath)\n        return zip_path in self.zipinfo or zip_path in self._index()\n\n    def _isdir(self, fspath):\n        return self._zipinfo_name(fspath) in self._index()\n\n    def _listdir(self, fspath):\n        return list(self._index().get(self._zipinfo_name(fspath), ()))\n\n    def _eager_to_zip(self, resource_name):\n        return self._zipinfo_name(self._fn(self.egg_root, resource_name))\n\n    def _resource_to_zip(self, resource_name):\n        return self._zipinfo_name(self._fn(self.module_path, resource_name))\n\n\nregister_loader_type(zipimport.zipimporter, ZipProvider)\n\n\nclass FileMetadata(EmptyProvider):\n    \"\"\"Metadata handler for standalone PKG-INFO files\n\n    Usage::\n\n        metadata = FileMetadata(\"/path/to/PKG-INFO\")\n\n    This provider rejects all data and metadata requests except for PKG-INFO,\n    which is treated as existing, and will be the contents of the file at\n    the provided location.\n    \"\"\"\n\n    def __init__(self, path):\n        self.path = path\n\n    def _get_metadata_path(self, name):\n        return self.path\n\n    def has_metadata(self, name):\n        return name == 'PKG-INFO' and os.path.isfile(self.path)\n\n    def get_metadata(self, name):\n        if name != 'PKG-INFO':\n            raise KeyError(\"No metadata except PKG-INFO is available\")\n\n        with io.open(self.path, encoding='utf-8', errors=\"replace\") as f:\n            metadata = f.read()\n        self._warn_on_replacement(metadata)\n        return metadata\n\n    def _warn_on_replacement(self, metadata):\n        # Python 2.7 compat for: replacement_char = '�'\n        replacement_char = b'\\xef\\xbf\\xbd'.decode('utf-8')\n        if replacement_char in metadata:\n            tmpl = \"{self.path} could not be properly decoded in UTF-8\"\n            msg = tmpl.format(**locals())\n            warnings.warn(msg)\n\n    def get_metadata_lines(self, name):\n        return yield_lines(self.get_metadata(name))\n\n\nclass PathMetadata(DefaultProvider):\n    \"\"\"Metadata provider for egg directories\n\n    Usage::\n\n        # Development eggs:\n\n        egg_info = \"/path/to/PackageName.egg-info\"\n        base_dir = os.path.dirname(egg_info)\n        metadata = PathMetadata(base_dir, egg_info)\n        dist_name = os.path.splitext(os.path.basename(egg_info))[0]\n        dist = Distribution(basedir, project_name=dist_name, metadata=metadata)\n\n        # Unpacked egg directories:\n\n        egg_path = \"/path/to/PackageName-ver-pyver-etc.egg\"\n        metadata = PathMetadata(egg_path, os.path.join(egg_path,'EGG-INFO'))\n        dist = Distribution.from_filename(egg_path, metadata=metadata)\n    \"\"\"\n\n    def __init__(self, path, egg_info):\n        self.module_path = path\n        self.egg_info = egg_info\n\n\nclass EggMetadata(ZipProvider):\n    \"\"\"Metadata provider for .egg files\"\"\"\n\n    def __init__(self, importer):\n        \"\"\"Create a metadata provider from a zipimporter\"\"\"\n\n        self.zip_pre = importer.archive + os.sep\n        self.loader = importer\n        if importer.prefix:\n            self.module_path = os.path.join(importer.archive, importer.prefix)\n        else:\n            self.module_path = importer.archive\n        self._setup_prefix()\n\n\n_declare_state('dict', _distribution_finders={})\n\n\ndef register_finder(importer_type, distribution_finder):\n    \"\"\"Register `distribution_finder` to find distributions in sys.path items\n\n    `importer_type` is the type or class of a PEP 302 \"Importer\" (sys.path item\n    handler), and `distribution_finder` is a callable that, passed a path\n    item and the importer instance, yields ``Distribution`` instances found on\n    that path item.  See ``pkg_resources.find_on_path`` for an example.\"\"\"\n    _distribution_finders[importer_type] = distribution_finder\n\n\ndef find_distributions(path_item, only=False):\n    \"\"\"Yield distributions accessible via `path_item`\"\"\"\n    importer = get_importer(path_item)\n    finder = _find_adapter(_distribution_finders, importer)\n    return finder(importer, path_item, only)\n\n\ndef find_eggs_in_zip(importer, path_item, only=False):\n    \"\"\"\n    Find eggs in zip files; possibly multiple nested eggs.\n    \"\"\"\n    if importer.archive.endswith('.whl'):\n        # wheels are not supported with this finder\n        # they don't have PKG-INFO metadata, and won't ever contain eggs\n        return\n    metadata = EggMetadata(importer)\n    if metadata.has_metadata('PKG-INFO'):\n        yield Distribution.from_filename(path_item, metadata=metadata)\n    if only:\n        # don't yield nested distros\n        return\n    for subitem in metadata.resource_listdir(''):\n        if _is_egg_path(subitem):\n            subpath = os.path.join(path_item, subitem)\n            dists = find_eggs_in_zip(zipimport.zipimporter(subpath), subpath)\n            for dist in dists:\n                yield dist\n        elif subitem.lower().endswith('.dist-info'):\n            subpath = os.path.join(path_item, subitem)\n            submeta = EggMetadata(zipimport.zipimporter(subpath))\n            submeta.egg_info = subpath\n            yield Distribution.from_location(path_item, subitem, submeta)\n\n\nregister_finder(zipimport.zipimporter, find_eggs_in_zip)\n\n\ndef find_nothing(importer, path_item, only=False):\n    return ()\n\n\nregister_finder(object, find_nothing)\n\n\ndef _by_version_descending(names):\n    \"\"\"\n    Given a list of filenames, return them in descending order\n    by version number.\n\n    >>> names = 'bar', 'foo', 'Python-2.7.10.egg', 'Python-2.7.2.egg'\n    >>> _by_version_descending(names)\n    ['Python-2.7.10.egg', 'Python-2.7.2.egg', 'foo', 'bar']\n    >>> names = 'Setuptools-1.2.3b1.egg', 'Setuptools-1.2.3.egg'\n    >>> _by_version_descending(names)\n    ['Setuptools-1.2.3.egg', 'Setuptools-1.2.3b1.egg']\n    >>> names = 'Setuptools-1.2.3b1.egg', 'Setuptools-1.2.3.post1.egg'\n    >>> _by_version_descending(names)\n    ['Setuptools-1.2.3.post1.egg', 'Setuptools-1.2.3b1.egg']\n    \"\"\"\n    def _by_version(name):\n        \"\"\"\n        Parse each component of the filename\n        \"\"\"\n        name, ext = os.path.splitext(name)\n        parts = itertools.chain(name.split('-'), [ext])\n        return [packaging.version.parse(part) for part in parts]\n\n    return sorted(names, key=_by_version, reverse=True)\n\n\ndef find_on_path(importer, path_item, only=False):\n    \"\"\"Yield distributions accessible on a sys.path directory\"\"\"\n    path_item = _normalize_cached(path_item)\n\n    if _is_unpacked_egg(path_item):\n        yield Distribution.from_filename(\n            path_item, metadata=PathMetadata(\n                path_item, os.path.join(path_item, 'EGG-INFO')\n            )\n        )\n        return\n\n    entries = safe_listdir(path_item)\n\n    # for performance, before sorting by version,\n    # screen entries for only those that will yield\n    # distributions\n    filtered = (\n        entry\n        for entry in entries\n        if dist_factory(path_item, entry, only)\n    )\n\n    # scan for .egg and .egg-info in directory\n    path_item_entries = _by_version_descending(filtered)\n    for entry in path_item_entries:\n        fullpath = os.path.join(path_item, entry)\n        factory = dist_factory(path_item, entry, only)\n        for dist in factory(fullpath):\n            yield dist\n\n\ndef dist_factory(path_item, entry, only):\n    \"\"\"Return a dist_factory for the given entry.\"\"\"\n    lower = entry.lower()\n    is_egg_info = lower.endswith('.egg-info')\n    is_dist_info = (\n        lower.endswith('.dist-info') and\n        os.path.isdir(os.path.join(path_item, entry))\n    )\n    is_meta = is_egg_info or is_dist_info\n    return (\n        distributions_from_metadata\n        if is_meta else\n        find_distributions\n        if not only and _is_egg_path(entry) else\n        resolve_egg_link\n        if not only and lower.endswith('.egg-link') else\n        NoDists()\n    )\n\n\nclass NoDists:\n    \"\"\"\n    >>> bool(NoDists())\n    False\n\n    >>> list(NoDists()('anything'))\n    []\n    \"\"\"\n    def __bool__(self):\n        return False\n    if six.PY2:\n        __nonzero__ = __bool__\n\n    def __call__(self, fullpath):\n        return iter(())\n\n\ndef safe_listdir(path):\n    \"\"\"\n    Attempt to list contents of path, but suppress some exceptions.\n    \"\"\"\n    try:\n        return os.listdir(path)\n    except (PermissionError, NotADirectoryError):\n        pass\n    except OSError as e:\n        # Ignore the directory if does not exist, not a directory or\n        # permission denied\n        ignorable = (\n            e.errno in (errno.ENOTDIR, errno.EACCES, errno.ENOENT)\n            # Python 2 on Windows needs to be handled this way :(\n            or getattr(e, \"winerror\", None) == 267\n        )\n        if not ignorable:\n            raise\n    return ()\n\n\ndef distributions_from_metadata(path):\n    root = os.path.dirname(path)\n    if os.path.isdir(path):\n        if len(os.listdir(path)) == 0:\n            # empty metadata dir; skip\n            return\n        metadata = PathMetadata(root, path)\n    else:\n        metadata = FileMetadata(path)\n    entry = os.path.basename(path)\n    yield Distribution.from_location(\n        root, entry, metadata, precedence=DEVELOP_DIST,\n    )\n\n\ndef non_empty_lines(path):\n    \"\"\"\n    Yield non-empty lines from file at path\n    \"\"\"\n    with open(path) as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                yield line\n\n\ndef resolve_egg_link(path):\n    \"\"\"\n    Given a path to an .egg-link, resolve distributions\n    present in the referenced path.\n    \"\"\"\n    referenced_paths = non_empty_lines(path)\n    resolved_paths = (\n        os.path.join(os.path.dirname(path), ref)\n        for ref in referenced_paths\n    )\n    dist_groups = map(find_distributions, resolved_paths)\n    return next(dist_groups, ())\n\n\nregister_finder(pkgutil.ImpImporter, find_on_path)\n\nif hasattr(importlib_machinery, 'FileFinder'):\n    register_finder(importlib_machinery.FileFinder, find_on_path)\n\n_declare_state('dict', _namespace_handlers={})\n_declare_state('dict', _namespace_packages={})\n\n\ndef register_namespace_handler(importer_type, namespace_handler):\n    \"\"\"Register `namespace_handler` to declare namespace packages\n\n    `importer_type` is the type or class of a PEP 302 \"Importer\" (sys.path item\n    handler), and `namespace_handler` is a callable like this::\n\n        def namespace_handler(importer, path_entry, moduleName, module):\n            # return a path_entry to use for child packages\n\n    Namespace handlers are only called if the importer object has already\n    agreed that it can handle the relevant path item, and they should only\n    return a subpath if the module __path__ does not already contain an\n    equivalent subpath.  For an example namespace handler, see\n    ``pkg_resources.file_ns_handler``.\n    \"\"\"\n    _namespace_handlers[importer_type] = namespace_handler\n\n\ndef _handle_ns(packageName, path_item):\n    \"\"\"Ensure that named package includes a subpath of path_item (if needed)\"\"\"\n\n    importer = get_importer(path_item)\n    if importer is None:\n        return None\n\n    # use find_spec (PEP 451) and fall-back to find_module (PEP 302)\n    try:\n        loader = importer.find_spec(packageName).loader\n    except AttributeError:\n        # capture warnings due to #1111\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            loader = importer.find_module(packageName)\n\n    if loader is None:\n        return None\n    module = sys.modules.get(packageName)\n    if module is None:\n        module = sys.modules[packageName] = types.ModuleType(packageName)\n        module.__path__ = []\n        _set_parent_ns(packageName)\n    elif not hasattr(module, '__path__'):\n        raise TypeError(\"Not a package:\", packageName)\n    handler = _find_adapter(_namespace_handlers, importer)\n    subpath = handler(importer, path_item, packageName, module)\n    if subpath is not None:\n        path = module.__path__\n        path.append(subpath)\n        loader.load_module(packageName)\n        _rebuild_mod_path(path, packageName, module)\n    return subpath\n\n\ndef _rebuild_mod_path(orig_path, package_name, module):\n    \"\"\"\n    Rebuild module.__path__ ensuring that all entries are ordered\n    corresponding to their sys.path order\n    \"\"\"\n    sys_path = [_normalize_cached(p) for p in sys.path]\n\n    def safe_sys_path_index(entry):\n        \"\"\"\n        Workaround for #520 and #513.\n        \"\"\"\n        try:\n            return sys_path.index(entry)\n        except ValueError:\n            return float('inf')\n\n    def position_in_sys_path(path):\n        \"\"\"\n        Return the ordinal of the path based on its position in sys.path\n        \"\"\"\n        path_parts = path.split(os.sep)\n        module_parts = package_name.count('.') + 1\n        parts = path_parts[:-module_parts]\n        return safe_sys_path_index(_normalize_cached(os.sep.join(parts)))\n\n    new_path = sorted(orig_path, key=position_in_sys_path)\n    new_path = [_normalize_cached(p) for p in new_path]\n\n    if isinstance(module.__path__, list):\n        module.__path__[:] = new_path\n    else:\n        module.__path__ = new_path\n\n\ndef declare_namespace(packageName):\n    \"\"\"Declare that package 'packageName' is a namespace package\"\"\"\n\n    _imp.acquire_lock()\n    try:\n        if packageName in _namespace_packages:\n            return\n\n        path = sys.path\n        parent, _, _ = packageName.rpartition('.')\n\n        if parent:\n            declare_namespace(parent)\n            if parent not in _namespace_packages:\n                __import__(parent)\n            try:\n                path = sys.modules[parent].__path__\n            except AttributeError as e:\n                raise TypeError(\"Not a package:\", parent) from e\n\n        # Track what packages are namespaces, so when new path items are added,\n        # they can be updated\n        _namespace_packages.setdefault(parent or None, []).append(packageName)\n        _namespace_packages.setdefault(packageName, [])\n\n        for path_item in path:\n            # Ensure all the parent's path items are reflected in the child,\n            # if they apply\n            _handle_ns(packageName, path_item)\n\n    finally:\n        _imp.release_lock()\n\n\ndef fixup_namespace_packages(path_item, parent=None):\n    \"\"\"Ensure that previously-declared namespace packages include path_item\"\"\"\n    _imp.acquire_lock()\n    try:\n        for package in _namespace_packages.get(parent, ()):\n            subpath = _handle_ns(package, path_item)\n            if subpath:\n                fixup_namespace_packages(subpath, package)\n    finally:\n        _imp.release_lock()\n\n\ndef file_ns_handler(importer, path_item, packageName, module):\n    \"\"\"Compute an ns-package subpath for a filesystem or zipfile importer\"\"\"\n\n    subpath = os.path.join(path_item, packageName.split('.')[-1])\n    normalized = _normalize_cached(subpath)\n    for item in module.__path__:\n        if _normalize_cached(item) == normalized:\n            break\n    else:\n        # Only return the path if it's not already there\n        return subpath\n\n\nregister_namespace_handler(pkgutil.ImpImporter, file_ns_handler)\nregister_namespace_handler(zipimport.zipimporter, file_ns_handler)\n\nif hasattr(importlib_machinery, 'FileFinder'):\n    register_namespace_handler(importlib_machinery.FileFinder, file_ns_handler)\n\n\ndef null_ns_handler(importer, path_item, packageName, module):\n    return None\n\n\nregister_namespace_handler(object, null_ns_handler)\n\n\ndef normalize_path(filename):\n    \"\"\"Normalize a file/dir name for comparison purposes\"\"\"\n    return os.path.normcase(os.path.realpath(os.path.normpath(\n        _cygwin_patch(filename))))\n\n\ndef _cygwin_patch(filename):  # pragma: nocover\n    \"\"\"\n    Contrary to POSIX 2008, on Cygwin, getcwd (3) contains\n    symlink components. Using\n    os.path.abspath() works around this limitation. A fix in os.getcwd()\n    would probably better, in Cygwin even more so, except\n    that this seems to be by design...\n    \"\"\"\n    return os.path.abspath(filename) if sys.platform == 'cygwin' else filename\n\n\ndef _normalize_cached(filename, _cache={}):\n    try:\n        return _cache[filename]\n    except KeyError:\n        _cache[filename] = result = normalize_path(filename)\n        return result\n\n\ndef _is_egg_path(path):\n    \"\"\"\n    Determine if given path appears to be an egg.\n    \"\"\"\n    return path.lower().endswith('.egg')\n\n\ndef _is_unpacked_egg(path):\n    \"\"\"\n    Determine if given path appears to be an unpacked egg.\n    \"\"\"\n    return (\n        _is_egg_path(path) and\n        os.path.isfile(os.path.join(path, 'EGG-INFO', 'PKG-INFO'))\n    )\n\n\ndef _set_parent_ns(packageName):\n    parts = packageName.split('.')\n    name = parts.pop()\n    if parts:\n        parent = '.'.join(parts)\n        setattr(sys.modules[parent], name, sys.modules[packageName])\n\n\ndef yield_lines(strs):\n    \"\"\"Yield non-empty/non-comment lines of a string or sequence\"\"\"\n    if isinstance(strs, six.string_types):\n        for s in strs.splitlines():\n            s = s.strip()\n            # skip blank lines/comments\n            if s and not s.startswith('#'):\n                yield s\n    else:\n        for ss in strs:\n            for s in yield_lines(ss):\n                yield s\n\n\nMODULE = re.compile(r\"\\w+(\\.\\w+)*$\").match\nEGG_NAME = re.compile(\n    r\"\"\"\n    (?P<name>[^-]+) (\n        -(?P<ver>[^-]+) (\n            -py(?P<pyver>[^-]+) (\n                -(?P<plat>.+)\n            )?\n        )?\n    )?\n    \"\"\",\n    re.VERBOSE | re.IGNORECASE,\n).match\n\n\nclass EntryPoint:\n    \"\"\"Object representing an advertised importable object\"\"\"\n\n    def __init__(self, name, module_name, attrs=(), extras=(), dist=None):\n        if not MODULE(module_name):\n            raise ValueError(\"Invalid module name\", module_name)\n        self.name = name\n        self.module_name = module_name\n        self.attrs = tuple(attrs)\n        self.extras = tuple(extras)\n        self.dist = dist\n\n    def __str__(self):\n        s = \"%s = %s\" % (self.name, self.module_name)\n        if self.attrs:\n            s += ':' + '.'.join(self.attrs)\n        if self.extras:\n            s += ' [%s]' % ','.join(self.extras)\n        return s\n\n    def __repr__(self):\n        return \"EntryPoint.parse(%r)\" % str(self)\n\n    def load(self, require=True, *args, **kwargs):\n        \"\"\"\n        Require packages for this EntryPoint, then resolve it.\n        \"\"\"\n        if not require or args or kwargs:\n            warnings.warn(\n                \"Parameters to load are deprecated.  Call .resolve and \"\n                \".require separately.\",\n                PkgResourcesDeprecationWarning,\n                stacklevel=2,\n            )\n        if require:\n            self.require(*args, **kwargs)\n        return self.resolve()\n\n    def resolve(self):\n        \"\"\"\n        Resolve the entry point from its module and attrs.\n        \"\"\"\n        module = __import__(self.module_name, fromlist=['__name__'], level=0)\n        try:\n            return functools.reduce(getattr, self.attrs, module)\n        except AttributeError as exc:\n            raise ImportError(str(exc)) from exc\n\n    def require(self, env=None, installer=None):\n        if self.extras and not self.dist:\n            raise UnknownExtra(\"Can't require() without a distribution\", self)\n\n        # Get the requirements for this entry point with all its extras and\n        # then resolve them. We have to pass `extras` along when resolving so\n        # that the working set knows what extras we want. Otherwise, for\n        # dist-info distributions, the working set will assume that the\n        # requirements for that extra are purely optional and skip over them.\n        reqs = self.dist.requires(self.extras)\n        items = working_set.resolve(reqs, env, installer, extras=self.extras)\n        list(map(working_set.add, items))\n\n    pattern = re.compile(\n        r'\\s*'\n        r'(?P<name>.+?)\\s*'\n        r'=\\s*'\n        r'(?P<module>[\\w.]+)\\s*'\n        r'(:\\s*(?P<attr>[\\w.]+))?\\s*'\n        r'(?P<extras>\\[.*\\])?\\s*$'\n    )\n\n    @classmethod\n    def parse(cls, src, dist=None):\n        \"\"\"Parse a single entry point from string `src`\n\n        Entry point syntax follows the form::\n\n            name = some.module:some.attr [extra1, extra2]\n\n        The entry name and module name are required, but the ``:attrs`` and\n        ``[extras]`` parts are optional\n        \"\"\"\n        m = cls.pattern.match(src)\n        if not m:\n            msg = \"EntryPoint must be in 'name=module:attrs [extras]' format\"\n            raise ValueError(msg, src)\n        res = m.groupdict()\n        extras = cls._parse_extras(res['extras'])\n        attrs = res['attr'].split('.') if res['attr'] else ()\n        return cls(res['name'], res['module'], attrs, extras, dist)\n\n    @classmethod\n    def _parse_extras(cls, extras_spec):\n        if not extras_spec:\n            return ()\n        req = Requirement.parse('x' + extras_spec)\n        if req.specs:\n            raise ValueError()\n        return req.extras\n\n    @classmethod\n    def parse_group(cls, group, lines, dist=None):\n        \"\"\"Parse an entry point group\"\"\"\n        if not MODULE(group):\n            raise ValueError(\"Invalid group name\", group)\n        this = {}\n        for line in yield_lines(lines):\n            ep = cls.parse(line, dist)\n            if ep.name in this:\n                raise ValueError(\"Duplicate entry point\", group, ep.name)\n            this[ep.name] = ep\n        return this\n\n    @classmethod\n    def parse_map(cls, data, dist=None):\n        \"\"\"Parse a map of entry point groups\"\"\"\n        if isinstance(data, dict):\n            data = data.items()\n        else:\n            data = split_sections(data)\n        maps = {}\n        for group, lines in data:\n            if group is None:\n                if not lines:\n                    continue\n                raise ValueError(\"Entry points must be listed in groups\")\n            group = group.strip()\n            if group in maps:\n                raise ValueError(\"Duplicate group name\", group)\n            maps[group] = cls.parse_group(group, lines, dist)\n        return maps\n\n\ndef _version_from_file(lines):\n    \"\"\"\n    Given an iterable of lines from a Metadata file, return\n    the value of the Version field, if present, or None otherwise.\n    \"\"\"\n    def is_version_line(line):\n        return line.lower().startswith('version:')\n    version_lines = filter(is_version_line, lines)\n    line = next(iter(version_lines), '')\n    _, _, value = line.partition(':')\n    return safe_version(value.strip()) or None\n\n\nclass Distribution:\n    \"\"\"Wrap an actual or potential sys.path entry w/metadata\"\"\"\n    PKG_INFO = 'PKG-INFO'\n\n    def __init__(\n            self, location=None, metadata=None, project_name=None,\n            version=None, py_version=PY_MAJOR, platform=None,\n            precedence=EGG_DIST):\n        self.project_name = safe_name(project_name or 'Unknown')\n        if version is not None:\n            self._version = safe_version(version)\n        self.py_version = py_version\n        self.platform = platform\n        self.location = location\n        self.precedence = precedence\n        self._provider = metadata or empty_provider\n\n    @classmethod\n    def from_location(cls, location, basename, metadata=None, **kw):\n        project_name, version, py_version, platform = [None] * 4\n        basename, ext = os.path.splitext(basename)\n        if ext.lower() in _distributionImpl:\n            cls = _distributionImpl[ext.lower()]\n\n            match = EGG_NAME(basename)\n            if match:\n                project_name, version, py_version, platform = match.group(\n                    'name', 'ver', 'pyver', 'plat'\n                )\n        return cls(\n            location, metadata, project_name=project_name, version=version,\n            py_version=py_version, platform=platform, **kw\n        )._reload_version()\n\n    def _reload_version(self):\n        return self\n\n    @property\n    def hashcmp(self):\n        return (\n            self.parsed_version,\n            self.precedence,\n            self.key,\n            self.location,\n            self.py_version or '',\n            self.platform or '',\n        )\n\n    def __hash__(self):\n        return hash(self.hashcmp)\n\n    def __lt__(self, other):\n        return self.hashcmp < other.hashcmp\n\n    def __le__(self, other):\n        return self.hashcmp <= other.hashcmp\n\n    def __gt__(self, other):\n        return self.hashcmp > other.hashcmp\n\n    def __ge__(self, other):\n        return self.hashcmp >= other.hashcmp\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            # It's not a Distribution, so they are not equal\n            return False\n        return self.hashcmp == other.hashcmp\n\n    def __ne__(self, other):\n        return not self == other\n\n    # These properties have to be lazy so that we don't have to load any\n    # metadata until/unless it's actually needed.  (i.e., some distributions\n    # may not know their name or version without loading PKG-INFO)\n\n    @property\n    def key(self):\n        try:\n            return self._key\n        except AttributeError:\n            self._key = key = self.project_name.lower()\n            return key\n\n    @property\n    def parsed_version(self):\n        if not hasattr(self, \"_parsed_version\"):\n            self._parsed_version = parse_version(self.version)\n\n        return self._parsed_version\n\n    def _warn_legacy_version(self):\n        LV = packaging.version.LegacyVersion\n        is_legacy = isinstance(self._parsed_version, LV)\n        if not is_legacy:\n            return\n\n        # While an empty version is technically a legacy version and\n        # is not a valid PEP 440 version, it's also unlikely to\n        # actually come from someone and instead it is more likely that\n        # it comes from setuptools attempting to parse a filename and\n        # including it in the list. So for that we'll gate this warning\n        # on if the version is anything at all or not.\n        if not self.version:\n            return\n\n        tmpl = textwrap.dedent(\"\"\"\n            '{project_name} ({version})' is being parsed as a legacy,\n            non PEP 440,\n            version. You may find odd behavior and sort order.\n            In particular it will be sorted as less than 0.0. It\n            is recommended to migrate to PEP 440 compatible\n            versions.\n            \"\"\").strip().replace('\\n', ' ')\n\n        warnings.warn(tmpl.format(**vars(self)), PEP440Warning)\n\n    @property\n    def version(self):\n        try:\n            return self._version\n        except AttributeError as e:\n            version = self._get_version()\n            if version is None:\n                path = self._get_metadata_path_for_display(self.PKG_INFO)\n                msg = (\n                    \"Missing 'Version:' header and/or {} file at path: {}\"\n                ).format(self.PKG_INFO, path)\n                raise ValueError(msg, self) from e\n\n            return version\n\n    @property\n    def _dep_map(self):\n        \"\"\"\n        A map of extra to its list of (direct) requirements\n        for this distribution, including the null extra.\n        \"\"\"\n        try:\n            return self.__dep_map\n        except AttributeError:\n            self.__dep_map = self._filter_extras(self._build_dep_map())\n        return self.__dep_map\n\n    @staticmethod\n    def _filter_extras(dm):\n        \"\"\"\n        Given a mapping of extras to dependencies, strip off\n        environment markers and filter out any dependencies\n        not matching the markers.\n        \"\"\"\n        for extra in list(filter(None, dm)):\n            new_extra = extra\n            reqs = dm.pop(extra)\n            new_extra, _, marker = extra.partition(':')\n            fails_marker = marker and (\n                invalid_marker(marker)\n                or not evaluate_marker(marker)\n            )\n            if fails_marker:\n                reqs = []\n            new_extra = safe_extra(new_extra) or None\n\n            dm.setdefault(new_extra, []).extend(reqs)\n        return dm\n\n    def _build_dep_map(self):\n        dm = {}\n        for name in 'requires.txt', 'depends.txt':\n            for extra, reqs in split_sections(self._get_metadata(name)):\n                dm.setdefault(extra, []).extend(parse_requirements(reqs))\n        return dm\n\n    def requires(self, extras=()):\n        \"\"\"List of Requirements needed for this distro if `extras` are used\"\"\"\n        dm = self._dep_map\n        deps = []\n        deps.extend(dm.get(None, ()))\n        for ext in extras:\n            try:\n                deps.extend(dm[safe_extra(ext)])\n            except KeyError as e:\n                raise UnknownExtra(\n                    \"%s has no such extra feature %r\" % (self, ext)\n                ) from e\n        return deps\n\n    def _get_metadata_path_for_display(self, name):\n        \"\"\"\n        Return the path to the given metadata file, if available.\n        \"\"\"\n        try:\n            # We need to access _get_metadata_path() on the provider object\n            # directly rather than through this class's __getattr__()\n            # since _get_metadata_path() is marked private.\n            path = self._provider._get_metadata_path(name)\n\n        # Handle exceptions e.g. in case the distribution's metadata\n        # provider doesn't support _get_metadata_path().\n        except Exception:\n            return '[could not detect]'\n\n        return path\n\n    def _get_metadata(self, name):\n        if self.has_metadata(name):\n            for line in self.get_metadata_lines(name):\n                yield line\n\n    def _get_version(self):\n        lines = self._get_metadata(self.PKG_INFO)\n        version = _version_from_file(lines)\n\n        return version\n\n    def activate(self, path=None, replace=False):\n        \"\"\"Ensure distribution is importable on `path` (default=sys.path)\"\"\"\n        if path is None:\n            path = sys.path\n        self.insert_on(path, replace=replace)\n        if path is sys.path:\n            fixup_namespace_packages(self.location)\n            for pkg in self._get_metadata('namespace_packages.txt'):\n                if pkg in sys.modules:\n                    declare_namespace(pkg)\n\n    def egg_name(self):\n        \"\"\"Return what this distribution's standard .egg filename should be\"\"\"\n        filename = \"%s-%s-py%s\" % (\n            to_filename(self.project_name), to_filename(self.version),\n            self.py_version or PY_MAJOR\n        )\n\n        if self.platform:\n            filename += '-' + self.platform\n        return filename\n\n    def __repr__(self):\n        if self.location:\n            return \"%s (%s)\" % (self, self.location)\n        else:\n            return str(self)\n\n    def __str__(self):\n        try:\n            version = getattr(self, 'version', None)\n        except ValueError:\n            version = None\n        version = version or \"[unknown version]\"\n        return \"%s %s\" % (self.project_name, version)\n\n    def __getattr__(self, attr):\n        \"\"\"Delegate all unrecognized public attributes to .metadata provider\"\"\"\n        if attr.startswith('_'):\n            raise AttributeError(attr)\n        return getattr(self._provider, attr)\n\n    def __dir__(self):\n        return list(\n            set(super(Distribution, self).__dir__())\n            | set(\n                attr for attr in self._provider.__dir__()\n                if not attr.startswith('_')\n            )\n        )\n\n    if not hasattr(object, '__dir__'):\n        # python 2.7 not supported\n        del __dir__\n\n    @classmethod\n    def from_filename(cls, filename, metadata=None, **kw):\n        return cls.from_location(\n            _normalize_cached(filename), os.path.basename(filename), metadata,\n            **kw\n        )\n\n    def as_requirement(self):\n        \"\"\"Return a ``Requirement`` that matches this distribution exactly\"\"\"\n        if isinstance(self.parsed_version, packaging.version.Version):\n            spec = \"%s==%s\" % (self.project_name, self.parsed_version)\n        else:\n            spec = \"%s===%s\" % (self.project_name, self.parsed_version)\n\n        return Requirement.parse(spec)\n\n    def load_entry_point(self, group, name):\n        \"\"\"Return the `name` entry point of `group` or raise ImportError\"\"\"\n        ep = self.get_entry_info(group, name)\n        if ep is None:\n            raise ImportError(\"Entry point %r not found\" % ((group, name),))\n        return ep.load()\n\n    def get_entry_map(self, group=None):\n        \"\"\"Return the entry point map for `group`, or the full entry map\"\"\"\n        try:\n            ep_map = self._ep_map\n        except AttributeError:\n            ep_map = self._ep_map = EntryPoint.parse_map(\n                self._get_metadata('entry_points.txt'), self\n            )\n        if group is not None:\n            return ep_map.get(group, {})\n        return ep_map\n\n    def get_entry_info(self, group, name):\n        \"\"\"Return the EntryPoint object for `group`+`name`, or ``None``\"\"\"\n        return self.get_entry_map(group).get(name)\n\n    def insert_on(self, path, loc=None, replace=False):\n        \"\"\"Ensure self.location is on path\n\n        If replace=False (default):\n            - If location is already in path anywhere, do nothing.\n            - Else:\n              - If it's an egg and its parent directory is on path,\n                insert just ahead of the parent.\n              - Else: add to the end of path.\n        If replace=True:\n            - If location is already on path anywhere (not eggs)\n              or higher priority than its parent (eggs)\n              do nothing.\n            - Else:\n              - If it's an egg and its parent directory is on path,\n                insert just ahead of the parent,\n                removing any lower-priority entries.\n              - Else: add it to the front of path.\n        \"\"\"\n\n        loc = loc or self.location\n        if not loc:\n            return\n\n        nloc = _normalize_cached(loc)\n        bdir = os.path.dirname(nloc)\n        npath = [(p and _normalize_cached(p) or p) for p in path]\n\n        for p, item in enumerate(npath):\n            if item == nloc:\n                if replace:\n                    break\n                else:\n                    # don't modify path (even removing duplicates) if\n                    # found and not replace\n                    return\n            elif item == bdir and self.precedence == EGG_DIST:\n                # if it's an .egg, give it precedence over its directory\n                # UNLESS it's already been added to sys.path and replace=False\n                if (not replace) and nloc in npath[p:]:\n                    return\n                if path is sys.path:\n                    self.check_version_conflict()\n                path.insert(p, loc)\n                npath.insert(p, nloc)\n                break\n        else:\n            if path is sys.path:\n                self.check_version_conflict()\n            if replace:\n                path.insert(0, loc)\n            else:\n                path.append(loc)\n            return\n\n        # p is the spot where we found or inserted loc; now remove duplicates\n        while True:\n            try:\n                np = npath.index(nloc, p + 1)\n            except ValueError:\n                break\n            else:\n                del npath[np], path[np]\n                # ha!\n                p = np\n\n        return\n\n    def check_version_conflict(self):\n        if self.key == 'setuptools':\n            # ignore the inevitable setuptools self-conflicts  :(\n            return\n\n        nsp = dict.fromkeys(self._get_metadata('namespace_packages.txt'))\n        loc = normalize_path(self.location)\n        for modname in self._get_metadata('top_level.txt'):\n            if (modname not in sys.modules or modname in nsp\n                    or modname in _namespace_packages):\n                continue\n            if modname in ('pkg_resources', 'setuptools', 'site'):\n                continue\n            fn = getattr(sys.modules[modname], '__file__', None)\n            if fn and (normalize_path(fn).startswith(loc) or\n                       fn.startswith(self.location)):\n                continue\n            issue_warning(\n                \"Module %s was already imported from %s, but %s is being added\"\n                \" to sys.path\" % (modname, fn, self.location),\n            )\n\n    def has_version(self):\n        try:\n            self.version\n        except ValueError:\n            issue_warning(\"Unbuilt egg for \" + repr(self))\n            return False\n        return True\n\n    def clone(self, **kw):\n        \"\"\"Copy this distribution, substituting in any changed keyword args\"\"\"\n        names = 'project_name version py_version platform location precedence'\n        for attr in names.split():\n            kw.setdefault(attr, getattr(self, attr, None))\n        kw.setdefault('metadata', self._provider)\n        return self.__class__(**kw)\n\n    @property\n    def extras(self):\n        return [dep for dep in self._dep_map if dep]\n\n\nclass EggInfoDistribution(Distribution):\n    def _reload_version(self):\n        \"\"\"\n        Packages installed by distutils (e.g. numpy or scipy),\n        which uses an old safe_version, and so\n        their version numbers can get mangled when\n        converted to filenames (e.g., 1.11.0.dev0+2329eae to\n        1.11.0.dev0_2329eae). These distributions will not be\n        parsed properly\n        downstream by Distribution and safe_version, so\n        take an extra step and try to get the version number from\n        the metadata file itself instead of the filename.\n        \"\"\"\n        md_version = self._get_version()\n        if md_version:\n            self._version = md_version\n        return self\n\n\nclass DistInfoDistribution(Distribution):\n    \"\"\"\n    Wrap an actual or potential sys.path entry\n    w/metadata, .dist-info style.\n    \"\"\"\n    PKG_INFO = 'METADATA'\n    EQEQ = re.compile(r\"([\\(,])\\s*(\\d.*?)\\s*([,\\)])\")\n\n    @property\n    def _parsed_pkg_info(self):\n        \"\"\"Parse and cache metadata\"\"\"\n        try:\n            return self._pkg_info\n        except AttributeError:\n            metadata = self.get_metadata(self.PKG_INFO)\n            self._pkg_info = email.parser.Parser().parsestr(metadata)\n            return self._pkg_info\n\n    @property\n    def _dep_map(self):\n        try:\n            return self.__dep_map\n        except AttributeError:\n            self.__dep_map = self._compute_dependencies()\n            return self.__dep_map\n\n    def _compute_dependencies(self):\n        \"\"\"Recompute this distribution's dependencies.\"\"\"\n        dm = self.__dep_map = {None: []}\n\n        reqs = []\n        # Including any condition expressions\n        for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n            reqs.extend(parse_requirements(req))\n\n        def reqs_for_extra(extra):\n            for req in reqs:\n                if not req.marker or req.marker.evaluate({'extra': extra}):\n                    yield req\n\n        common = frozenset(reqs_for_extra(None))\n        dm[None].extend(common)\n\n        for extra in self._parsed_pkg_info.get_all('Provides-Extra') or []:\n            s_extra = safe_extra(extra.strip())\n            dm[s_extra] = list(frozenset(reqs_for_extra(extra)) - common)\n\n        return dm\n\n\n_distributionImpl = {\n    '.egg': Distribution,\n    '.egg-info': EggInfoDistribution,\n    '.dist-info': DistInfoDistribution,\n}\n\n\ndef issue_warning(*args, **kw):\n    level = 1\n    g = globals()\n    try:\n        # find the first stack frame that is *not* code in\n        # the pkg_resources module, to use for the warning\n        while sys._getframe(level).f_globals is g:\n            level += 1\n    except ValueError:\n        pass\n    warnings.warn(stacklevel=level + 1, *args, **kw)\n\n\ndef parse_requirements(strs):\n    \"\"\"Yield ``Requirement`` objects for each specification in `strs`\n\n    `strs` must be a string, or a (possibly-nested) iterable thereof.\n    \"\"\"\n    # create a steppable iterator, so we can handle \\-continuations\n    lines = iter(yield_lines(strs))\n\n    for line in lines:\n        # Drop comments -- a hash without a space may be in a URL.\n        if ' #' in line:\n            line = line[:line.find(' #')]\n        # If there is a line continuation, drop it, and append the next line.\n        if line.endswith('\\\\'):\n            line = line[:-2].strip()\n            try:\n                line += next(lines)\n            except StopIteration:\n                return\n        yield Requirement(line)\n\n\nclass RequirementParseError(packaging.requirements.InvalidRequirement):\n    \"Compatibility wrapper for InvalidRequirement\"\n\n\nclass Requirement(packaging.requirements.Requirement):\n    def __init__(self, requirement_string):\n        \"\"\"DO NOT CALL THIS UNDOCUMENTED METHOD; use Requirement.parse()!\"\"\"\n        super(Requirement, self).__init__(requirement_string)\n        self.unsafe_name = self.name\n        project_name = safe_name(self.name)\n        self.project_name, self.key = project_name, project_name.lower()\n        self.specs = [\n            (spec.operator, spec.version) for spec in self.specifier]\n        self.extras = tuple(map(safe_extra, self.extras))\n        self.hashCmp = (\n            self.key,\n            self.url,\n            self.specifier,\n            frozenset(self.extras),\n            str(self.marker) if self.marker else None,\n        )\n        self.__hash = hash(self.hashCmp)\n\n    def __eq__(self, other):\n        return (\n            isinstance(other, Requirement) and\n            self.hashCmp == other.hashCmp\n        )\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __contains__(self, item):\n        if isinstance(item, Distribution):\n            if item.key != self.key:\n                return False\n\n            item = item.version\n\n        # Allow prereleases always in order to match the previous behavior of\n        # this method. In the future this should be smarter and follow PEP 440\n        # more accurately.\n        return self.specifier.contains(item, prereleases=True)\n\n    def __hash__(self):\n        return self.__hash\n\n    def __repr__(self):\n        return \"Requirement.parse(%r)\" % str(self)\n\n    @staticmethod\n    def parse(s):\n        req, = parse_requirements(s)\n        return req\n\n\ndef _always_object(classes):\n    \"\"\"\n    Ensure object appears in the mro even\n    for old-style classes.\n    \"\"\"\n    if object not in classes:\n        return classes + (object,)\n    return classes\n\n\ndef _find_adapter(registry, ob):\n    \"\"\"Return an adapter factory for `ob` from `registry`\"\"\"\n    types = _always_object(inspect.getmro(getattr(ob, '__class__', type(ob))))\n    for t in types:\n        if t in registry:\n            return registry[t]\n\n\ndef ensure_directory(path):\n    \"\"\"Ensure that the parent directory of `path` exists\"\"\"\n    dirname = os.path.dirname(path)\n    os.makedirs(dirname, exist_ok=True)\n\n\ndef _bypass_ensure_directory(path):\n    \"\"\"Sandbox-bypassing version of ensure_directory()\"\"\"\n    if not WRITE_SUPPORT:\n        raise IOError('\"os.mkdir\" not supported on this platform.')\n    dirname, filename = split(path)\n    if dirname and filename and not isdir(dirname):\n        _bypass_ensure_directory(dirname)\n        try:\n            mkdir(dirname, 0o755)\n        except FileExistsError:\n            pass\n\n\ndef split_sections(s):\n    \"\"\"Split a string or iterable thereof into (section, content) pairs\n\n    Each ``section`` is a stripped version of the section header (\"[section]\")\n    and each ``content`` is a list of stripped lines excluding blank lines and\n    comment-only lines.  If there are any such lines before the first section\n    header, they're returned in a first ``section`` of ``None``.\n    \"\"\"\n    section = None\n    content = []\n    for line in yield_lines(s):\n        if line.startswith(\"[\"):\n            if line.endswith(\"]\"):\n                if section or content:\n                    yield section, content\n                section = line[1:-1].strip()\n                content = []\n            else:\n                raise ValueError(\"Invalid section heading\", line)\n        else:\n            content.append(line)\n\n    # wrap up last segment\n    yield section, content\n\n\ndef _mkstemp(*args, **kw):\n    old_open = os.open\n    try:\n        # temporarily bypass sandboxing\n        os.open = os_open\n        return tempfile.mkstemp(*args, **kw)\n    finally:\n        # and then put it back\n        os.open = old_open\n\n\n# Silence the PEP440Warning by default, so that end users don't get hit by it\n# randomly just because they use pkg_resources. We want to append the rule\n# because we want earlier uses of filterwarnings to take precedence over this\n# one.\nwarnings.filterwarnings(\"ignore\", category=PEP440Warning, append=True)\n\n\n# from jaraco.functools 1.3\ndef _call_aside(f, *args, **kwargs):\n    f(*args, **kwargs)\n    return f\n\n\n@_call_aside\ndef _initialize(g=globals()):\n    \"Set up global resource manager (deliberately not state-saved)\"\n    manager = ResourceManager()\n    g['_manager'] = manager\n    g.update(\n        (name, getattr(manager, name))\n        for name in dir(manager)\n        if not name.startswith('_')\n    )\n\n\n@_call_aside\ndef _initialize_master_working_set():\n    \"\"\"\n    Prepare the master working set and make the ``require()``\n    API available.\n\n    This function has explicit effects on the global state\n    of pkg_resources. It is intended to be invoked once at\n    the initialization of this module.\n\n    Invocation by other packages is unsupported and done\n    at their own risk.\n    \"\"\"\n    working_set = WorkingSet._build_master()\n    _declare_state('object', working_set=working_set)\n\n    require = working_set.require\n    iter_entry_points = working_set.iter_entry_points\n    add_activation_listener = working_set.subscribe\n    run_script = working_set.run_script\n    # backward compatibility\n    run_main = run_script\n    # Activate all distributions already on sys.path with replace=False and\n    # ensure that all distributions added to the working set in the future\n    # (e.g. by calling ``require()``) will get activated as well,\n    # with higher priority (replace=True).\n    tuple(\n        dist.activate(replace=False)\n        for dist in working_set\n    )\n    add_activation_listener(\n        lambda dist: dist.activate(replace=True),\n        existing=False,\n    )\n    working_set.entries = []\n    # match order\n    list(map(working_set.add_entry, sys.path))\n    globals().update(locals())\n\n\nclass PkgResourcesDeprecationWarning(Warning):\n    \"\"\"\n    Base class for warning about deprecations in ``pkg_resources``\n\n    This class is not derived from ``DeprecationWarning``, and as such is\n    visible by default.\n    \"\"\"\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pkg_resources/__init__.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pkg_resources/__init__.py	(date 1602088701589)
@@ -1,4 +1,3 @@
-# coding: utf-8
 """
 Package resource API
 --------------------
@@ -15,8 +14,6 @@
 method.
 """
 
-from __future__ import absolute_import
-
 import sys
 import os
 import io
@@ -54,9 +51,6 @@
 except NameError:
     FileExistsError = OSError
 
-from pkg_resources.extern import six
-from pkg_resources.extern.six.moves import map, filter
-
 # capture these to bypass sandboxing
 from os import utime
 try:
@@ -83,18 +77,9 @@
 __import__('pkg_resources.extern.packaging.requirements')
 __import__('pkg_resources.extern.packaging.markers')
 
-
-__metaclass__ = type
-
-
-if (3, 0) < sys.version_info < (3, 5):
+if sys.version_info < (3, 5):
     raise RuntimeError("Python 3.5 or later is required")
 
-if six.PY2:
-    # Those builtin exceptions are only defined in Python 3
-    PermissionError = None
-    NotADirectoryError = None
-
 # declare some globals that will be defined later to
 # satisfy the linters.
 require = None
@@ -474,7 +459,7 @@
 
 def get_distribution(dist):
     """Return a current distribution object for a Requirement or string"""
-    if isinstance(dist, six.string_types):
+    if isinstance(dist, str):
         dist = Requirement.parse(dist)
     if isinstance(dist, Requirement):
         dist = get_provider(dist)
@@ -1418,8 +1403,6 @@
             return ""
         path = self._get_metadata_path(name)
         value = self._get(path)
-        if six.PY2:
-            return value
         try:
             return value.decode('utf-8')
         except UnicodeDecodeError as exc:
@@ -1910,8 +1893,7 @@
         return metadata
 
     def _warn_on_replacement(self, metadata):
-        # Python 2.7 compat for: replacement_char = '�'
-        replacement_char = b'\xef\xbf\xbd'.decode('utf-8')
+        replacement_char = '�'
         if replacement_char in metadata:
             tmpl = "{self.path} could not be properly decoded in UTF-8"
             msg = tmpl.format(**locals())
@@ -2056,7 +2038,10 @@
         )
         return
 
-    entries = safe_listdir(path_item)
+    entries = (
+        os.path.join(path_item, child)
+        for child in safe_listdir(path_item)
+    )
 
     # for performance, before sorting by version,
     # screen entries for only those that will yield
@@ -2106,8 +2091,6 @@
     """
     def __bool__(self):
         return False
-    if six.PY2:
-        __nonzero__ = __bool__
 
     def __call__(self, fullpath):
         return iter(())
@@ -2124,12 +2107,7 @@
     except OSError as e:
         # Ignore the directory if does not exist, not a directory or
         # permission denied
-        ignorable = (
-            e.errno in (errno.ENOTDIR, errno.EACCES, errno.ENOENT)
-            # Python 2 on Windows needs to be handled this way :(
-            or getattr(e, "winerror", None) == 267
-        )
-        if not ignorable:
+        if e.errno not in (errno.ENOTDIR, errno.EACCES, errno.ENOENT):
             raise
     return ()
 
@@ -2372,7 +2350,15 @@
     """
     Determine if given path appears to be an egg.
     """
-    return path.lower().endswith('.egg')
+    return _is_zip_egg(path) or _is_unpacked_egg(path)
+
+
+def _is_zip_egg(path):
+    return (
+        path.lower().endswith('.egg') and
+        os.path.isfile(path) and
+        zipfile.is_zipfile(path)
+    )
 
 
 def _is_unpacked_egg(path):
@@ -2380,7 +2366,7 @@
     Determine if given path appears to be an unpacked egg.
     """
     return (
-        _is_egg_path(path) and
+        path.lower().endswith('.egg') and
         os.path.isfile(os.path.join(path, 'EGG-INFO', 'PKG-INFO'))
     )
 
@@ -2395,7 +2381,7 @@
 
 def yield_lines(strs):
     """Yield non-empty/non-comment lines of a string or sequence"""
-    if isinstance(strs, six.string_types):
+    if isinstance(strs, str):
         for s in strs.splitlines():
             s = s.strip()
             # skip blank lines/comments
@@ -2833,10 +2819,6 @@
             )
         )
 
-    if not hasattr(object, '__dir__'):
-        # python 2.7 not supported
-        del __dir__
-
     @classmethod
     def from_filename(cls, filename, metadata=None, **kw):
         return cls.from_location(
Index: env/lib/python3.8/site-packages/setuptools/command/py36compat.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nfrom glob import glob\nfrom distutils.util import convert_path\nfrom distutils.command import sdist\n\nfrom setuptools.extern.six.moves import filter\n\n\nclass sdist_add_defaults:\n    \"\"\"\n    Mix-in providing forward-compatibility for functionality as found in\n    distutils on Python 3.7.\n\n    Do not edit the code in this class except to update functionality\n    as implemented in distutils. Instead, override in the subclass.\n    \"\"\"\n\n    def add_defaults(self):\n        \"\"\"Add all the default files to self.filelist:\n          - README or README.txt\n          - setup.py\n          - test/test*.py\n          - all pure Python modules mentioned in setup script\n          - all files pointed by package_data (build_py)\n          - all files defined in data_files.\n          - all files defined as scripts.\n          - all C sources listed as part of extensions or C libraries\n            in the setup script (doesn't catch C headers!)\n        Warns if (README or README.txt) or setup.py are missing; everything\n        else is optional.\n        \"\"\"\n        self._add_defaults_standards()\n        self._add_defaults_optional()\n        self._add_defaults_python()\n        self._add_defaults_data_files()\n        self._add_defaults_ext()\n        self._add_defaults_c_libs()\n        self._add_defaults_scripts()\n\n    @staticmethod\n    def _cs_path_exists(fspath):\n        \"\"\"\n        Case-sensitive path existence check\n\n        >>> sdist_add_defaults._cs_path_exists(__file__)\n        True\n        >>> sdist_add_defaults._cs_path_exists(__file__.upper())\n        False\n        \"\"\"\n        if not os.path.exists(fspath):\n            return False\n        # make absolute so we always have a directory\n        abspath = os.path.abspath(fspath)\n        directory, filename = os.path.split(abspath)\n        return filename in os.listdir(directory)\n\n    def _add_defaults_standards(self):\n        standards = [self.READMES, self.distribution.script_name]\n        for fn in standards:\n            if isinstance(fn, tuple):\n                alts = fn\n                got_it = False\n                for fn in alts:\n                    if self._cs_path_exists(fn):\n                        got_it = True\n                        self.filelist.append(fn)\n                        break\n\n                if not got_it:\n                    self.warn(\"standard file not found: should have one of \" +\n                              ', '.join(alts))\n            else:\n                if self._cs_path_exists(fn):\n                    self.filelist.append(fn)\n                else:\n                    self.warn(\"standard file '%s' not found\" % fn)\n\n    def _add_defaults_optional(self):\n        optional = ['test/test*.py', 'setup.cfg']\n        for pattern in optional:\n            files = filter(os.path.isfile, glob(pattern))\n            self.filelist.extend(files)\n\n    def _add_defaults_python(self):\n        # build_py is used to get:\n        #  - python modules\n        #  - files defined in package_data\n        build_py = self.get_finalized_command('build_py')\n\n        # getting python files\n        if self.distribution.has_pure_modules():\n            self.filelist.extend(build_py.get_source_files())\n\n        # getting package_data files\n        # (computed in build_py.data_files by build_py.finalize_options)\n        for pkg, src_dir, build_dir, filenames in build_py.data_files:\n            for filename in filenames:\n                self.filelist.append(os.path.join(src_dir, filename))\n\n    def _add_defaults_data_files(self):\n        # getting distribution.data_files\n        if self.distribution.has_data_files():\n            for item in self.distribution.data_files:\n                if isinstance(item, str):\n                    # plain file\n                    item = convert_path(item)\n                    if os.path.isfile(item):\n                        self.filelist.append(item)\n                else:\n                    # a (dirname, filenames) tuple\n                    dirname, filenames = item\n                    for f in filenames:\n                        f = convert_path(f)\n                        if os.path.isfile(f):\n                            self.filelist.append(f)\n\n    def _add_defaults_ext(self):\n        if self.distribution.has_ext_modules():\n            build_ext = self.get_finalized_command('build_ext')\n            self.filelist.extend(build_ext.get_source_files())\n\n    def _add_defaults_c_libs(self):\n        if self.distribution.has_c_libraries():\n            build_clib = self.get_finalized_command('build_clib')\n            self.filelist.extend(build_clib.get_source_files())\n\n    def _add_defaults_scripts(self):\n        if self.distribution.has_scripts():\n            build_scripts = self.get_finalized_command('build_scripts')\n            self.filelist.extend(build_scripts.get_source_files())\n\n\nif hasattr(sdist.sdist, '_add_defaults_standards'):\n    # disable the functionality already available upstream\n    class sdist_add_defaults:  # noqa\n        pass\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/command/py36compat.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/command/py36compat.py	(date 1602088701625)
@@ -3,8 +3,6 @@
 from distutils.util import convert_path
 from distutils.command import sdist
 
-from setuptools.extern.six.moves import filter
-
 
 class sdist_add_defaults:
     """
Index: env/lib/python3.8/site-packages/setuptools/command/egg_info.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"setuptools.command.egg_info\n\nCreate a distribution's .egg-info directory and contents\"\"\"\n\nfrom distutils.filelist import FileList as _FileList\nfrom distutils.errors import DistutilsInternalError\nfrom distutils.util import convert_path\nfrom distutils import log\nimport distutils.errors\nimport distutils.filelist\nimport os\nimport re\nimport sys\nimport io\nimport warnings\nimport time\nimport collections\n\nfrom setuptools.extern import six\nfrom setuptools.extern.six.moves import map\n\nfrom setuptools import Command\nfrom setuptools.command.sdist import sdist\nfrom setuptools.command.sdist import walk_revctrl\nfrom setuptools.command.setopt import edit_config\nfrom setuptools.command import bdist_egg\nfrom pkg_resources import (\n    parse_requirements, safe_name, parse_version,\n    safe_version, yield_lines, EntryPoint, iter_entry_points, to_filename)\nimport setuptools.unicode_utils as unicode_utils\nfrom setuptools.glob import glob\n\nfrom setuptools.extern import packaging\nfrom setuptools import SetuptoolsDeprecationWarning\n\n\ndef translate_pattern(glob):\n    \"\"\"\n    Translate a file path glob like '*.txt' in to a regular expression.\n    This differs from fnmatch.translate which allows wildcards to match\n    directory separators. It also knows about '**/' which matches any number of\n    directories.\n    \"\"\"\n    pat = ''\n\n    # This will split on '/' within [character classes]. This is deliberate.\n    chunks = glob.split(os.path.sep)\n\n    sep = re.escape(os.sep)\n    valid_char = '[^%s]' % (sep,)\n\n    for c, chunk in enumerate(chunks):\n        last_chunk = c == len(chunks) - 1\n\n        # Chunks that are a literal ** are globstars. They match anything.\n        if chunk == '**':\n            if last_chunk:\n                # Match anything if this is the last component\n                pat += '.*'\n            else:\n                # Match '(name/)*'\n                pat += '(?:%s+%s)*' % (valid_char, sep)\n            continue  # Break here as the whole path component has been handled\n\n        # Find any special characters in the remainder\n        i = 0\n        chunk_len = len(chunk)\n        while i < chunk_len:\n            char = chunk[i]\n            if char == '*':\n                # Match any number of name characters\n                pat += valid_char + '*'\n            elif char == '?':\n                # Match a name character\n                pat += valid_char\n            elif char == '[':\n                # Character class\n                inner_i = i + 1\n                # Skip initial !/] chars\n                if inner_i < chunk_len and chunk[inner_i] == '!':\n                    inner_i = inner_i + 1\n                if inner_i < chunk_len and chunk[inner_i] == ']':\n                    inner_i = inner_i + 1\n\n                # Loop till the closing ] is found\n                while inner_i < chunk_len and chunk[inner_i] != ']':\n                    inner_i = inner_i + 1\n\n                if inner_i >= chunk_len:\n                    # Got to the end of the string without finding a closing ]\n                    # Do not treat this as a matching group, but as a literal [\n                    pat += re.escape(char)\n                else:\n                    # Grab the insides of the [brackets]\n                    inner = chunk[i + 1:inner_i]\n                    char_class = ''\n\n                    # Class negation\n                    if inner[0] == '!':\n                        char_class = '^'\n                        inner = inner[1:]\n\n                    char_class += re.escape(inner)\n                    pat += '[%s]' % (char_class,)\n\n                    # Skip to the end ]\n                    i = inner_i\n            else:\n                pat += re.escape(char)\n            i += 1\n\n        # Join each chunk with the dir separator\n        if not last_chunk:\n            pat += sep\n\n    pat += r'\\Z'\n    return re.compile(pat, flags=re.MULTILINE | re.DOTALL)\n\n\nclass InfoCommon:\n    tag_build = None\n    tag_date = None\n\n    @property\n    def name(self):\n        return safe_name(self.distribution.get_name())\n\n    def tagged_version(self):\n        version = self.distribution.get_version()\n        # egg_info may be called more than once for a distribution,\n        # in which case the version string already contains all tags.\n        if self.vtags and version.endswith(self.vtags):\n            return safe_version(version)\n        return safe_version(version + self.vtags)\n\n    def tags(self):\n        version = ''\n        if self.tag_build:\n            version += self.tag_build\n        if self.tag_date:\n            version += time.strftime(\"-%Y%m%d\")\n        return version\n    vtags = property(tags)\n\n\nclass egg_info(InfoCommon, Command):\n    description = \"create a distribution's .egg-info directory\"\n\n    user_options = [\n        ('egg-base=', 'e', \"directory containing .egg-info directories\"\n                           \" (default: top of the source tree)\"),\n        ('tag-date', 'd', \"Add date stamp (e.g. 20050528) to version number\"),\n        ('tag-build=', 'b', \"Specify explicit tag to add to version number\"),\n        ('no-date', 'D', \"Don't include date stamp [default]\"),\n    ]\n\n    boolean_options = ['tag-date']\n    negative_opt = {\n        'no-date': 'tag-date',\n    }\n\n    def initialize_options(self):\n        self.egg_base = None\n        self.egg_name = None\n        self.egg_info = None\n        self.egg_version = None\n        self.broken_egg_info = False\n\n    ####################################\n    # allow the 'tag_svn_revision' to be detected and\n    # set, supporting sdists built on older Setuptools.\n    @property\n    def tag_svn_revision(self):\n        pass\n\n    @tag_svn_revision.setter\n    def tag_svn_revision(self, value):\n        pass\n    ####################################\n\n    def save_version_info(self, filename):\n        \"\"\"\n        Materialize the value of date into the\n        build tag. Install build keys in a deterministic order\n        to avoid arbitrary reordering on subsequent builds.\n        \"\"\"\n        egg_info = collections.OrderedDict()\n        # follow the order these keys would have been added\n        # when PYTHONHASHSEED=0\n        egg_info['tag_build'] = self.tags()\n        egg_info['tag_date'] = 0\n        edit_config(filename, dict(egg_info=egg_info))\n\n    def finalize_options(self):\n        # Note: we need to capture the current value returned\n        # by `self.tagged_version()`, so we can later update\n        # `self.distribution.metadata.version` without\n        # repercussions.\n        self.egg_name = self.name\n        self.egg_version = self.tagged_version()\n        parsed_version = parse_version(self.egg_version)\n\n        try:\n            is_version = isinstance(parsed_version, packaging.version.Version)\n            spec = (\n                \"%s==%s\" if is_version else \"%s===%s\"\n            )\n            list(\n                parse_requirements(spec % (self.egg_name, self.egg_version))\n            )\n        except ValueError as e:\n            raise distutils.errors.DistutilsOptionError(\n                \"Invalid distribution name or version syntax: %s-%s\" %\n                (self.egg_name, self.egg_version)\n            ) from e\n\n        if self.egg_base is None:\n            dirs = self.distribution.package_dir\n            self.egg_base = (dirs or {}).get('', os.curdir)\n\n        self.ensure_dirname('egg_base')\n        self.egg_info = to_filename(self.egg_name) + '.egg-info'\n        if self.egg_base != os.curdir:\n            self.egg_info = os.path.join(self.egg_base, self.egg_info)\n        if '-' in self.egg_name:\n            self.check_broken_egg_info()\n\n        # Set package version for the benefit of dumber commands\n        # (e.g. sdist, bdist_wininst, etc.)\n        #\n        self.distribution.metadata.version = self.egg_version\n\n        # If we bootstrapped around the lack of a PKG-INFO, as might be the\n        # case in a fresh checkout, make sure that any special tags get added\n        # to the version info\n        #\n        pd = self.distribution._patched_dist\n        if pd is not None and pd.key == self.egg_name.lower():\n            pd._version = self.egg_version\n            pd._parsed_version = parse_version(self.egg_version)\n            self.distribution._patched_dist = None\n\n    def write_or_delete_file(self, what, filename, data, force=False):\n        \"\"\"Write `data` to `filename` or delete if empty\n\n        If `data` is non-empty, this routine is the same as ``write_file()``.\n        If `data` is empty but not ``None``, this is the same as calling\n        ``delete_file(filename)`.  If `data` is ``None``, then this is a no-op\n        unless `filename` exists, in which case a warning is issued about the\n        orphaned file (if `force` is false), or deleted (if `force` is true).\n        \"\"\"\n        if data:\n            self.write_file(what, filename, data)\n        elif os.path.exists(filename):\n            if data is None and not force:\n                log.warn(\n                    \"%s not set in setup(), but %s exists\", what, filename\n                )\n                return\n            else:\n                self.delete_file(filename)\n\n    def write_file(self, what, filename, data):\n        \"\"\"Write `data` to `filename` (if not a dry run) after announcing it\n\n        `what` is used in a log message to identify what is being written\n        to the file.\n        \"\"\"\n        log.info(\"writing %s to %s\", what, filename)\n        if not six.PY2:\n            data = data.encode(\"utf-8\")\n        if not self.dry_run:\n            f = open(filename, 'wb')\n            f.write(data)\n            f.close()\n\n    def delete_file(self, filename):\n        \"\"\"Delete `filename` (if not a dry run) after announcing it\"\"\"\n        log.info(\"deleting %s\", filename)\n        if not self.dry_run:\n            os.unlink(filename)\n\n    def run(self):\n        self.mkpath(self.egg_info)\n        os.utime(self.egg_info, None)\n        installer = self.distribution.fetch_build_egg\n        for ep in iter_entry_points('egg_info.writers'):\n            ep.require(installer=installer)\n            writer = ep.resolve()\n            writer(self, ep.name, os.path.join(self.egg_info, ep.name))\n\n        # Get rid of native_libs.txt if it was put there by older bdist_egg\n        nl = os.path.join(self.egg_info, \"native_libs.txt\")\n        if os.path.exists(nl):\n            self.delete_file(nl)\n\n        self.find_sources()\n\n    def find_sources(self):\n        \"\"\"Generate SOURCES.txt manifest file\"\"\"\n        manifest_filename = os.path.join(self.egg_info, \"SOURCES.txt\")\n        mm = manifest_maker(self.distribution)\n        mm.manifest = manifest_filename\n        mm.run()\n        self.filelist = mm.filelist\n\n    def check_broken_egg_info(self):\n        bei = self.egg_name + '.egg-info'\n        if self.egg_base != os.curdir:\n            bei = os.path.join(self.egg_base, bei)\n        if os.path.exists(bei):\n            log.warn(\n                \"-\" * 78 + '\\n'\n                \"Note: Your current .egg-info directory has a '-' in its name;\"\n                '\\nthis will not work correctly with \"setup.py develop\".\\n\\n'\n                'Please rename %s to %s to correct this problem.\\n' + '-' * 78,\n                bei, self.egg_info\n            )\n            self.broken_egg_info = self.egg_info\n            self.egg_info = bei  # make it work for now\n\n\nclass FileList(_FileList):\n    # Implementations of the various MANIFEST.in commands\n\n    def process_template_line(self, line):\n        # Parse the line: split it up, make sure the right number of words\n        # is there, and return the relevant words.  'action' is always\n        # defined: it's the first word of the line.  Which of the other\n        # three are defined depends on the action; it'll be either\n        # patterns, (dir and patterns), or (dir_pattern).\n        (action, patterns, dir, dir_pattern) = self._parse_template_line(line)\n\n        # OK, now we know that the action is valid and we have the\n        # right number of words on the line for that action -- so we\n        # can proceed with minimal error-checking.\n        if action == 'include':\n            self.debug_print(\"include \" + ' '.join(patterns))\n            for pattern in patterns:\n                if not self.include(pattern):\n                    log.warn(\"warning: no files found matching '%s'\", pattern)\n\n        elif action == 'exclude':\n            self.debug_print(\"exclude \" + ' '.join(patterns))\n            for pattern in patterns:\n                if not self.exclude(pattern):\n                    log.warn((\"warning: no previously-included files \"\n                              \"found matching '%s'\"), pattern)\n\n        elif action == 'global-include':\n            self.debug_print(\"global-include \" + ' '.join(patterns))\n            for pattern in patterns:\n                if not self.global_include(pattern):\n                    log.warn((\"warning: no files found matching '%s' \"\n                              \"anywhere in distribution\"), pattern)\n\n        elif action == 'global-exclude':\n            self.debug_print(\"global-exclude \" + ' '.join(patterns))\n            for pattern in patterns:\n                if not self.global_exclude(pattern):\n                    log.warn((\"warning: no previously-included files matching \"\n                              \"'%s' found anywhere in distribution\"),\n                             pattern)\n\n        elif action == 'recursive-include':\n            self.debug_print(\"recursive-include %s %s\" %\n                             (dir, ' '.join(patterns)))\n            for pattern in patterns:\n                if not self.recursive_include(dir, pattern):\n                    log.warn((\"warning: no files found matching '%s' \"\n                              \"under directory '%s'\"),\n                             pattern, dir)\n\n        elif action == 'recursive-exclude':\n            self.debug_print(\"recursive-exclude %s %s\" %\n                             (dir, ' '.join(patterns)))\n            for pattern in patterns:\n                if not self.recursive_exclude(dir, pattern):\n                    log.warn((\"warning: no previously-included files matching \"\n                              \"'%s' found under directory '%s'\"),\n                             pattern, dir)\n\n        elif action == 'graft':\n            self.debug_print(\"graft \" + dir_pattern)\n            if not self.graft(dir_pattern):\n                log.warn(\"warning: no directories found matching '%s'\",\n                         dir_pattern)\n\n        elif action == 'prune':\n            self.debug_print(\"prune \" + dir_pattern)\n            if not self.prune(dir_pattern):\n                log.warn((\"no previously-included directories found \"\n                          \"matching '%s'\"), dir_pattern)\n\n        else:\n            raise DistutilsInternalError(\n                \"this cannot happen: invalid action '%s'\" % action)\n\n    def _remove_files(self, predicate):\n        \"\"\"\n        Remove all files from the file list that match the predicate.\n        Return True if any matching files were removed\n        \"\"\"\n        found = False\n        for i in range(len(self.files) - 1, -1, -1):\n            if predicate(self.files[i]):\n                self.debug_print(\" removing \" + self.files[i])\n                del self.files[i]\n                found = True\n        return found\n\n    def include(self, pattern):\n        \"\"\"Include files that match 'pattern'.\"\"\"\n        found = [f for f in glob(pattern) if not os.path.isdir(f)]\n        self.extend(found)\n        return bool(found)\n\n    def exclude(self, pattern):\n        \"\"\"Exclude files that match 'pattern'.\"\"\"\n        match = translate_pattern(pattern)\n        return self._remove_files(match.match)\n\n    def recursive_include(self, dir, pattern):\n        \"\"\"\n        Include all files anywhere in 'dir/' that match the pattern.\n        \"\"\"\n        full_pattern = os.path.join(dir, '**', pattern)\n        found = [f for f in glob(full_pattern, recursive=True)\n                 if not os.path.isdir(f)]\n        self.extend(found)\n        return bool(found)\n\n    def recursive_exclude(self, dir, pattern):\n        \"\"\"\n        Exclude any file anywhere in 'dir/' that match the pattern.\n        \"\"\"\n        match = translate_pattern(os.path.join(dir, '**', pattern))\n        return self._remove_files(match.match)\n\n    def graft(self, dir):\n        \"\"\"Include all files from 'dir/'.\"\"\"\n        found = [\n            item\n            for match_dir in glob(dir)\n            for item in distutils.filelist.findall(match_dir)\n        ]\n        self.extend(found)\n        return bool(found)\n\n    def prune(self, dir):\n        \"\"\"Filter out files from 'dir/'.\"\"\"\n        match = translate_pattern(os.path.join(dir, '**'))\n        return self._remove_files(match.match)\n\n    def global_include(self, pattern):\n        \"\"\"\n        Include all files anywhere in the current directory that match the\n        pattern. This is very inefficient on large file trees.\n        \"\"\"\n        if self.allfiles is None:\n            self.findall()\n        match = translate_pattern(os.path.join('**', pattern))\n        found = [f for f in self.allfiles if match.match(f)]\n        self.extend(found)\n        return bool(found)\n\n    def global_exclude(self, pattern):\n        \"\"\"\n        Exclude all files anywhere that match the pattern.\n        \"\"\"\n        match = translate_pattern(os.path.join('**', pattern))\n        return self._remove_files(match.match)\n\n    def append(self, item):\n        if item.endswith('\\r'):  # Fix older sdists built on Windows\n            item = item[:-1]\n        path = convert_path(item)\n\n        if self._safe_path(path):\n            self.files.append(path)\n\n    def extend(self, paths):\n        self.files.extend(filter(self._safe_path, paths))\n\n    def _repair(self):\n        \"\"\"\n        Replace self.files with only safe paths\n\n        Because some owners of FileList manipulate the underlying\n        ``files`` attribute directly, this method must be called to\n        repair those paths.\n        \"\"\"\n        self.files = list(filter(self._safe_path, self.files))\n\n    def _safe_path(self, path):\n        enc_warn = \"'%s' not %s encodable -- skipping\"\n\n        # To avoid accidental trans-codings errors, first to unicode\n        u_path = unicode_utils.filesys_decode(path)\n        if u_path is None:\n            log.warn(\"'%s' in unexpected encoding -- skipping\" % path)\n            return False\n\n        # Must ensure utf-8 encodability\n        utf8_path = unicode_utils.try_encode(u_path, \"utf-8\")\n        if utf8_path is None:\n            log.warn(enc_warn, path, 'utf-8')\n            return False\n\n        try:\n            # accept is either way checks out\n            if os.path.exists(u_path) or os.path.exists(utf8_path):\n                return True\n        # this will catch any encode errors decoding u_path\n        except UnicodeEncodeError:\n            log.warn(enc_warn, path, sys.getfilesystemencoding())\n\n\nclass manifest_maker(sdist):\n    template = \"MANIFEST.in\"\n\n    def initialize_options(self):\n        self.use_defaults = 1\n        self.prune = 1\n        self.manifest_only = 1\n        self.force_manifest = 1\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        self.filelist = FileList()\n        if not os.path.exists(self.manifest):\n            self.write_manifest()  # it must exist so it'll get in the list\n        self.add_defaults()\n        if os.path.exists(self.template):\n            self.read_template()\n        self.prune_file_list()\n        self.filelist.sort()\n        self.filelist.remove_duplicates()\n        self.write_manifest()\n\n    def _manifest_normalize(self, path):\n        path = unicode_utils.filesys_decode(path)\n        return path.replace(os.sep, '/')\n\n    def write_manifest(self):\n        \"\"\"\n        Write the file list in 'self.filelist' to the manifest file\n        named by 'self.manifest'.\n        \"\"\"\n        self.filelist._repair()\n\n        # Now _repairs should encodability, but not unicode\n        files = [self._manifest_normalize(f) for f in self.filelist.files]\n        msg = \"writing manifest file '%s'\" % self.manifest\n        self.execute(write_file, (self.manifest, files), msg)\n\n    def warn(self, msg):\n        if not self._should_suppress_warning(msg):\n            sdist.warn(self, msg)\n\n    @staticmethod\n    def _should_suppress_warning(msg):\n        \"\"\"\n        suppress missing-file warnings from sdist\n        \"\"\"\n        return re.match(r\"standard file .*not found\", msg)\n\n    def add_defaults(self):\n        sdist.add_defaults(self)\n        self.check_license()\n        self.filelist.append(self.template)\n        self.filelist.append(self.manifest)\n        rcfiles = list(walk_revctrl())\n        if rcfiles:\n            self.filelist.extend(rcfiles)\n        elif os.path.exists(self.manifest):\n            self.read_manifest()\n\n        if os.path.exists(\"setup.py\"):\n            # setup.py should be included by default, even if it's not\n            # the script called to create the sdist\n            self.filelist.append(\"setup.py\")\n\n        ei_cmd = self.get_finalized_command('egg_info')\n        self.filelist.graft(ei_cmd.egg_info)\n\n    def prune_file_list(self):\n        build = self.get_finalized_command('build')\n        base_dir = self.distribution.get_fullname()\n        self.filelist.prune(build.build_base)\n        self.filelist.prune(base_dir)\n        sep = re.escape(os.sep)\n        self.filelist.exclude_pattern(r'(^|' + sep + r')(RCS|CVS|\\.svn)' + sep,\n                                      is_regex=1)\n\n\ndef write_file(filename, contents):\n    \"\"\"Create a file with the specified name and write 'contents' (a\n    sequence of strings without line terminators) to it.\n    \"\"\"\n    contents = \"\\n\".join(contents)\n\n    # assuming the contents has been vetted for utf-8 encoding\n    contents = contents.encode(\"utf-8\")\n\n    with open(filename, \"wb\") as f:  # always write POSIX-style manifest\n        f.write(contents)\n\n\ndef write_pkg_info(cmd, basename, filename):\n    log.info(\"writing %s\", filename)\n    if not cmd.dry_run:\n        metadata = cmd.distribution.metadata\n        metadata.version, oldver = cmd.egg_version, metadata.version\n        metadata.name, oldname = cmd.egg_name, metadata.name\n\n        try:\n            # write unescaped data to PKG-INFO, so older pkg_resources\n            # can still parse it\n            metadata.write_pkg_info(cmd.egg_info)\n        finally:\n            metadata.name, metadata.version = oldname, oldver\n\n        safe = getattr(cmd.distribution, 'zip_safe', None)\n\n        bdist_egg.write_safety_flag(cmd.egg_info, safe)\n\n\ndef warn_depends_obsolete(cmd, basename, filename):\n    if os.path.exists(filename):\n        log.warn(\n            \"WARNING: 'depends.txt' is not used by setuptools 0.6!\\n\"\n            \"Use the install_requires/extras_require setup() args instead.\"\n        )\n\n\ndef _write_requirements(stream, reqs):\n    lines = yield_lines(reqs or ())\n\n    def append_cr(line):\n        return line + '\\n'\n    lines = map(append_cr, lines)\n    stream.writelines(lines)\n\n\ndef write_requirements(cmd, basename, filename):\n    dist = cmd.distribution\n    data = six.StringIO()\n    _write_requirements(data, dist.install_requires)\n    extras_require = dist.extras_require or {}\n    for extra in sorted(extras_require):\n        data.write('\\n[{extra}]\\n'.format(**vars()))\n        _write_requirements(data, extras_require[extra])\n    cmd.write_or_delete_file(\"requirements\", filename, data.getvalue())\n\n\ndef write_setup_requirements(cmd, basename, filename):\n    data = io.StringIO()\n    _write_requirements(data, cmd.distribution.setup_requires)\n    cmd.write_or_delete_file(\"setup-requirements\", filename, data.getvalue())\n\n\ndef write_toplevel_names(cmd, basename, filename):\n    pkgs = dict.fromkeys(\n        [\n            k.split('.', 1)[0]\n            for k in cmd.distribution.iter_distribution_names()\n        ]\n    )\n    cmd.write_file(\"top-level names\", filename, '\\n'.join(sorted(pkgs)) + '\\n')\n\n\ndef overwrite_arg(cmd, basename, filename):\n    write_arg(cmd, basename, filename, True)\n\n\ndef write_arg(cmd, basename, filename, force=False):\n    argname = os.path.splitext(basename)[0]\n    value = getattr(cmd.distribution, argname, None)\n    if value is not None:\n        value = '\\n'.join(value) + '\\n'\n    cmd.write_or_delete_file(argname, filename, value, force)\n\n\ndef write_entries(cmd, basename, filename):\n    ep = cmd.distribution.entry_points\n\n    if isinstance(ep, six.string_types) or ep is None:\n        data = ep\n    elif ep is not None:\n        data = []\n        for section, contents in sorted(ep.items()):\n            if not isinstance(contents, six.string_types):\n                contents = EntryPoint.parse_group(section, contents)\n                contents = '\\n'.join(sorted(map(str, contents.values())))\n            data.append('[%s]\\n%s\\n\\n' % (section, contents))\n        data = ''.join(data)\n\n    cmd.write_or_delete_file('entry points', filename, data, True)\n\n\ndef get_pkg_info_revision():\n    \"\"\"\n    Get a -r### off of PKG-INFO Version in case this is an sdist of\n    a subversion revision.\n    \"\"\"\n    warnings.warn(\n        \"get_pkg_info_revision is deprecated.\", EggInfoDeprecationWarning)\n    if os.path.exists('PKG-INFO'):\n        with io.open('PKG-INFO') as f:\n            for line in f:\n                match = re.match(r\"Version:.*-r(\\d+)\\s*$\", line)\n                if match:\n                    return int(match.group(1))\n    return 0\n\n\nclass EggInfoDeprecationWarning(SetuptoolsDeprecationWarning):\n    \"\"\"Deprecated behavior warning for EggInfo, bypassing suppression.\"\"\"\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/command/egg_info.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/command/egg_info.py	(date 1602088701621)
@@ -16,9 +16,6 @@
 import time
 import collections
 
-from setuptools.extern import six
-from setuptools.extern.six.moves import map
-
 from setuptools import Command
 from setuptools.command.sdist import sdist
 from setuptools.command.sdist import walk_revctrl
@@ -267,8 +264,7 @@
         to the file.
         """
         log.info("writing %s to %s", what, filename)
-        if not six.PY2:
-            data = data.encode("utf-8")
+        data = data.encode("utf-8")
         if not self.dry_run:
             f = open(filename, 'wb')
             f.write(data)
@@ -647,7 +643,7 @@
 
 def write_requirements(cmd, basename, filename):
     dist = cmd.distribution
-    data = six.StringIO()
+    data = io.StringIO()
     _write_requirements(data, dist.install_requires)
     extras_require = dist.extras_require or {}
     for extra in sorted(extras_require):
@@ -687,12 +683,12 @@
 def write_entries(cmd, basename, filename):
     ep = cmd.distribution.entry_points
 
-    if isinstance(ep, six.string_types) or ep is None:
+    if isinstance(ep, str) or ep is None:
         data = ep
     elif ep is not None:
         data = []
         for section, contents in sorted(ep.items()):
-            if not isinstance(contents, six.string_types):
+            if not isinstance(contents, str):
                 contents = EntryPoint.parse_group(section, contents)
                 contents = '\n'.join(sorted(map(str, contents.values())))
             data.append('[%s]\n%s\n\n' % (section, contents))
Index: env/lib/python3.8/site-packages/setuptools/command/build_ext.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport sys\nimport itertools\nfrom distutils.command.build_ext import build_ext as _du_build_ext\nfrom distutils.file_util import copy_file\nfrom distutils.ccompiler import new_compiler\nfrom distutils.sysconfig import customize_compiler, get_config_var\nfrom distutils.errors import DistutilsError\nfrom distutils import log\n\nfrom setuptools.extension import Library\nfrom setuptools.extern import six\n\nif six.PY2:\n    import imp\n\n    EXTENSION_SUFFIXES = [\n        s for s, _, tp in imp.get_suffixes() if tp == imp.C_EXTENSION]\nelse:\n    from importlib.machinery import EXTENSION_SUFFIXES\n\ntry:\n    # Attempt to use Cython for building extensions, if available\n    from Cython.Distutils.build_ext import build_ext as _build_ext\n    # Additionally, assert that the compiler module will load\n    # also. Ref #1229.\n    __import__('Cython.Compiler.Main')\nexcept ImportError:\n    _build_ext = _du_build_ext\n\n# make sure _config_vars is initialized\nget_config_var(\"LDSHARED\")\nfrom distutils.sysconfig import _config_vars as _CONFIG_VARS  # noqa\n\n\ndef _customize_compiler_for_shlib(compiler):\n    if sys.platform == \"darwin\":\n        # building .dylib requires additional compiler flags on OSX; here we\n        # temporarily substitute the pyconfig.h variables so that distutils'\n        # 'customize_compiler' uses them before we build the shared libraries.\n        tmp = _CONFIG_VARS.copy()\n        try:\n            # XXX Help!  I don't have any idea whether these are right...\n            _CONFIG_VARS['LDSHARED'] = (\n                \"gcc -Wl,-x -dynamiclib -undefined dynamic_lookup\")\n            _CONFIG_VARS['CCSHARED'] = \" -dynamiclib\"\n            _CONFIG_VARS['SO'] = \".dylib\"\n            customize_compiler(compiler)\n        finally:\n            _CONFIG_VARS.clear()\n            _CONFIG_VARS.update(tmp)\n    else:\n        customize_compiler(compiler)\n\n\nhave_rtld = False\nuse_stubs = False\nlibtype = 'shared'\n\nif sys.platform == \"darwin\":\n    use_stubs = True\nelif os.name != 'nt':\n    try:\n        import dl\n        use_stubs = have_rtld = hasattr(dl, 'RTLD_NOW')\n    except ImportError:\n        pass\n\n\ndef if_dl(s):\n    return s if have_rtld else ''\n\n\ndef get_abi3_suffix():\n    \"\"\"Return the file extension for an abi3-compliant Extension()\"\"\"\n    for suffix in EXTENSION_SUFFIXES:\n        if '.abi3' in suffix:  # Unix\n            return suffix\n        elif suffix == '.pyd':  # Windows\n            return suffix\n\n\nclass build_ext(_build_ext):\n    def run(self):\n        \"\"\"Build extensions in build directory, then copy if --inplace\"\"\"\n        old_inplace, self.inplace = self.inplace, 0\n        _build_ext.run(self)\n        self.inplace = old_inplace\n        if old_inplace:\n            self.copy_extensions_to_source()\n\n    def copy_extensions_to_source(self):\n        build_py = self.get_finalized_command('build_py')\n        for ext in self.extensions:\n            fullname = self.get_ext_fullname(ext.name)\n            filename = self.get_ext_filename(fullname)\n            modpath = fullname.split('.')\n            package = '.'.join(modpath[:-1])\n            package_dir = build_py.get_package_dir(package)\n            dest_filename = os.path.join(package_dir,\n                                         os.path.basename(filename))\n            src_filename = os.path.join(self.build_lib, filename)\n\n            # Always copy, even if source is older than destination, to ensure\n            # that the right extensions for the current Python/platform are\n            # used.\n            copy_file(\n                src_filename, dest_filename, verbose=self.verbose,\n                dry_run=self.dry_run\n            )\n            if ext._needs_stub:\n                self.write_stub(package_dir or os.curdir, ext, True)\n\n    def get_ext_filename(self, fullname):\n        filename = _build_ext.get_ext_filename(self, fullname)\n        if fullname in self.ext_map:\n            ext = self.ext_map[fullname]\n            use_abi3 = (\n                not six.PY2\n                and getattr(ext, 'py_limited_api')\n                and get_abi3_suffix()\n            )\n            if use_abi3:\n                so_ext = get_config_var('EXT_SUFFIX')\n                filename = filename[:-len(so_ext)]\n                filename = filename + get_abi3_suffix()\n            if isinstance(ext, Library):\n                fn, ext = os.path.splitext(filename)\n                return self.shlib_compiler.library_filename(fn, libtype)\n            elif use_stubs and ext._links_to_dynamic:\n                d, fn = os.path.split(filename)\n                return os.path.join(d, 'dl-' + fn)\n        return filename\n\n    def initialize_options(self):\n        _build_ext.initialize_options(self)\n        self.shlib_compiler = None\n        self.shlibs = []\n        self.ext_map = {}\n\n    def finalize_options(self):\n        _build_ext.finalize_options(self)\n        self.extensions = self.extensions or []\n        self.check_extensions_list(self.extensions)\n        self.shlibs = [ext for ext in self.extensions\n                       if isinstance(ext, Library)]\n        if self.shlibs:\n            self.setup_shlib_compiler()\n        for ext in self.extensions:\n            ext._full_name = self.get_ext_fullname(ext.name)\n        for ext in self.extensions:\n            fullname = ext._full_name\n            self.ext_map[fullname] = ext\n\n            # distutils 3.1 will also ask for module names\n            # XXX what to do with conflicts?\n            self.ext_map[fullname.split('.')[-1]] = ext\n\n            ltd = self.shlibs and self.links_to_dynamic(ext) or False\n            ns = ltd and use_stubs and not isinstance(ext, Library)\n            ext._links_to_dynamic = ltd\n            ext._needs_stub = ns\n            filename = ext._file_name = self.get_ext_filename(fullname)\n            libdir = os.path.dirname(os.path.join(self.build_lib, filename))\n            if ltd and libdir not in ext.library_dirs:\n                ext.library_dirs.append(libdir)\n            if ltd and use_stubs and os.curdir not in ext.runtime_library_dirs:\n                ext.runtime_library_dirs.append(os.curdir)\n\n    def setup_shlib_compiler(self):\n        compiler = self.shlib_compiler = new_compiler(\n            compiler=self.compiler, dry_run=self.dry_run, force=self.force\n        )\n        _customize_compiler_for_shlib(compiler)\n\n        if self.include_dirs is not None:\n            compiler.set_include_dirs(self.include_dirs)\n        if self.define is not None:\n            # 'define' option is a list of (name,value) tuples\n            for (name, value) in self.define:\n                compiler.define_macro(name, value)\n        if self.undef is not None:\n            for macro in self.undef:\n                compiler.undefine_macro(macro)\n        if self.libraries is not None:\n            compiler.set_libraries(self.libraries)\n        if self.library_dirs is not None:\n            compiler.set_library_dirs(self.library_dirs)\n        if self.rpath is not None:\n            compiler.set_runtime_library_dirs(self.rpath)\n        if self.link_objects is not None:\n            compiler.set_link_objects(self.link_objects)\n\n        # hack so distutils' build_extension() builds a library instead\n        compiler.link_shared_object = link_shared_object.__get__(compiler)\n\n    def get_export_symbols(self, ext):\n        if isinstance(ext, Library):\n            return ext.export_symbols\n        return _build_ext.get_export_symbols(self, ext)\n\n    def build_extension(self, ext):\n        ext._convert_pyx_sources_to_lang()\n        _compiler = self.compiler\n        try:\n            if isinstance(ext, Library):\n                self.compiler = self.shlib_compiler\n            _build_ext.build_extension(self, ext)\n            if ext._needs_stub:\n                cmd = self.get_finalized_command('build_py').build_lib\n                self.write_stub(cmd, ext)\n        finally:\n            self.compiler = _compiler\n\n    def links_to_dynamic(self, ext):\n        \"\"\"Return true if 'ext' links to a dynamic lib in the same package\"\"\"\n        # XXX this should check to ensure the lib is actually being built\n        # XXX as dynamic, and not just using a locally-found version or a\n        # XXX static-compiled version\n        libnames = dict.fromkeys([lib._full_name for lib in self.shlibs])\n        pkg = '.'.join(ext._full_name.split('.')[:-1] + [''])\n        return any(pkg + libname in libnames for libname in ext.libraries)\n\n    def get_outputs(self):\n        return _build_ext.get_outputs(self) + self.__get_stubs_outputs()\n\n    def __get_stubs_outputs(self):\n        # assemble the base name for each extension that needs a stub\n        ns_ext_bases = (\n            os.path.join(self.build_lib, *ext._full_name.split('.'))\n            for ext in self.extensions\n            if ext._needs_stub\n        )\n        # pair each base with the extension\n        pairs = itertools.product(ns_ext_bases, self.__get_output_extensions())\n        return list(base + fnext for base, fnext in pairs)\n\n    def __get_output_extensions(self):\n        yield '.py'\n        yield '.pyc'\n        if self.get_finalized_command('build_py').optimize:\n            yield '.pyo'\n\n    def write_stub(self, output_dir, ext, compile=False):\n        log.info(\"writing stub loader for %s to %s\", ext._full_name,\n                 output_dir)\n        stub_file = (os.path.join(output_dir, *ext._full_name.split('.')) +\n                     '.py')\n        if compile and os.path.exists(stub_file):\n            raise DistutilsError(stub_file + \" already exists! Please delete.\")\n        if not self.dry_run:\n            f = open(stub_file, 'w')\n            f.write(\n                '\\n'.join([\n                    \"def __bootstrap__():\",\n                    \"   global __bootstrap__, __file__, __loader__\",\n                    \"   import sys, os, pkg_resources\" + if_dl(\", dl\"),\n                    \"   from importlib.machinery import ExtensionFileLoader\",\n                    \"   __file__ = pkg_resources.resource_filename\"\n                    \"(__name__,%r)\"\n                    % os.path.basename(ext._file_name),\n                    \"   del __bootstrap__\",\n                    \"   if '__loader__' in globals():\",\n                    \"       del __loader__\",\n                    if_dl(\"   old_flags = sys.getdlopenflags()\"),\n                    \"   old_dir = os.getcwd()\",\n                    \"   try:\",\n                    \"     os.chdir(os.path.dirname(__file__))\",\n                    if_dl(\"     sys.setdlopenflags(dl.RTLD_NOW)\"),\n                    \"     ExtensionFileLoader(__name__,\",\n                    \"                         __file__).load_module()\",\n                    \"   finally:\",\n                    if_dl(\"     sys.setdlopenflags(old_flags)\"),\n                    \"     os.chdir(old_dir)\",\n                    \"__bootstrap__()\",\n                    \"\"  # terminal \\n\n                ])\n            )\n            f.close()\n        if compile:\n            from distutils.util import byte_compile\n\n            byte_compile([stub_file], optimize=0,\n                         force=True, dry_run=self.dry_run)\n            optimize = self.get_finalized_command('install_lib').optimize\n            if optimize > 0:\n                byte_compile([stub_file], optimize=optimize,\n                             force=True, dry_run=self.dry_run)\n            if os.path.exists(stub_file) and not self.dry_run:\n                os.unlink(stub_file)\n\n\nif use_stubs or os.name == 'nt':\n    # Build shared libraries\n    #\n    def link_shared_object(\n            self, objects, output_libname, output_dir=None, libraries=None,\n            library_dirs=None, runtime_library_dirs=None, export_symbols=None,\n            debug=0, extra_preargs=None, extra_postargs=None, build_temp=None,\n            target_lang=None):\n        self.link(\n            self.SHARED_LIBRARY, objects, output_libname,\n            output_dir, libraries, library_dirs, runtime_library_dirs,\n            export_symbols, debug, extra_preargs, extra_postargs,\n            build_temp, target_lang\n        )\nelse:\n    # Build static libraries everywhere else\n    libtype = 'static'\n\n    def link_shared_object(\n            self, objects, output_libname, output_dir=None, libraries=None,\n            library_dirs=None, runtime_library_dirs=None, export_symbols=None,\n            debug=0, extra_preargs=None, extra_postargs=None, build_temp=None,\n            target_lang=None):\n        # XXX we need to either disallow these attrs on Library instances,\n        # or warn/abort here if set, or something...\n        # libraries=None, library_dirs=None, runtime_library_dirs=None,\n        # export_symbols=None, extra_preargs=None, extra_postargs=None,\n        # build_temp=None\n\n        assert output_dir is None  # distutils build_ext doesn't pass this\n        output_dir, filename = os.path.split(output_libname)\n        basename, ext = os.path.splitext(filename)\n        if self.library_filename(\"x\").startswith('lib'):\n            # strip 'lib' prefix; this is kludgy if some platform uses\n            # a different prefix\n            basename = basename[3:]\n\n        self.create_static_lib(\n            objects, basename, output_dir, debug, target_lang\n        )\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/command/build_ext.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/command/build_ext.py	(date 1602088701621)
@@ -1,6 +1,7 @@
 import os
 import sys
 import itertools
+from importlib.machinery import EXTENSION_SUFFIXES
 from distutils.command.build_ext import build_ext as _du_build_ext
 from distutils.file_util import copy_file
 from distutils.ccompiler import new_compiler
@@ -9,15 +10,6 @@
 from distutils import log
 
 from setuptools.extension import Library
-from setuptools.extern import six
-
-if six.PY2:
-    import imp
-
-    EXTENSION_SUFFIXES = [
-        s for s, _, tp in imp.get_suffixes() if tp == imp.C_EXTENSION]
-else:
-    from importlib.machinery import EXTENSION_SUFFIXES
 
 try:
     # Attempt to use Cython for building extensions, if available
@@ -115,11 +107,7 @@
         filename = _build_ext.get_ext_filename(self, fullname)
         if fullname in self.ext_map:
             ext = self.ext_map[fullname]
-            use_abi3 = (
-                not six.PY2
-                and getattr(ext, 'py_limited_api')
-                and get_abi3_suffix()
-            )
+            use_abi3 = getattr(ext, 'py_limited_api') and get_abi3_suffix()
             if use_abi3:
                 so_ext = get_config_var('EXT_SUFFIX')
                 filename = filename[:-len(so_ext)]
@@ -254,8 +242,8 @@
                 '\n'.join([
                     "def __bootstrap__():",
                     "   global __bootstrap__, __file__, __loader__",
-                    "   import sys, os, pkg_resources" + if_dl(", dl"),
-                    "   from importlib.machinery import ExtensionFileLoader",
+                    "   import sys, os, pkg_resources, importlib.util" +
+                    if_dl(", dl"),
                     "   __file__ = pkg_resources.resource_filename"
                     "(__name__,%r)"
                     % os.path.basename(ext._file_name),
@@ -267,8 +255,10 @@
                     "   try:",
                     "     os.chdir(os.path.dirname(__file__))",
                     if_dl("     sys.setdlopenflags(dl.RTLD_NOW)"),
-                    "     ExtensionFileLoader(__name__,",
-                    "                         __file__).load_module()",
+                    "     spec = importlib.util.spec_from_file_location(",
+                    "                __name__, __file__)",
+                    "     mod = importlib.util.module_from_spec(spec)",
+                    "     spec.loader.exec_module(mod)",
                     "   finally:",
                     if_dl("     sys.setdlopenflags(old_flags)"),
                     "     os.chdir(old_dir)",
Index: env/lib/python3.8/site-packages/setuptools/command/alias.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from distutils.errors import DistutilsOptionError\n\nfrom setuptools.extern.six.moves import map\n\nfrom setuptools.command.setopt import edit_config, option_base, config_file\n\n\ndef shquote(arg):\n    \"\"\"Quote an argument for later parsing by shlex.split()\"\"\"\n    for c in '\"', \"'\", \"\\\\\", \"#\":\n        if c in arg:\n            return repr(arg)\n    if arg.split() != [arg]:\n        return repr(arg)\n    return arg\n\n\nclass alias(option_base):\n    \"\"\"Define a shortcut that invokes one or more commands\"\"\"\n\n    description = \"define a shortcut to invoke one or more commands\"\n    command_consumes_arguments = True\n\n    user_options = [\n        ('remove', 'r', 'remove (unset) the alias'),\n    ] + option_base.user_options\n\n    boolean_options = option_base.boolean_options + ['remove']\n\n    def initialize_options(self):\n        option_base.initialize_options(self)\n        self.args = None\n        self.remove = None\n\n    def finalize_options(self):\n        option_base.finalize_options(self)\n        if self.remove and len(self.args) != 1:\n            raise DistutilsOptionError(\n                \"Must specify exactly one argument (the alias name) when \"\n                \"using --remove\"\n            )\n\n    def run(self):\n        aliases = self.distribution.get_option_dict('aliases')\n\n        if not self.args:\n            print(\"Command Aliases\")\n            print(\"---------------\")\n            for alias in aliases:\n                print(\"setup.py alias\", format_alias(alias, aliases))\n            return\n\n        elif len(self.args) == 1:\n            alias, = self.args\n            if self.remove:\n                command = None\n            elif alias in aliases:\n                print(\"setup.py alias\", format_alias(alias, aliases))\n                return\n            else:\n                print(\"No alias definition found for %r\" % alias)\n                return\n        else:\n            alias = self.args[0]\n            command = ' '.join(map(shquote, self.args[1:]))\n\n        edit_config(self.filename, {'aliases': {alias: command}}, self.dry_run)\n\n\ndef format_alias(name, aliases):\n    source, command = aliases[name]\n    if source == config_file('global'):\n        source = '--global-config '\n    elif source == config_file('user'):\n        source = '--user-config '\n    elif source == config_file('local'):\n        source = ''\n    else:\n        source = '--filename=%r' % source\n    return source + name + ' ' + command\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/command/alias.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/command/alias.py	(date 1602088701617)
@@ -1,7 +1,5 @@
 from distutils.errors import DistutilsOptionError
 
-from setuptools.extern.six.moves import map
-
 from setuptools.command.setopt import edit_config, option_base, config_file
 
 
Index: env/lib/python3.8/site-packages/setuptools/_distutils/command/build_ext.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"distutils.command.build_ext\n\nImplements the Distutils 'build_ext' command, for building extension\nmodules (currently limited to C extensions, should accommodate C++\nextensions ASAP).\"\"\"\n\nimport contextlib\nimport os\nimport re\nimport sys\nfrom distutils.core import Command\nfrom distutils.errors import *\nfrom distutils.sysconfig import customize_compiler, get_python_version\nfrom distutils.sysconfig import get_config_h_filename\nfrom distutils.dep_util import newer_group\nfrom distutils.extension import Extension\nfrom distutils.util import get_platform\nfrom distutils import log\n\nfrom site import USER_BASE\n\n# An extension name is just a dot-separated list of Python NAMEs (ie.\n# the same as a fully-qualified module name).\nextension_name_re = re.compile \\\n    (r'^[a-zA-Z_][a-zA-Z_0-9]*(\\.[a-zA-Z_][a-zA-Z_0-9]*)*$')\n\n\ndef show_compilers ():\n    from distutils.ccompiler import show_compilers\n    show_compilers()\n\n\nclass build_ext(Command):\n\n    description = \"build C/C++ extensions (compile/link to build directory)\"\n\n    # XXX thoughts on how to deal with complex command-line options like\n    # these, i.e. how to make it so fancy_getopt can suck them off the\n    # command line and make it look like setup.py defined the appropriate\n    # lists of tuples of what-have-you.\n    #   - each command needs a callback to process its command-line options\n    #   - Command.__init__() needs access to its share of the whole\n    #     command line (must ultimately come from\n    #     Distribution.parse_command_line())\n    #   - it then calls the current command class' option-parsing\n    #     callback to deal with weird options like -D, which have to\n    #     parse the option text and churn out some custom data\n    #     structure\n    #   - that data structure (in this case, a list of 2-tuples)\n    #     will then be present in the command object by the time\n    #     we get to finalize_options() (i.e. the constructor\n    #     takes care of both command-line and client options\n    #     in between initialize_options() and finalize_options())\n\n    sep_by = \" (separated by '%s')\" % os.pathsep\n    user_options = [\n        ('build-lib=', 'b',\n         \"directory for compiled extension modules\"),\n        ('build-temp=', 't',\n         \"directory for temporary files (build by-products)\"),\n        ('plat-name=', 'p',\n         \"platform name to cross-compile for, if supported \"\n         \"(default: %s)\" % get_platform()),\n        ('inplace', 'i',\n         \"ignore build-lib and put compiled extensions into the source \" +\n         \"directory alongside your pure Python modules\"),\n        ('include-dirs=', 'I',\n         \"list of directories to search for header files\" + sep_by),\n        ('define=', 'D',\n         \"C preprocessor macros to define\"),\n        ('undef=', 'U',\n         \"C preprocessor macros to undefine\"),\n        ('libraries=', 'l',\n         \"external C libraries to link with\"),\n        ('library-dirs=', 'L',\n         \"directories to search for external C libraries\" + sep_by),\n        ('rpath=', 'R',\n         \"directories to search for shared C libraries at runtime\"),\n        ('link-objects=', 'O',\n         \"extra explicit link objects to include in the link\"),\n        ('debug', 'g',\n         \"compile/link with debugging information\"),\n        ('force', 'f',\n         \"forcibly build everything (ignore file timestamps)\"),\n        ('compiler=', 'c',\n         \"specify the compiler type\"),\n        ('parallel=', 'j',\n         \"number of parallel build jobs\"),\n        ('swig-cpp', None,\n         \"make SWIG create C++ files (default is C)\"),\n        ('swig-opts=', None,\n         \"list of SWIG command line options\"),\n        ('swig=', None,\n         \"path to the SWIG executable\"),\n        ('user', None,\n         \"add user include, library and rpath\")\n        ]\n\n    boolean_options = ['inplace', 'debug', 'force', 'swig-cpp', 'user']\n\n    help_options = [\n        ('help-compiler', None,\n         \"list available compilers\", show_compilers),\n        ]\n\n    def initialize_options(self):\n        self.extensions = None\n        self.build_lib = None\n        self.plat_name = None\n        self.build_temp = None\n        self.inplace = 0\n        self.package = None\n\n        self.include_dirs = None\n        self.define = None\n        self.undef = None\n        self.libraries = None\n        self.library_dirs = None\n        self.rpath = None\n        self.link_objects = None\n        self.debug = None\n        self.force = None\n        self.compiler = None\n        self.swig = None\n        self.swig_cpp = None\n        self.swig_opts = None\n        self.user = None\n        self.parallel = None\n\n    def finalize_options(self):\n        from distutils import sysconfig\n\n        self.set_undefined_options('build',\n                                   ('build_lib', 'build_lib'),\n                                   ('build_temp', 'build_temp'),\n                                   ('compiler', 'compiler'),\n                                   ('debug', 'debug'),\n                                   ('force', 'force'),\n                                   ('parallel', 'parallel'),\n                                   ('plat_name', 'plat_name'),\n                                   )\n\n        if self.package is None:\n            self.package = self.distribution.ext_package\n\n        self.extensions = self.distribution.ext_modules\n\n        # Make sure Python's include directories (for Python.h, pyconfig.h,\n        # etc.) are in the include search path.\n        py_include = sysconfig.get_python_inc()\n        plat_py_include = sysconfig.get_python_inc(plat_specific=1)\n        if self.include_dirs is None:\n            self.include_dirs = self.distribution.include_dirs or []\n        if isinstance(self.include_dirs, str):\n            self.include_dirs = self.include_dirs.split(os.pathsep)\n\n        # If in a virtualenv, add its include directory\n        # Issue 16116\n        if sys.exec_prefix != sys.base_exec_prefix:\n            self.include_dirs.append(os.path.join(sys.exec_prefix, 'include'))\n\n        # Put the Python \"system\" include dir at the end, so that\n        # any local include dirs take precedence.\n        self.include_dirs.extend(py_include.split(os.path.pathsep))\n        if plat_py_include != py_include:\n            self.include_dirs.extend(\n                plat_py_include.split(os.path.pathsep))\n\n        self.ensure_string_list('libraries')\n        self.ensure_string_list('link_objects')\n\n        # Life is easier if we're not forever checking for None, so\n        # simplify these options to empty lists if unset\n        if self.libraries is None:\n            self.libraries = []\n        if self.library_dirs is None:\n            self.library_dirs = []\n        elif isinstance(self.library_dirs, str):\n            self.library_dirs = self.library_dirs.split(os.pathsep)\n\n        if self.rpath is None:\n            self.rpath = []\n        elif isinstance(self.rpath, str):\n            self.rpath = self.rpath.split(os.pathsep)\n\n        # for extensions under windows use different directories\n        # for Release and Debug builds.\n        # also Python's library directory must be appended to library_dirs\n        if os.name == 'nt':\n            # the 'libs' directory is for binary installs - we assume that\n            # must be the *native* platform.  But we don't really support\n            # cross-compiling via a binary install anyway, so we let it go.\n            self.library_dirs.append(os.path.join(sys.exec_prefix, 'libs'))\n            if sys.base_exec_prefix != sys.prefix:  # Issue 16116\n                self.library_dirs.append(os.path.join(sys.base_exec_prefix, 'libs'))\n            if self.debug:\n                self.build_temp = os.path.join(self.build_temp, \"Debug\")\n            else:\n                self.build_temp = os.path.join(self.build_temp, \"Release\")\n\n            # Append the source distribution include and library directories,\n            # this allows distutils on windows to work in the source tree\n            self.include_dirs.append(os.path.dirname(get_config_h_filename()))\n            _sys_home = getattr(sys, '_home', None)\n            if _sys_home:\n                self.library_dirs.append(_sys_home)\n\n            # Use the .lib files for the correct architecture\n            if self.plat_name == 'win32':\n                suffix = 'win32'\n            else:\n                # win-amd64\n                suffix = self.plat_name[4:]\n            new_lib = os.path.join(sys.exec_prefix, 'PCbuild')\n            if suffix:\n                new_lib = os.path.join(new_lib, suffix)\n            self.library_dirs.append(new_lib)\n\n        # For extensions under Cygwin, Python's library directory must be\n        # appended to library_dirs\n        if sys.platform[:6] == 'cygwin':\n            if sys.executable.startswith(os.path.join(sys.exec_prefix, \"bin\")):\n                # building third party extensions\n                self.library_dirs.append(os.path.join(sys.prefix, \"lib\",\n                                                      \"python\" + get_python_version(),\n                                                      \"config\"))\n            else:\n                # building python standard extensions\n                self.library_dirs.append('.')\n\n        # For building extensions with a shared Python library,\n        # Python's library directory must be appended to library_dirs\n        # See Issues: #1600860, #4366\n        if (sysconfig.get_config_var('Py_ENABLE_SHARED')):\n            if not sysconfig.python_build:\n                # building third party extensions\n                self.library_dirs.append(sysconfig.get_config_var('LIBDIR'))\n            else:\n                # building python standard extensions\n                self.library_dirs.append('.')\n\n        # The argument parsing will result in self.define being a string, but\n        # it has to be a list of 2-tuples.  All the preprocessor symbols\n        # specified by the 'define' option will be set to '1'.  Multiple\n        # symbols can be separated with commas.\n\n        if self.define:\n            defines = self.define.split(',')\n            self.define = [(symbol, '1') for symbol in defines]\n\n        # The option for macros to undefine is also a string from the\n        # option parsing, but has to be a list.  Multiple symbols can also\n        # be separated with commas here.\n        if self.undef:\n            self.undef = self.undef.split(',')\n\n        if self.swig_opts is None:\n            self.swig_opts = []\n        else:\n            self.swig_opts = self.swig_opts.split(' ')\n\n        # Finally add the user include and library directories if requested\n        if self.user:\n            user_include = os.path.join(USER_BASE, \"include\")\n            user_lib = os.path.join(USER_BASE, \"lib\")\n            if os.path.isdir(user_include):\n                self.include_dirs.append(user_include)\n            if os.path.isdir(user_lib):\n                self.library_dirs.append(user_lib)\n                self.rpath.append(user_lib)\n\n        if isinstance(self.parallel, str):\n            try:\n                self.parallel = int(self.parallel)\n            except ValueError:\n                raise DistutilsOptionError(\"parallel should be an integer\")\n\n    def run(self):\n        from distutils.ccompiler import new_compiler\n\n        # 'self.extensions', as supplied by setup.py, is a list of\n        # Extension instances.  See the documentation for Extension (in\n        # distutils.extension) for details.\n        #\n        # For backwards compatibility with Distutils 0.8.2 and earlier, we\n        # also allow the 'extensions' list to be a list of tuples:\n        #    (ext_name, build_info)\n        # where build_info is a dictionary containing everything that\n        # Extension instances do except the name, with a few things being\n        # differently named.  We convert these 2-tuples to Extension\n        # instances as needed.\n\n        if not self.extensions:\n            return\n\n        # If we were asked to build any C/C++ libraries, make sure that the\n        # directory where we put them is in the library search path for\n        # linking extensions.\n        if self.distribution.has_c_libraries():\n            build_clib = self.get_finalized_command('build_clib')\n            self.libraries.extend(build_clib.get_library_names() or [])\n            self.library_dirs.append(build_clib.build_clib)\n\n        # Setup the CCompiler object that we'll use to do all the\n        # compiling and linking\n        self.compiler = new_compiler(compiler=self.compiler,\n                                     verbose=self.verbose,\n                                     dry_run=self.dry_run,\n                                     force=self.force)\n        customize_compiler(self.compiler)\n        # If we are cross-compiling, init the compiler now (if we are not\n        # cross-compiling, init would not hurt, but people may rely on\n        # late initialization of compiler even if they shouldn't...)\n        if os.name == 'nt' and self.plat_name != get_platform():\n            self.compiler.initialize(self.plat_name)\n\n        # And make sure that any compile/link-related options (which might\n        # come from the command-line or from the setup script) are set in\n        # that CCompiler object -- that way, they automatically apply to\n        # all compiling and linking done here.\n        if self.include_dirs is not None:\n            self.compiler.set_include_dirs(self.include_dirs)\n        if self.define is not None:\n            # 'define' option is a list of (name,value) tuples\n            for (name, value) in self.define:\n                self.compiler.define_macro(name, value)\n        if self.undef is not None:\n            for macro in self.undef:\n                self.compiler.undefine_macro(macro)\n        if self.libraries is not None:\n            self.compiler.set_libraries(self.libraries)\n        if self.library_dirs is not None:\n            self.compiler.set_library_dirs(self.library_dirs)\n        if self.rpath is not None:\n            self.compiler.set_runtime_library_dirs(self.rpath)\n        if self.link_objects is not None:\n            self.compiler.set_link_objects(self.link_objects)\n\n        # Now actually compile and link everything.\n        self.build_extensions()\n\n    def check_extensions_list(self, extensions):\n        \"\"\"Ensure that the list of extensions (presumably provided as a\n        command option 'extensions') is valid, i.e. it is a list of\n        Extension objects.  We also support the old-style list of 2-tuples,\n        where the tuples are (ext_name, build_info), which are converted to\n        Extension instances here.\n\n        Raise DistutilsSetupError if the structure is invalid anywhere;\n        just returns otherwise.\n        \"\"\"\n        if not isinstance(extensions, list):\n            raise DistutilsSetupError(\n                  \"'ext_modules' option must be a list of Extension instances\")\n\n        for i, ext in enumerate(extensions):\n            if isinstance(ext, Extension):\n                continue                # OK! (assume type-checking done\n                                        # by Extension constructor)\n\n            if not isinstance(ext, tuple) or len(ext) != 2:\n                raise DistutilsSetupError(\n                       \"each element of 'ext_modules' option must be an \"\n                       \"Extension instance or 2-tuple\")\n\n            ext_name, build_info = ext\n\n            log.warn(\"old-style (ext_name, build_info) tuple found in \"\n                     \"ext_modules for extension '%s' \"\n                     \"-- please convert to Extension instance\", ext_name)\n\n            if not (isinstance(ext_name, str) and\n                    extension_name_re.match(ext_name)):\n                raise DistutilsSetupError(\n                       \"first element of each tuple in 'ext_modules' \"\n                       \"must be the extension name (a string)\")\n\n            if not isinstance(build_info, dict):\n                raise DistutilsSetupError(\n                       \"second element of each tuple in 'ext_modules' \"\n                       \"must be a dictionary (build info)\")\n\n            # OK, the (ext_name, build_info) dict is type-safe: convert it\n            # to an Extension instance.\n            ext = Extension(ext_name, build_info['sources'])\n\n            # Easy stuff: one-to-one mapping from dict elements to\n            # instance attributes.\n            for key in ('include_dirs', 'library_dirs', 'libraries',\n                        'extra_objects', 'extra_compile_args',\n                        'extra_link_args'):\n                val = build_info.get(key)\n                if val is not None:\n                    setattr(ext, key, val)\n\n            # Medium-easy stuff: same syntax/semantics, different names.\n            ext.runtime_library_dirs = build_info.get('rpath')\n            if 'def_file' in build_info:\n                log.warn(\"'def_file' element of build info dict \"\n                         \"no longer supported\")\n\n            # Non-trivial stuff: 'macros' split into 'define_macros'\n            # and 'undef_macros'.\n            macros = build_info.get('macros')\n            if macros:\n                ext.define_macros = []\n                ext.undef_macros = []\n                for macro in macros:\n                    if not (isinstance(macro, tuple) and len(macro) in (1, 2)):\n                        raise DistutilsSetupError(\n                              \"'macros' element of build info dict \"\n                              \"must be 1- or 2-tuple\")\n                    if len(macro) == 1:\n                        ext.undef_macros.append(macro[0])\n                    elif len(macro) == 2:\n                        ext.define_macros.append(macro)\n\n            extensions[i] = ext\n\n    def get_source_files(self):\n        self.check_extensions_list(self.extensions)\n        filenames = []\n\n        # Wouldn't it be neat if we knew the names of header files too...\n        for ext in self.extensions:\n            filenames.extend(ext.sources)\n        return filenames\n\n    def get_outputs(self):\n        # Sanity check the 'extensions' list -- can't assume this is being\n        # done in the same run as a 'build_extensions()' call (in fact, we\n        # can probably assume that it *isn't*!).\n        self.check_extensions_list(self.extensions)\n\n        # And build the list of output (built) filenames.  Note that this\n        # ignores the 'inplace' flag, and assumes everything goes in the\n        # \"build\" tree.\n        outputs = []\n        for ext in self.extensions:\n            outputs.append(self.get_ext_fullpath(ext.name))\n        return outputs\n\n    def build_extensions(self):\n        # First, sanity-check the 'extensions' list\n        self.check_extensions_list(self.extensions)\n        if self.parallel:\n            self._build_extensions_parallel()\n        else:\n            self._build_extensions_serial()\n\n    def _build_extensions_parallel(self):\n        workers = self.parallel\n        if self.parallel is True:\n            workers = os.cpu_count()  # may return None\n        try:\n            from concurrent.futures import ThreadPoolExecutor\n        except ImportError:\n            workers = None\n\n        if workers is None:\n            self._build_extensions_serial()\n            return\n\n        with ThreadPoolExecutor(max_workers=workers) as executor:\n            futures = [executor.submit(self.build_extension, ext)\n                       for ext in self.extensions]\n            for ext, fut in zip(self.extensions, futures):\n                with self._filter_build_errors(ext):\n                    fut.result()\n\n    def _build_extensions_serial(self):\n        for ext in self.extensions:\n            with self._filter_build_errors(ext):\n                self.build_extension(ext)\n\n    @contextlib.contextmanager\n    def _filter_build_errors(self, ext):\n        try:\n            yield\n        except (CCompilerError, DistutilsError, CompileError) as e:\n            if not ext.optional:\n                raise\n            self.warn('building extension \"%s\" failed: %s' %\n                      (ext.name, e))\n\n    def build_extension(self, ext):\n        sources = ext.sources\n        if sources is None or not isinstance(sources, (list, tuple)):\n            raise DistutilsSetupError(\n                  \"in 'ext_modules' option (extension '%s'), \"\n                  \"'sources' must be present and must be \"\n                  \"a list of source filenames\" % ext.name)\n        # sort to make the resulting .so file build reproducible\n        sources = sorted(sources)\n\n        ext_path = self.get_ext_fullpath(ext.name)\n        depends = sources + ext.depends\n        if not (self.force or newer_group(depends, ext_path, 'newer')):\n            log.debug(\"skipping '%s' extension (up-to-date)\", ext.name)\n            return\n        else:\n            log.info(\"building '%s' extension\", ext.name)\n\n        # First, scan the sources for SWIG definition files (.i), run\n        # SWIG on 'em to create .c files, and modify the sources list\n        # accordingly.\n        sources = self.swig_sources(sources, ext)\n\n        # Next, compile the source code to object files.\n\n        # XXX not honouring 'define_macros' or 'undef_macros' -- the\n        # CCompiler API needs to change to accommodate this, and I\n        # want to do one thing at a time!\n\n        # Two possible sources for extra compiler arguments:\n        #   - 'extra_compile_args' in Extension object\n        #   - CFLAGS environment variable (not particularly\n        #     elegant, but people seem to expect it and I\n        #     guess it's useful)\n        # The environment variable should take precedence, and\n        # any sensible compiler will give precedence to later\n        # command line args.  Hence we combine them in order:\n        extra_args = ext.extra_compile_args or []\n\n        macros = ext.define_macros[:]\n        for undef in ext.undef_macros:\n            macros.append((undef,))\n\n        objects = self.compiler.compile(sources,\n                                         output_dir=self.build_temp,\n                                         macros=macros,\n                                         include_dirs=ext.include_dirs,\n                                         debug=self.debug,\n                                         extra_postargs=extra_args,\n                                         depends=ext.depends)\n\n        # XXX outdated variable, kept here in case third-part code\n        # needs it.\n        self._built_objects = objects[:]\n\n        # Now link the object files together into a \"shared object\" --\n        # of course, first we have to figure out all the other things\n        # that go into the mix.\n        if ext.extra_objects:\n            objects.extend(ext.extra_objects)\n        extra_args = ext.extra_link_args or []\n\n        # Detect target language, if not provided\n        language = ext.language or self.compiler.detect_language(sources)\n\n        self.compiler.link_shared_object(\n            objects, ext_path,\n            libraries=self.get_libraries(ext),\n            library_dirs=ext.library_dirs,\n            runtime_library_dirs=ext.runtime_library_dirs,\n            extra_postargs=extra_args,\n            export_symbols=self.get_export_symbols(ext),\n            debug=self.debug,\n            build_temp=self.build_temp,\n            target_lang=language)\n\n    def swig_sources(self, sources, extension):\n        \"\"\"Walk the list of source files in 'sources', looking for SWIG\n        interface (.i) files.  Run SWIG on all that are found, and\n        return a modified 'sources' list with SWIG source files replaced\n        by the generated C (or C++) files.\n        \"\"\"\n        new_sources = []\n        swig_sources = []\n        swig_targets = {}\n\n        # XXX this drops generated C/C++ files into the source tree, which\n        # is fine for developers who want to distribute the generated\n        # source -- but there should be an option to put SWIG output in\n        # the temp dir.\n\n        if self.swig_cpp:\n            log.warn(\"--swig-cpp is deprecated - use --swig-opts=-c++\")\n\n        if self.swig_cpp or ('-c++' in self.swig_opts) or \\\n           ('-c++' in extension.swig_opts):\n            target_ext = '.cpp'\n        else:\n            target_ext = '.c'\n\n        for source in sources:\n            (base, ext) = os.path.splitext(source)\n            if ext == \".i\":             # SWIG interface file\n                new_sources.append(base + '_wrap' + target_ext)\n                swig_sources.append(source)\n                swig_targets[source] = new_sources[-1]\n            else:\n                new_sources.append(source)\n\n        if not swig_sources:\n            return new_sources\n\n        swig = self.swig or self.find_swig()\n        swig_cmd = [swig, \"-python\"]\n        swig_cmd.extend(self.swig_opts)\n        if self.swig_cpp:\n            swig_cmd.append(\"-c++\")\n\n        # Do not override commandline arguments\n        if not self.swig_opts:\n            for o in extension.swig_opts:\n                swig_cmd.append(o)\n\n        for source in swig_sources:\n            target = swig_targets[source]\n            log.info(\"swigging %s to %s\", source, target)\n            self.spawn(swig_cmd + [\"-o\", target, source])\n\n        return new_sources\n\n    def find_swig(self):\n        \"\"\"Return the name of the SWIG executable.  On Unix, this is\n        just \"swig\" -- it should be in the PATH.  Tries a bit harder on\n        Windows.\n        \"\"\"\n        if os.name == \"posix\":\n            return \"swig\"\n        elif os.name == \"nt\":\n            # Look for SWIG in its standard installation directory on\n            # Windows (or so I presume!).  If we find it there, great;\n            # if not, act like Unix and assume it's in the PATH.\n            for vers in (\"1.3\", \"1.2\", \"1.1\"):\n                fn = os.path.join(\"c:\\\\swig%s\" % vers, \"swig.exe\")\n                if os.path.isfile(fn):\n                    return fn\n            else:\n                return \"swig.exe\"\n        else:\n            raise DistutilsPlatformError(\n                  \"I don't know how to find (much less run) SWIG \"\n                  \"on platform '%s'\" % os.name)\n\n    # -- Name generators -----------------------------------------------\n    # (extension names, filenames, whatever)\n    def get_ext_fullpath(self, ext_name):\n        \"\"\"Returns the path of the filename for a given extension.\n\n        The file is located in `build_lib` or directly in the package\n        (inplace option).\n        \"\"\"\n        fullname = self.get_ext_fullname(ext_name)\n        modpath = fullname.split('.')\n        filename = self.get_ext_filename(modpath[-1])\n\n        if not self.inplace:\n            # no further work needed\n            # returning :\n            #   build_dir/package/path/filename\n            filename = os.path.join(*modpath[:-1]+[filename])\n            return os.path.join(self.build_lib, filename)\n\n        # the inplace option requires to find the package directory\n        # using the build_py command for that\n        package = '.'.join(modpath[0:-1])\n        build_py = self.get_finalized_command('build_py')\n        package_dir = os.path.abspath(build_py.get_package_dir(package))\n\n        # returning\n        #   package_dir/filename\n        return os.path.join(package_dir, filename)\n\n    def get_ext_fullname(self, ext_name):\n        \"\"\"Returns the fullname of a given extension name.\n\n        Adds the `package.` prefix\"\"\"\n        if self.package is None:\n            return ext_name\n        else:\n            return self.package + '.' + ext_name\n\n    def get_ext_filename(self, ext_name):\n        r\"\"\"Convert the name of an extension (eg. \"foo.bar\") into the name\n        of the file from which it will be loaded (eg. \"foo/bar.so\", or\n        \"foo\\bar.pyd\").\n        \"\"\"\n        from distutils.sysconfig import get_config_var\n        ext_path = ext_name.split('.')\n        ext_suffix = get_config_var('EXT_SUFFIX')\n        return os.path.join(*ext_path) + ext_suffix\n\n    def get_export_symbols(self, ext):\n        \"\"\"Return the list of symbols that a shared extension has to\n        export.  This either uses 'ext.export_symbols' or, if it's not\n        provided, \"PyInit_\" + module_name.  Only relevant on Windows, where\n        the .pyd file (DLL) must export the module \"PyInit_\" function.\n        \"\"\"\n        suffix = '_' + ext.name.split('.')[-1]\n        try:\n            # Unicode module name support as defined in PEP-489\n            # https://www.python.org/dev/peps/pep-0489/#export-hook-name\n            suffix.encode('ascii')\n        except UnicodeEncodeError:\n            suffix = 'U' + suffix.encode('punycode').replace(b'-', b'_').decode('ascii')\n\n        initfunc_name = \"PyInit\" + suffix\n        if initfunc_name not in ext.export_symbols:\n            ext.export_symbols.append(initfunc_name)\n        return ext.export_symbols\n\n    def get_libraries(self, ext):\n        \"\"\"Return the list of libraries to link against when building a\n        shared extension.  On most platforms, this is just 'ext.libraries';\n        on Windows, we add the Python library (eg. python20.dll).\n        \"\"\"\n        # The python library is always needed on Windows.  For MSVC, this\n        # is redundant, since the library is mentioned in a pragma in\n        # pyconfig.h that MSVC groks.  The other Windows compilers all seem\n        # to need it mentioned explicitly, though, so that's what we do.\n        # Append '_d' to the python import library on debug builds.\n        if sys.platform == \"win32\":\n            from distutils._msvccompiler import MSVCCompiler\n            if not isinstance(self.compiler, MSVCCompiler):\n                template = \"python%d%d\"\n                if self.debug:\n                    template = template + '_d'\n                pythonlib = (template %\n                       (sys.hexversion >> 24, (sys.hexversion >> 16) & 0xff))\n                # don't extend ext.libraries, it may be shared with other\n                # extensions, it is a reference to the original list\n                return ext.libraries + [pythonlib]\n        else:\n            # On Android only the main executable and LD_PRELOADs are considered\n            # to be RTLD_GLOBAL, all the dependencies of the main executable\n            # remain RTLD_LOCAL and so the shared libraries must be linked with\n            # libpython when python is built with a shared python library (issue\n            # bpo-21536).\n            # On Cygwin (and if required, other POSIX-like platforms based on\n            # Windows like MinGW) it is simply necessary that all symbols in\n            # shared libraries are resolved at link time.\n            from distutils.sysconfig import get_config_var\n            link_libpython = False\n            if get_config_var('Py_ENABLE_SHARED'):\n                # A native build on an Android device or on Cygwin\n                if hasattr(sys, 'getandroidapilevel'):\n                    link_libpython = True\n                elif sys.platform == 'cygwin':\n                    link_libpython = True\n                elif '_PYTHON_HOST_PLATFORM' in os.environ:\n                    # We are cross-compiling for one of the relevant platforms\n                    if get_config_var('ANDROID_API_LEVEL') != 0:\n                        link_libpython = True\n                    elif get_config_var('MACHDEP') == 'cygwin':\n                        link_libpython = True\n\n            if link_libpython:\n                ldversion = get_config_var('LDVERSION')\n                return ext.libraries + ['python' + ldversion]\n\n        return ext.libraries\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/_distutils/command/build_ext.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/_distutils/command/build_ext.py	(date 1602088701609)
@@ -16,6 +16,7 @@
 from distutils.extension import Extension
 from distutils.util import get_platform
 from distutils import log
+from . import py37compat
 
 from site import USER_BASE
 
@@ -751,4 +752,4 @@
                 ldversion = get_config_var('LDVERSION')
                 return ext.libraries + ['python' + ldversion]
 
-        return ext.libraries
+        return ext.libraries + py37compat.pythonlib()
Index: env/lib/python3.8/site-packages/setuptools/command/rotate.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from distutils.util import convert_path\nfrom distutils import log\nfrom distutils.errors import DistutilsOptionError\nimport os\nimport shutil\n\nfrom setuptools.extern import six\n\nfrom setuptools import Command\n\n\nclass rotate(Command):\n    \"\"\"Delete older distributions\"\"\"\n\n    description = \"delete older distributions, keeping N newest files\"\n    user_options = [\n        ('match=', 'm', \"patterns to match (required)\"),\n        ('dist-dir=', 'd', \"directory where the distributions are\"),\n        ('keep=', 'k', \"number of matching distributions to keep\"),\n    ]\n\n    boolean_options = []\n\n    def initialize_options(self):\n        self.match = None\n        self.dist_dir = None\n        self.keep = None\n\n    def finalize_options(self):\n        if self.match is None:\n            raise DistutilsOptionError(\n                \"Must specify one or more (comma-separated) match patterns \"\n                \"(e.g. '.zip' or '.egg')\"\n            )\n        if self.keep is None:\n            raise DistutilsOptionError(\"Must specify number of files to keep\")\n        try:\n            self.keep = int(self.keep)\n        except ValueError as e:\n            raise DistutilsOptionError(\"--keep must be an integer\") from e\n        if isinstance(self.match, six.string_types):\n            self.match = [\n                convert_path(p.strip()) for p in self.match.split(',')\n            ]\n        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))\n\n    def run(self):\n        self.run_command(\"egg_info\")\n        from glob import glob\n\n        for pattern in self.match:\n            pattern = self.distribution.get_name() + '*' + pattern\n            files = glob(os.path.join(self.dist_dir, pattern))\n            files = [(os.path.getmtime(f), f) for f in files]\n            files.sort()\n            files.reverse()\n\n            log.info(\"%d file(s) matching %s\", len(files), pattern)\n            files = files[self.keep:]\n            for (t, f) in files:\n                log.info(\"Deleting %s\", f)\n                if not self.dry_run:\n                    if os.path.isdir(f):\n                        shutil.rmtree(f)\n                    else:\n                        os.unlink(f)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/command/rotate.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/command/rotate.py	(date 1602088701625)
@@ -4,8 +4,6 @@
 import os
 import shutil
 
-from setuptools.extern import six
-
 from setuptools import Command
 
 
@@ -38,7 +36,7 @@
             self.keep = int(self.keep)
         except ValueError as e:
             raise DistutilsOptionError("--keep must be an integer") from e
-        if isinstance(self.match, six.string_types):
+        if isinstance(self.match, str):
             self.match = [
                 convert_path(p.strip()) for p in self.match.split(',')
             ]
Index: env/lib/python3.8/site-packages/setuptools/command/bdist_rpm.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import distutils.command.bdist_rpm as orig\n\n\nclass bdist_rpm(orig.bdist_rpm):\n    \"\"\"\n    Override the default bdist_rpm behavior to do the following:\n\n    1. Run egg_info to ensure the name and version are properly calculated.\n    2. Always run 'install' using --single-version-externally-managed to\n       disable eggs in RPM distributions.\n    3. Replace dash with underscore in the version numbers for better RPM\n       compatibility.\n    \"\"\"\n\n    def run(self):\n        # ensure distro name is up-to-date\n        self.run_command('egg_info')\n\n        orig.bdist_rpm.run(self)\n\n    def _make_spec_file(self):\n        version = self.distribution.get_version()\n        rpmversion = version.replace('-', '_')\n        spec = orig.bdist_rpm._make_spec_file(self)\n        line23 = '%define version ' + version\n        line24 = '%define version ' + rpmversion\n        spec = [\n            line.replace(\n                \"Source0: %{name}-%{version}.tar\",\n                \"Source0: %{name}-%{unmangled_version}.tar\"\n            ).replace(\n                \"setup.py install \",\n                \"setup.py install --single-version-externally-managed \"\n            ).replace(\n                \"%setup\",\n                \"%setup -n %{name}-%{unmangled_version}\"\n            ).replace(line23, line24)\n            for line in spec\n        ]\n        insert_loc = spec.index(line24) + 1\n        unmangled_version = \"%define unmangled_version \" + version\n        spec.insert(insert_loc, unmangled_version)\n        return spec\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/command/bdist_rpm.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/command/bdist_rpm.py	(date 1602088701621)
@@ -8,8 +8,6 @@
     1. Run egg_info to ensure the name and version are properly calculated.
     2. Always run 'install' using --single-version-externally-managed to
        disable eggs in RPM distributions.
-    3. Replace dash with underscore in the version numbers for better RPM
-       compatibility.
     """
 
     def run(self):
@@ -19,25 +17,15 @@
         orig.bdist_rpm.run(self)
 
     def _make_spec_file(self):
-        version = self.distribution.get_version()
-        rpmversion = version.replace('-', '_')
         spec = orig.bdist_rpm._make_spec_file(self)
-        line23 = '%define version ' + version
-        line24 = '%define version ' + rpmversion
         spec = [
             line.replace(
-                "Source0: %{name}-%{version}.tar",
-                "Source0: %{name}-%{unmangled_version}.tar"
-            ).replace(
                 "setup.py install ",
                 "setup.py install --single-version-externally-managed "
             ).replace(
                 "%setup",
                 "%setup -n %{name}-%{unmangled_version}"
-            ).replace(line23, line24)
+            )
             for line in spec
         ]
-        insert_loc = spec.index(line24) + 1
-        unmangled_version = "%define unmangled_version " + version
-        spec.insert(insert_loc, unmangled_version)
         return spec
Index: env/lib/python3.8/site-packages/setuptools/command/build_py.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from glob import glob\nfrom distutils.util import convert_path\nimport distutils.command.build_py as orig\nimport os\nimport fnmatch\nimport textwrap\nimport io\nimport distutils.errors\nimport itertools\nimport stat\n\nfrom setuptools.extern import six\nfrom setuptools.extern.six.moves import map, filter, filterfalse\n\ntry:\n    from setuptools.lib2to3_ex import Mixin2to3\nexcept ImportError:\n\n    class Mixin2to3:\n        def run_2to3(self, files, doctests=True):\n            \"do nothing\"\n\n\ndef make_writable(target):\n    os.chmod(target, os.stat(target).st_mode | stat.S_IWRITE)\n\n\nclass build_py(orig.build_py, Mixin2to3):\n    \"\"\"Enhanced 'build_py' command that includes data files with packages\n\n    The data files are specified via a 'package_data' argument to 'setup()'.\n    See 'setuptools.dist.Distribution' for more details.\n\n    Also, this version of the 'build_py' command allows you to specify both\n    'py_modules' and 'packages' in the same setup operation.\n    \"\"\"\n\n    def finalize_options(self):\n        orig.build_py.finalize_options(self)\n        self.package_data = self.distribution.package_data\n        self.exclude_package_data = (self.distribution.exclude_package_data or\n                                     {})\n        if 'data_files' in self.__dict__:\n            del self.__dict__['data_files']\n        self.__updated_files = []\n        self.__doctests_2to3 = []\n\n    def run(self):\n        \"\"\"Build modules, packages, and copy data files to build directory\"\"\"\n        if not self.py_modules and not self.packages:\n            return\n\n        if self.py_modules:\n            self.build_modules()\n\n        if self.packages:\n            self.build_packages()\n            self.build_package_data()\n\n        self.run_2to3(self.__updated_files, False)\n        self.run_2to3(self.__updated_files, True)\n        self.run_2to3(self.__doctests_2to3, True)\n\n        # Only compile actual .py files, using our base class' idea of what our\n        # output files are.\n        self.byte_compile(orig.build_py.get_outputs(self, include_bytecode=0))\n\n    def __getattr__(self, attr):\n        \"lazily compute data files\"\n        if attr == 'data_files':\n            self.data_files = self._get_data_files()\n            return self.data_files\n        return orig.build_py.__getattr__(self, attr)\n\n    def build_module(self, module, module_file, package):\n        if six.PY2 and isinstance(package, six.string_types):\n            # avoid errors on Python 2 when unicode is passed (#190)\n            package = package.split('.')\n        outfile, copied = orig.build_py.build_module(self, module, module_file,\n                                                     package)\n        if copied:\n            self.__updated_files.append(outfile)\n        return outfile, copied\n\n    def _get_data_files(self):\n        \"\"\"Generate list of '(package,src_dir,build_dir,filenames)' tuples\"\"\"\n        self.analyze_manifest()\n        return list(map(self._get_pkg_data_files, self.packages or ()))\n\n    def _get_pkg_data_files(self, package):\n        # Locate package source directory\n        src_dir = self.get_package_dir(package)\n\n        # Compute package build directory\n        build_dir = os.path.join(*([self.build_lib] + package.split('.')))\n\n        # Strip directory from globbed filenames\n        filenames = [\n            os.path.relpath(file, src_dir)\n            for file in self.find_data_files(package, src_dir)\n        ]\n        return package, src_dir, build_dir, filenames\n\n    def find_data_files(self, package, src_dir):\n        \"\"\"Return filenames for package's data files in 'src_dir'\"\"\"\n        patterns = self._get_platform_patterns(\n            self.package_data,\n            package,\n            src_dir,\n        )\n        globs_expanded = map(glob, patterns)\n        # flatten the expanded globs into an iterable of matches\n        globs_matches = itertools.chain.from_iterable(globs_expanded)\n        glob_files = filter(os.path.isfile, globs_matches)\n        files = itertools.chain(\n            self.manifest_files.get(package, []),\n            glob_files,\n        )\n        return self.exclude_data_files(package, src_dir, files)\n\n    def build_package_data(self):\n        \"\"\"Copy data files into build directory\"\"\"\n        for package, src_dir, build_dir, filenames in self.data_files:\n            for filename in filenames:\n                target = os.path.join(build_dir, filename)\n                self.mkpath(os.path.dirname(target))\n                srcfile = os.path.join(src_dir, filename)\n                outf, copied = self.copy_file(srcfile, target)\n                make_writable(target)\n                srcfile = os.path.abspath(srcfile)\n                if (copied and\n                        srcfile in self.distribution.convert_2to3_doctests):\n                    self.__doctests_2to3.append(outf)\n\n    def analyze_manifest(self):\n        self.manifest_files = mf = {}\n        if not self.distribution.include_package_data:\n            return\n        src_dirs = {}\n        for package in self.packages or ():\n            # Locate package source directory\n            src_dirs[assert_relative(self.get_package_dir(package))] = package\n\n        self.run_command('egg_info')\n        ei_cmd = self.get_finalized_command('egg_info')\n        for path in ei_cmd.filelist.files:\n            d, f = os.path.split(assert_relative(path))\n            prev = None\n            oldf = f\n            while d and d != prev and d not in src_dirs:\n                prev = d\n                d, df = os.path.split(d)\n                f = os.path.join(df, f)\n            if d in src_dirs:\n                if path.endswith('.py') and f == oldf:\n                    continue  # it's a module, not data\n                mf.setdefault(src_dirs[d], []).append(path)\n\n    def get_data_files(self):\n        pass  # Lazily compute data files in _get_data_files() function.\n\n    def check_package(self, package, package_dir):\n        \"\"\"Check namespace packages' __init__ for declare_namespace\"\"\"\n        try:\n            return self.packages_checked[package]\n        except KeyError:\n            pass\n\n        init_py = orig.build_py.check_package(self, package, package_dir)\n        self.packages_checked[package] = init_py\n\n        if not init_py or not self.distribution.namespace_packages:\n            return init_py\n\n        for pkg in self.distribution.namespace_packages:\n            if pkg == package or pkg.startswith(package + '.'):\n                break\n        else:\n            return init_py\n\n        with io.open(init_py, 'rb') as f:\n            contents = f.read()\n        if b'declare_namespace' not in contents:\n            raise distutils.errors.DistutilsError(\n                \"Namespace package problem: %s is a namespace package, but \"\n                \"its\\n__init__.py does not call declare_namespace()! Please \"\n                'fix it.\\n(See the setuptools manual under '\n                '\"Namespace Packages\" for details.)\\n\"' % (package,)\n            )\n        return init_py\n\n    def initialize_options(self):\n        self.packages_checked = {}\n        orig.build_py.initialize_options(self)\n\n    def get_package_dir(self, package):\n        res = orig.build_py.get_package_dir(self, package)\n        if self.distribution.src_root is not None:\n            return os.path.join(self.distribution.src_root, res)\n        return res\n\n    def exclude_data_files(self, package, src_dir, files):\n        \"\"\"Filter filenames for package's data files in 'src_dir'\"\"\"\n        files = list(files)\n        patterns = self._get_platform_patterns(\n            self.exclude_package_data,\n            package,\n            src_dir,\n        )\n        match_groups = (\n            fnmatch.filter(files, pattern)\n            for pattern in patterns\n        )\n        # flatten the groups of matches into an iterable of matches\n        matches = itertools.chain.from_iterable(match_groups)\n        bad = set(matches)\n        keepers = (\n            fn\n            for fn in files\n            if fn not in bad\n        )\n        # ditch dupes\n        return list(_unique_everseen(keepers))\n\n    @staticmethod\n    def _get_platform_patterns(spec, package, src_dir):\n        \"\"\"\n        yield platform-specific path patterns (suitable for glob\n        or fn_match) from a glob-based spec (such as\n        self.package_data or self.exclude_package_data)\n        matching package in src_dir.\n        \"\"\"\n        raw_patterns = itertools.chain(\n            spec.get('', []),\n            spec.get(package, []),\n        )\n        return (\n            # Each pattern has to be converted to a platform-specific path\n            os.path.join(src_dir, convert_path(pattern))\n            for pattern in raw_patterns\n        )\n\n\n# from Python docs\ndef _unique_everseen(iterable, key=None):\n    \"List unique elements, preserving order. Remember all elements ever seen.\"\n    # unique_everseen('AAAABBBCCDAABBB') --> A B C D\n    # unique_everseen('ABBCcAD', str.lower) --> A B C D\n    seen = set()\n    seen_add = seen.add\n    if key is None:\n        for element in filterfalse(seen.__contains__, iterable):\n            seen_add(element)\n            yield element\n    else:\n        for element in iterable:\n            k = key(element)\n            if k not in seen:\n                seen_add(k)\n                yield element\n\n\ndef assert_relative(path):\n    if not os.path.isabs(path):\n        return path\n    from distutils.errors import DistutilsSetupError\n\n    msg = textwrap.dedent(\"\"\"\n        Error: setup script specifies an absolute path:\n\n            %s\n\n        setup() arguments must *always* be /-separated paths relative to the\n        setup.py directory, *never* absolute paths.\n        \"\"\").lstrip() % path\n    raise DistutilsSetupError(msg)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/command/build_py.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/command/build_py.py	(date 1602088701621)
@@ -9,9 +9,6 @@
 import itertools
 import stat
 
-from setuptools.extern import six
-from setuptools.extern.six.moves import map, filter, filterfalse
-
 try:
     from setuptools.lib2to3_ex import Mixin2to3
 except ImportError:
@@ -73,9 +70,6 @@
         return orig.build_py.__getattr__(self, attr)
 
     def build_module(self, module, module_file, package):
-        if six.PY2 and isinstance(package, six.string_types):
-            # avoid errors on Python 2 when unicode is passed (#190)
-            package = package.split('.')
         outfile, copied = orig.build_py.build_module(self, module, module_file,
                                                      package)
         if copied:
@@ -249,7 +243,7 @@
     seen = set()
     seen_add = seen.add
     if key is None:
-        for element in filterfalse(seen.__contains__, iterable):
+        for element in itertools.filterfalse(seen.__contains__, iterable):
             seen_add(element)
             yield element
     else:
Index: env/lib/python3.8/site-packages/setuptools/command/test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport operator\nimport sys\nimport contextlib\nimport itertools\nimport unittest\nfrom distutils.errors import DistutilsError, DistutilsOptionError\nfrom distutils import log\nfrom unittest import TestLoader\n\nfrom setuptools.extern import six\nfrom setuptools.extern.six.moves import map, filter\n\nfrom pkg_resources import (resource_listdir, resource_exists, normalize_path,\n                           working_set, _namespace_packages, evaluate_marker,\n                           add_activation_listener, require, EntryPoint)\nfrom setuptools import Command\nfrom .build_py import _unique_everseen\n\n__metaclass__ = type\n\n\nclass ScanningLoader(TestLoader):\n\n    def __init__(self):\n        TestLoader.__init__(self)\n        self._visited = set()\n\n    def loadTestsFromModule(self, module, pattern=None):\n        \"\"\"Return a suite of all tests cases contained in the given module\n\n        If the module is a package, load tests from all the modules in it.\n        If the module has an ``additional_tests`` function, call it and add\n        the return value to the tests.\n        \"\"\"\n        if module in self._visited:\n            return None\n        self._visited.add(module)\n\n        tests = []\n        tests.append(TestLoader.loadTestsFromModule(self, module))\n\n        if hasattr(module, \"additional_tests\"):\n            tests.append(module.additional_tests())\n\n        if hasattr(module, '__path__'):\n            for file in resource_listdir(module.__name__, ''):\n                if file.endswith('.py') and file != '__init__.py':\n                    submodule = module.__name__ + '.' + file[:-3]\n                else:\n                    if resource_exists(module.__name__, file + '/__init__.py'):\n                        submodule = module.__name__ + '.' + file\n                    else:\n                        continue\n                tests.append(self.loadTestsFromName(submodule))\n\n        if len(tests) != 1:\n            return self.suiteClass(tests)\n        else:\n            return tests[0]  # don't create a nested suite for only one return\n\n\n# adapted from jaraco.classes.properties:NonDataProperty\nclass NonDataProperty:\n    def __init__(self, fget):\n        self.fget = fget\n\n    def __get__(self, obj, objtype=None):\n        if obj is None:\n            return self\n        return self.fget(obj)\n\n\nclass test(Command):\n    \"\"\"Command to run unit tests after in-place build\"\"\"\n\n    description = \"run unit tests after in-place build (deprecated)\"\n\n    user_options = [\n        ('test-module=', 'm', \"Run 'test_suite' in specified module\"),\n        ('test-suite=', 's',\n         \"Run single test, case or suite (e.g. 'module.test_suite')\"),\n        ('test-runner=', 'r', \"Test runner to use\"),\n    ]\n\n    def initialize_options(self):\n        self.test_suite = None\n        self.test_module = None\n        self.test_loader = None\n        self.test_runner = None\n\n    def finalize_options(self):\n\n        if self.test_suite and self.test_module:\n            msg = \"You may specify a module or a suite, but not both\"\n            raise DistutilsOptionError(msg)\n\n        if self.test_suite is None:\n            if self.test_module is None:\n                self.test_suite = self.distribution.test_suite\n            else:\n                self.test_suite = self.test_module + \".test_suite\"\n\n        if self.test_loader is None:\n            self.test_loader = getattr(self.distribution, 'test_loader', None)\n        if self.test_loader is None:\n            self.test_loader = \"setuptools.command.test:ScanningLoader\"\n        if self.test_runner is None:\n            self.test_runner = getattr(self.distribution, 'test_runner', None)\n\n    @NonDataProperty\n    def test_args(self):\n        return list(self._test_args())\n\n    def _test_args(self):\n        if not self.test_suite and sys.version_info >= (2, 7):\n            yield 'discover'\n        if self.verbose:\n            yield '--verbose'\n        if self.test_suite:\n            yield self.test_suite\n\n    def with_project_on_sys_path(self, func):\n        \"\"\"\n        Backward compatibility for project_on_sys_path context.\n        \"\"\"\n        with self.project_on_sys_path():\n            func()\n\n    @contextlib.contextmanager\n    def project_on_sys_path(self, include_dists=[]):\n        with_2to3 = not six.PY2 and getattr(\n            self.distribution, 'use_2to3', False)\n\n        if with_2to3:\n            # If we run 2to3 we can not do this inplace:\n\n            # Ensure metadata is up-to-date\n            self.reinitialize_command('build_py', inplace=0)\n            self.run_command('build_py')\n            bpy_cmd = self.get_finalized_command(\"build_py\")\n            build_path = normalize_path(bpy_cmd.build_lib)\n\n            # Build extensions\n            self.reinitialize_command('egg_info', egg_base=build_path)\n            self.run_command('egg_info')\n\n            self.reinitialize_command('build_ext', inplace=0)\n            self.run_command('build_ext')\n        else:\n            # Without 2to3 inplace works fine:\n            self.run_command('egg_info')\n\n            # Build extensions in-place\n            self.reinitialize_command('build_ext', inplace=1)\n            self.run_command('build_ext')\n\n        ei_cmd = self.get_finalized_command(\"egg_info\")\n\n        old_path = sys.path[:]\n        old_modules = sys.modules.copy()\n\n        try:\n            project_path = normalize_path(ei_cmd.egg_base)\n            sys.path.insert(0, project_path)\n            working_set.__init__()\n            add_activation_listener(lambda dist: dist.activate())\n            require('%s==%s' % (ei_cmd.egg_name, ei_cmd.egg_version))\n            with self.paths_on_pythonpath([project_path]):\n                yield\n        finally:\n            sys.path[:] = old_path\n            sys.modules.clear()\n            sys.modules.update(old_modules)\n            working_set.__init__()\n\n    @staticmethod\n    @contextlib.contextmanager\n    def paths_on_pythonpath(paths):\n        \"\"\"\n        Add the indicated paths to the head of the PYTHONPATH environment\n        variable so that subprocesses will also see the packages at\n        these paths.\n\n        Do this in a context that restores the value on exit.\n        \"\"\"\n        nothing = object()\n        orig_pythonpath = os.environ.get('PYTHONPATH', nothing)\n        current_pythonpath = os.environ.get('PYTHONPATH', '')\n        try:\n            prefix = os.pathsep.join(_unique_everseen(paths))\n            to_join = filter(None, [prefix, current_pythonpath])\n            new_path = os.pathsep.join(to_join)\n            if new_path:\n                os.environ['PYTHONPATH'] = new_path\n            yield\n        finally:\n            if orig_pythonpath is nothing:\n                os.environ.pop('PYTHONPATH', None)\n            else:\n                os.environ['PYTHONPATH'] = orig_pythonpath\n\n    @staticmethod\n    def install_dists(dist):\n        \"\"\"\n        Install the requirements indicated by self.distribution and\n        return an iterable of the dists that were built.\n        \"\"\"\n        ir_d = dist.fetch_build_eggs(dist.install_requires)\n        tr_d = dist.fetch_build_eggs(dist.tests_require or [])\n        er_d = dist.fetch_build_eggs(\n            v for k, v in dist.extras_require.items()\n            if k.startswith(':') and evaluate_marker(k[1:])\n        )\n        return itertools.chain(ir_d, tr_d, er_d)\n\n    def run(self):\n        self.announce(\n            \"WARNING: Testing via this command is deprecated and will be \"\n            \"removed in a future version. Users looking for a generic test \"\n            \"entry point independent of test runner are encouraged to use \"\n            \"tox.\",\n            log.WARN,\n        )\n\n        installed_dists = self.install_dists(self.distribution)\n\n        cmd = ' '.join(self._argv)\n        if self.dry_run:\n            self.announce('skipping \"%s\" (dry run)' % cmd)\n            return\n\n        self.announce('running \"%s\"' % cmd)\n\n        paths = map(operator.attrgetter('location'), installed_dists)\n        with self.paths_on_pythonpath(paths):\n            with self.project_on_sys_path():\n                self.run_tests()\n\n    def run_tests(self):\n        # Purge modules under test from sys.modules. The test loader will\n        # re-import them from the build location. Required when 2to3 is used\n        # with namespace packages.\n        if not six.PY2 and getattr(self.distribution, 'use_2to3', False):\n            module = self.test_suite.split('.')[0]\n            if module in _namespace_packages:\n                del_modules = []\n                if module in sys.modules:\n                    del_modules.append(module)\n                module += '.'\n                for name in sys.modules:\n                    if name.startswith(module):\n                        del_modules.append(name)\n                list(map(sys.modules.__delitem__, del_modules))\n\n        test = unittest.main(\n            None, None, self._argv,\n            testLoader=self._resolve_as_ep(self.test_loader),\n            testRunner=self._resolve_as_ep(self.test_runner),\n            exit=False,\n        )\n        if not test.result.wasSuccessful():\n            msg = 'Test failed: %s' % test.result\n            self.announce(msg, log.ERROR)\n            raise DistutilsError(msg)\n\n    @property\n    def _argv(self):\n        return ['unittest'] + self.test_args\n\n    @staticmethod\n    def _resolve_as_ep(val):\n        \"\"\"\n        Load the indicated attribute value, called, as a as if it were\n        specified as an entry point.\n        \"\"\"\n        if val is None:\n            return\n        parsed = EntryPoint.parse(\"x=\" + val)\n        return parsed.resolve()()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/command/test.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/command/test.py	(date 1602088701625)
@@ -8,17 +8,12 @@
 from distutils import log
 from unittest import TestLoader
 
-from setuptools.extern import six
-from setuptools.extern.six.moves import map, filter
-
 from pkg_resources import (resource_listdir, resource_exists, normalize_path,
                            working_set, _namespace_packages, evaluate_marker,
                            add_activation_listener, require, EntryPoint)
 from setuptools import Command
 from .build_py import _unique_everseen
 
-__metaclass__ = type
-
 
 class ScanningLoader(TestLoader):
 
@@ -129,8 +124,7 @@
 
     @contextlib.contextmanager
     def project_on_sys_path(self, include_dists=[]):
-        with_2to3 = not six.PY2 and getattr(
-            self.distribution, 'use_2to3', False)
+        with_2to3 = getattr(self.distribution, 'use_2to3', False)
 
         if with_2to3:
             # If we run 2to3 we can not do this inplace:
@@ -241,7 +235,7 @@
         # Purge modules under test from sys.modules. The test loader will
         # re-import them from the build location. Required when 2to3 is used
         # with namespace packages.
-        if not six.PY2 and getattr(self.distribution, 'use_2to3', False):
+        if getattr(self.distribution, 'use_2to3', False):
             module = self.test_suite.split('.')[0]
             if module in _namespace_packages:
                 del_modules = []
Index: env/lib/python3.8/site-packages/setuptools/command/develop.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from distutils.util import convert_path\nfrom distutils import log\nfrom distutils.errors import DistutilsError, DistutilsOptionError\nimport os\nimport glob\nimport io\n\nfrom setuptools.extern import six\n\nimport pkg_resources\nfrom setuptools.command.easy_install import easy_install\nfrom setuptools import namespaces\nimport setuptools\n\n__metaclass__ = type\n\n\nclass develop(namespaces.DevelopInstaller, easy_install):\n    \"\"\"Set up package for development\"\"\"\n\n    description = \"install package in 'development mode'\"\n\n    user_options = easy_install.user_options + [\n        (\"uninstall\", \"u\", \"Uninstall this source package\"),\n        (\"egg-path=\", None, \"Set the path to be used in the .egg-link file\"),\n    ]\n\n    boolean_options = easy_install.boolean_options + ['uninstall']\n\n    command_consumes_arguments = False  # override base\n\n    def run(self):\n        if self.uninstall:\n            self.multi_version = True\n            self.uninstall_link()\n            self.uninstall_namespaces()\n        else:\n            self.install_for_development()\n        self.warn_deprecated_options()\n\n    def initialize_options(self):\n        self.uninstall = None\n        self.egg_path = None\n        easy_install.initialize_options(self)\n        self.setup_path = None\n        self.always_copy_from = '.'  # always copy eggs installed in curdir\n\n    def finalize_options(self):\n        ei = self.get_finalized_command(\"egg_info\")\n        if ei.broken_egg_info:\n            template = \"Please rename %r to %r before using 'develop'\"\n            args = ei.egg_info, ei.broken_egg_info\n            raise DistutilsError(template % args)\n        self.args = [ei.egg_name]\n\n        easy_install.finalize_options(self)\n        self.expand_basedirs()\n        self.expand_dirs()\n        # pick up setup-dir .egg files only: no .egg-info\n        self.package_index.scan(glob.glob('*.egg'))\n\n        egg_link_fn = ei.egg_name + '.egg-link'\n        self.egg_link = os.path.join(self.install_dir, egg_link_fn)\n        self.egg_base = ei.egg_base\n        if self.egg_path is None:\n            self.egg_path = os.path.abspath(ei.egg_base)\n\n        target = pkg_resources.normalize_path(self.egg_base)\n        egg_path = pkg_resources.normalize_path(\n            os.path.join(self.install_dir, self.egg_path))\n        if egg_path != target:\n            raise DistutilsOptionError(\n                \"--egg-path must be a relative path from the install\"\n                \" directory to \" + target\n            )\n\n        # Make a distribution for the package's source\n        self.dist = pkg_resources.Distribution(\n            target,\n            pkg_resources.PathMetadata(target, os.path.abspath(ei.egg_info)),\n            project_name=ei.egg_name\n        )\n\n        self.setup_path = self._resolve_setup_path(\n            self.egg_base,\n            self.install_dir,\n            self.egg_path,\n        )\n\n    @staticmethod\n    def _resolve_setup_path(egg_base, install_dir, egg_path):\n        \"\"\"\n        Generate a path from egg_base back to '.' where the\n        setup script resides and ensure that path points to the\n        setup path from $install_dir/$egg_path.\n        \"\"\"\n        path_to_setup = egg_base.replace(os.sep, '/').rstrip('/')\n        if path_to_setup != os.curdir:\n            path_to_setup = '../' * (path_to_setup.count('/') + 1)\n        resolved = pkg_resources.normalize_path(\n            os.path.join(install_dir, egg_path, path_to_setup)\n        )\n        if resolved != pkg_resources.normalize_path(os.curdir):\n            raise DistutilsOptionError(\n                \"Can't get a consistent path to setup script from\"\n                \" installation directory\", resolved,\n                pkg_resources.normalize_path(os.curdir))\n        return path_to_setup\n\n    def install_for_development(self):\n        if not six.PY2 and getattr(self.distribution, 'use_2to3', False):\n            # If we run 2to3 we can not do this inplace:\n\n            # Ensure metadata is up-to-date\n            self.reinitialize_command('build_py', inplace=0)\n            self.run_command('build_py')\n            bpy_cmd = self.get_finalized_command(\"build_py\")\n            build_path = pkg_resources.normalize_path(bpy_cmd.build_lib)\n\n            # Build extensions\n            self.reinitialize_command('egg_info', egg_base=build_path)\n            self.run_command('egg_info')\n\n            self.reinitialize_command('build_ext', inplace=0)\n            self.run_command('build_ext')\n\n            # Fixup egg-link and easy-install.pth\n            ei_cmd = self.get_finalized_command(\"egg_info\")\n            self.egg_path = build_path\n            self.dist.location = build_path\n            # XXX\n            self.dist._provider = pkg_resources.PathMetadata(\n                build_path, ei_cmd.egg_info)\n        else:\n            # Without 2to3 inplace works fine:\n            self.run_command('egg_info')\n\n            # Build extensions in-place\n            self.reinitialize_command('build_ext', inplace=1)\n            self.run_command('build_ext')\n\n        if setuptools.bootstrap_install_from:\n            self.easy_install(setuptools.bootstrap_install_from)\n            setuptools.bootstrap_install_from = None\n\n        self.install_namespaces()\n\n        # create an .egg-link in the installation dir, pointing to our egg\n        log.info(\"Creating %s (link to %s)\", self.egg_link, self.egg_base)\n        if not self.dry_run:\n            with open(self.egg_link, \"w\") as f:\n                f.write(self.egg_path + \"\\n\" + self.setup_path)\n        # postprocess the installed distro, fixing up .pth, installing scripts,\n        # and handling requirements\n        self.process_distribution(None, self.dist, not self.no_deps)\n\n    def uninstall_link(self):\n        if os.path.exists(self.egg_link):\n            log.info(\"Removing %s (link to %s)\", self.egg_link, self.egg_base)\n            egg_link_file = open(self.egg_link)\n            contents = [line.rstrip() for line in egg_link_file]\n            egg_link_file.close()\n            if contents not in ([self.egg_path],\n                                [self.egg_path, self.setup_path]):\n                log.warn(\"Link points to %s: uninstall aborted\", contents)\n                return\n            if not self.dry_run:\n                os.unlink(self.egg_link)\n        if not self.dry_run:\n            self.update_pth(self.dist)  # remove any .pth link to us\n        if self.distribution.scripts:\n            # XXX should also check for entry point scripts!\n            log.warn(\"Note: you must uninstall or replace scripts manually!\")\n\n    def install_egg_scripts(self, dist):\n        if dist is not self.dist:\n            # Installing a dependency, so fall back to normal behavior\n            return easy_install.install_egg_scripts(self, dist)\n\n        # create wrapper scripts in the script dir, pointing to dist.scripts\n\n        # new-style...\n        self.install_wrapper_scripts(dist)\n\n        # ...and old-style\n        for script_name in self.distribution.scripts or []:\n            script_path = os.path.abspath(convert_path(script_name))\n            script_name = os.path.basename(script_path)\n            with io.open(script_path) as strm:\n                script_text = strm.read()\n            self.install_script(dist, script_name, script_text, script_path)\n\n    def install_wrapper_scripts(self, dist):\n        dist = VersionlessRequirement(dist)\n        return easy_install.install_wrapper_scripts(self, dist)\n\n\nclass VersionlessRequirement:\n    \"\"\"\n    Adapt a pkg_resources.Distribution to simply return the project\n    name as the 'requirement' so that scripts will work across\n    multiple versions.\n\n    >>> from pkg_resources import Distribution\n    >>> dist = Distribution(project_name='foo', version='1.0')\n    >>> str(dist.as_requirement())\n    'foo==1.0'\n    >>> adapted_dist = VersionlessRequirement(dist)\n    >>> str(adapted_dist.as_requirement())\n    'foo'\n    \"\"\"\n\n    def __init__(self, dist):\n        self.__dist = dist\n\n    def __getattr__(self, name):\n        return getattr(self.__dist, name)\n\n    def as_requirement(self):\n        return self.project_name\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/command/develop.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/command/develop.py	(date 1602088701621)
@@ -5,15 +5,11 @@
 import glob
 import io
 
-from setuptools.extern import six
-
 import pkg_resources
 from setuptools.command.easy_install import easy_install
 from setuptools import namespaces
 import setuptools
 
-__metaclass__ = type
-
 
 class develop(namespaces.DevelopInstaller, easy_install):
     """Set up package for development"""
@@ -108,7 +104,7 @@
         return path_to_setup
 
     def install_for_development(self):
-        if not six.PY2 and getattr(self.distribution, 'use_2to3', False):
+        if getattr(self.distribution, 'use_2to3', False):
             # If we run 2to3 we can not do this inplace:
 
             # Ensure metadata is up-to-date
Index: .gitignore
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>env/
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .gitignore	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ .gitignore	(date 1602089686269)
@@ -1,1 +1,2 @@
-env/
\ No newline at end of file
+env
+.idea
\ No newline at end of file
Index: env/lib/python3.8/site-packages/setuptools/command/easy_install.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\nEasy Install\n------------\n\nA tool for doing automatic download/extract/build of distutils-based Python\npackages.  For detailed documentation, see the accompanying EasyInstall.txt\nfile, or visit the `EasyInstall home page`__.\n\n__ https://setuptools.readthedocs.io/en/latest/easy_install.html\n\n\"\"\"\n\nfrom glob import glob\nfrom distutils.util import get_platform\nfrom distutils.util import convert_path, subst_vars\nfrom distutils.errors import (\n    DistutilsArgError, DistutilsOptionError,\n    DistutilsError, DistutilsPlatformError,\n)\nfrom distutils.command.install import INSTALL_SCHEMES, SCHEME_KEYS\nfrom distutils import log, dir_util\nfrom distutils.command.build_scripts import first_line_re\nfrom distutils.spawn import find_executable\nimport sys\nimport os\nimport zipimport\nimport shutil\nimport tempfile\nimport zipfile\nimport re\nimport stat\nimport random\nimport textwrap\nimport warnings\nimport site\nimport struct\nimport contextlib\nimport subprocess\nimport shlex\nimport io\n\n\nfrom sysconfig import get_config_vars, get_path\n\nfrom setuptools import SetuptoolsDeprecationWarning\n\nfrom setuptools.extern import six\nfrom setuptools.extern.six.moves import configparser, map\n\nfrom setuptools import Command\nfrom setuptools.sandbox import run_setup\nfrom setuptools.py27compat import rmtree_safe\nfrom setuptools.command import setopt\nfrom setuptools.archive_util import unpack_archive\nfrom setuptools.package_index import (\n    PackageIndex, parse_requirement_arg, URL_SCHEME,\n)\nfrom setuptools.command import bdist_egg, egg_info\nfrom setuptools.wheel import Wheel\nfrom pkg_resources import (\n    yield_lines, normalize_path, resource_string, ensure_directory,\n    get_distribution, find_distributions, Environment, Requirement,\n    Distribution, PathMetadata, EggMetadata, WorkingSet, DistributionNotFound,\n    VersionConflict, DEVELOP_DIST,\n)\nimport pkg_resources\n\n__metaclass__ = type\n\n# Turn on PEP440Warnings\nwarnings.filterwarnings(\"default\", category=pkg_resources.PEP440Warning)\n\n__all__ = [\n    'samefile', 'easy_install', 'PthDistributions', 'extract_wininst_cfg',\n    'main', 'get_exe_prefixes',\n]\n\n\ndef is_64bit():\n    return struct.calcsize(\"P\") == 8\n\n\ndef samefile(p1, p2):\n    \"\"\"\n    Determine if two paths reference the same file.\n\n    Augments os.path.samefile to work on Windows and\n    suppresses errors if the path doesn't exist.\n    \"\"\"\n    both_exist = os.path.exists(p1) and os.path.exists(p2)\n    use_samefile = hasattr(os.path, 'samefile') and both_exist\n    if use_samefile:\n        return os.path.samefile(p1, p2)\n    norm_p1 = os.path.normpath(os.path.normcase(p1))\n    norm_p2 = os.path.normpath(os.path.normcase(p2))\n    return norm_p1 == norm_p2\n\n\nif six.PY2:\n\n    def _to_bytes(s):\n        return s\n\n    def isascii(s):\n        try:\n            six.text_type(s, 'ascii')\n            return True\n        except UnicodeError:\n            return False\nelse:\n\n    def _to_bytes(s):\n        return s.encode('utf8')\n\n    def isascii(s):\n        try:\n            s.encode('ascii')\n            return True\n        except UnicodeError:\n            return False\n\n\ndef _one_liner(text):\n    return textwrap.dedent(text).strip().replace('\\n', '; ')\n\n\nclass easy_install(Command):\n    \"\"\"Manage a download/build/install process\"\"\"\n    description = \"Find/get/install Python packages\"\n    command_consumes_arguments = True\n\n    user_options = [\n        ('prefix=', None, \"installation prefix\"),\n        (\"zip-ok\", \"z\", \"install package as a zipfile\"),\n        (\"multi-version\", \"m\", \"make apps have to require() a version\"),\n        (\"upgrade\", \"U\", \"force upgrade (searches PyPI for latest versions)\"),\n        (\"install-dir=\", \"d\", \"install package to DIR\"),\n        (\"script-dir=\", \"s\", \"install scripts to DIR\"),\n        (\"exclude-scripts\", \"x\", \"Don't install scripts\"),\n        (\"always-copy\", \"a\", \"Copy all needed packages to install dir\"),\n        (\"index-url=\", \"i\", \"base URL of Python Package Index\"),\n        (\"find-links=\", \"f\", \"additional URL(s) to search for packages\"),\n        (\"build-directory=\", \"b\",\n         \"download/extract/build in DIR; keep the results\"),\n        ('optimize=', 'O',\n         \"also compile with optimization: -O1 for \\\"python -O\\\", \"\n         \"-O2 for \\\"python -OO\\\", and -O0 to disable [default: -O0]\"),\n        ('record=', None,\n         \"filename in which to record list of installed files\"),\n        ('always-unzip', 'Z', \"don't install as a zipfile, no matter what\"),\n        ('site-dirs=', 'S', \"list of directories where .pth files work\"),\n        ('editable', 'e', \"Install specified packages in editable form\"),\n        ('no-deps', 'N', \"don't install dependencies\"),\n        ('allow-hosts=', 'H', \"pattern(s) that hostnames must match\"),\n        ('local-snapshots-ok', 'l',\n         \"allow building eggs from local checkouts\"),\n        ('version', None, \"print version information and exit\"),\n        ('no-find-links', None,\n         \"Don't load find-links defined in packages being installed\"),\n        ('user', None, \"install in user site-package '%s'\" % site.USER_SITE)\n    ]\n    boolean_options = [\n        'zip-ok', 'multi-version', 'exclude-scripts', 'upgrade', 'always-copy',\n        'editable',\n        'no-deps', 'local-snapshots-ok', 'version',\n        'user'\n    ]\n\n    negative_opt = {'always-unzip': 'zip-ok'}\n    create_index = PackageIndex\n\n    def initialize_options(self):\n        # the --user option seems to be an opt-in one,\n        # so the default should be False.\n        self.user = 0\n        self.zip_ok = self.local_snapshots_ok = None\n        self.install_dir = self.script_dir = self.exclude_scripts = None\n        self.index_url = None\n        self.find_links = None\n        self.build_directory = None\n        self.args = None\n        self.optimize = self.record = None\n        self.upgrade = self.always_copy = self.multi_version = None\n        self.editable = self.no_deps = self.allow_hosts = None\n        self.root = self.prefix = self.no_report = None\n        self.version = None\n        self.install_purelib = None  # for pure module distributions\n        self.install_platlib = None  # non-pure (dists w/ extensions)\n        self.install_headers = None  # for C/C++ headers\n        self.install_lib = None  # set to either purelib or platlib\n        self.install_scripts = None\n        self.install_data = None\n        self.install_base = None\n        self.install_platbase = None\n        if site.ENABLE_USER_SITE:\n            self.install_userbase = site.USER_BASE\n            self.install_usersite = site.USER_SITE\n        else:\n            self.install_userbase = None\n            self.install_usersite = None\n        self.no_find_links = None\n\n        # Options not specifiable via command line\n        self.package_index = None\n        self.pth_file = self.always_copy_from = None\n        self.site_dirs = None\n        self.installed_projects = {}\n        # Always read easy_install options, even if we are subclassed, or have\n        # an independent instance created.  This ensures that defaults will\n        # always come from the standard configuration file(s)' \"easy_install\"\n        # section, even if this is a \"develop\" or \"install\" command, or some\n        # other embedding.\n        self._dry_run = None\n        self.verbose = self.distribution.verbose\n        self.distribution._set_command_options(\n            self, self.distribution.get_option_dict('easy_install')\n        )\n\n    def delete_blockers(self, blockers):\n        extant_blockers = (\n            filename for filename in blockers\n            if os.path.exists(filename) or os.path.islink(filename)\n        )\n        list(map(self._delete_path, extant_blockers))\n\n    def _delete_path(self, path):\n        log.info(\"Deleting %s\", path)\n        if self.dry_run:\n            return\n\n        is_tree = os.path.isdir(path) and not os.path.islink(path)\n        remover = rmtree if is_tree else os.unlink\n        remover(path)\n\n    @staticmethod\n    def _render_version():\n        \"\"\"\n        Render the Setuptools version and installation details, then exit.\n        \"\"\"\n        ver = '{}.{}'.format(*sys.version_info)\n        dist = get_distribution('setuptools')\n        tmpl = 'setuptools {dist.version} from {dist.location} (Python {ver})'\n        print(tmpl.format(**locals()))\n        raise SystemExit()\n\n    def finalize_options(self):\n        self.version and self._render_version()\n\n        py_version = sys.version.split()[0]\n        prefix, exec_prefix = get_config_vars('prefix', 'exec_prefix')\n\n        self.config_vars = {\n            'dist_name': self.distribution.get_name(),\n            'dist_version': self.distribution.get_version(),\n            'dist_fullname': self.distribution.get_fullname(),\n            'py_version': py_version,\n            'py_version_short': py_version[0:3],\n            'py_version_nodot': py_version[0] + py_version[2],\n            'sys_prefix': prefix,\n            'prefix': prefix,\n            'sys_exec_prefix': exec_prefix,\n            'exec_prefix': exec_prefix,\n            # Only python 3.2+ has abiflags\n            'abiflags': getattr(sys, 'abiflags', ''),\n        }\n\n        if site.ENABLE_USER_SITE:\n            self.config_vars['userbase'] = self.install_userbase\n            self.config_vars['usersite'] = self.install_usersite\n\n        elif self.user:\n            log.warn(\"WARNING: The user site-packages directory is disabled.\")\n\n        self._fix_install_dir_for_user_site()\n\n        self.expand_basedirs()\n        self.expand_dirs()\n\n        self._expand(\n            'install_dir', 'script_dir', 'build_directory',\n            'site_dirs',\n        )\n        # If a non-default installation directory was specified, default the\n        # script directory to match it.\n        if self.script_dir is None:\n            self.script_dir = self.install_dir\n\n        if self.no_find_links is None:\n            self.no_find_links = False\n\n        # Let install_dir get set by install_lib command, which in turn\n        # gets its info from the install command, and takes into account\n        # --prefix and --home and all that other crud.\n        self.set_undefined_options(\n            'install_lib', ('install_dir', 'install_dir')\n        )\n        # Likewise, set default script_dir from 'install_scripts.install_dir'\n        self.set_undefined_options(\n            'install_scripts', ('install_dir', 'script_dir')\n        )\n\n        if self.user and self.install_purelib:\n            self.install_dir = self.install_purelib\n            self.script_dir = self.install_scripts\n        # default --record from the install command\n        self.set_undefined_options('install', ('record', 'record'))\n        # Should this be moved to the if statement below? It's not used\n        # elsewhere\n        normpath = map(normalize_path, sys.path)\n        self.all_site_dirs = get_site_dirs()\n        if self.site_dirs is not None:\n            site_dirs = [\n                os.path.expanduser(s.strip()) for s in\n                self.site_dirs.split(',')\n            ]\n            for d in site_dirs:\n                if not os.path.isdir(d):\n                    log.warn(\"%s (in --site-dirs) does not exist\", d)\n                elif normalize_path(d) not in normpath:\n                    raise DistutilsOptionError(\n                        d + \" (in --site-dirs) is not on sys.path\"\n                    )\n                else:\n                    self.all_site_dirs.append(normalize_path(d))\n        if not self.editable:\n            self.check_site_dir()\n        self.index_url = self.index_url or \"https://pypi.org/simple/\"\n        self.shadow_path = self.all_site_dirs[:]\n        for path_item in self.install_dir, normalize_path(self.script_dir):\n            if path_item not in self.shadow_path:\n                self.shadow_path.insert(0, path_item)\n\n        if self.allow_hosts is not None:\n            hosts = [s.strip() for s in self.allow_hosts.split(',')]\n        else:\n            hosts = ['*']\n        if self.package_index is None:\n            self.package_index = self.create_index(\n                self.index_url, search_path=self.shadow_path, hosts=hosts,\n            )\n        self.local_index = Environment(self.shadow_path + sys.path)\n\n        if self.find_links is not None:\n            if isinstance(self.find_links, six.string_types):\n                self.find_links = self.find_links.split()\n        else:\n            self.find_links = []\n        if self.local_snapshots_ok:\n            self.package_index.scan_egg_links(self.shadow_path + sys.path)\n        if not self.no_find_links:\n            self.package_index.add_find_links(self.find_links)\n        self.set_undefined_options('install_lib', ('optimize', 'optimize'))\n        if not isinstance(self.optimize, int):\n            try:\n                self.optimize = int(self.optimize)\n                if not (0 <= self.optimize <= 2):\n                    raise ValueError\n            except ValueError as e:\n                raise DistutilsOptionError(\n                    \"--optimize must be 0, 1, or 2\"\n                ) from e\n\n        if self.editable and not self.build_directory:\n            raise DistutilsArgError(\n                \"Must specify a build directory (-b) when using --editable\"\n            )\n        if not self.args:\n            raise DistutilsArgError(\n                \"No urls, filenames, or requirements specified (see --help)\")\n\n        self.outputs = []\n\n    def _fix_install_dir_for_user_site(self):\n        \"\"\"\n        Fix the install_dir if \"--user\" was used.\n        \"\"\"\n        if not self.user or not site.ENABLE_USER_SITE:\n            return\n\n        self.create_home_path()\n        if self.install_userbase is None:\n            msg = \"User base directory is not specified\"\n            raise DistutilsPlatformError(msg)\n        self.install_base = self.install_platbase = self.install_userbase\n        scheme_name = os.name.replace('posix', 'unix') + '_user'\n        self.select_scheme(scheme_name)\n\n    def _expand_attrs(self, attrs):\n        for attr in attrs:\n            val = getattr(self, attr)\n            if val is not None:\n                if os.name == 'posix' or os.name == 'nt':\n                    val = os.path.expanduser(val)\n                val = subst_vars(val, self.config_vars)\n                setattr(self, attr, val)\n\n    def expand_basedirs(self):\n        \"\"\"Calls `os.path.expanduser` on install_base, install_platbase and\n        root.\"\"\"\n        self._expand_attrs(['install_base', 'install_platbase', 'root'])\n\n    def expand_dirs(self):\n        \"\"\"Calls `os.path.expanduser` on install dirs.\"\"\"\n        dirs = [\n            'install_purelib',\n            'install_platlib',\n            'install_lib',\n            'install_headers',\n            'install_scripts',\n            'install_data',\n        ]\n        self._expand_attrs(dirs)\n\n    def run(self, show_deprecation=True):\n        if show_deprecation:\n            self.announce(\n                \"WARNING: The easy_install command is deprecated \"\n                \"and will be removed in a future version.\",\n                log.WARN,\n            )\n        if self.verbose != self.distribution.verbose:\n            log.set_verbosity(self.verbose)\n        try:\n            for spec in self.args:\n                self.easy_install(spec, not self.no_deps)\n            if self.record:\n                outputs = self.outputs\n                if self.root:  # strip any package prefix\n                    root_len = len(self.root)\n                    for counter in range(len(outputs)):\n                        outputs[counter] = outputs[counter][root_len:]\n                from distutils import file_util\n\n                self.execute(\n                    file_util.write_file, (self.record, outputs),\n                    \"writing list of installed files to '%s'\" %\n                    self.record\n                )\n            self.warn_deprecated_options()\n        finally:\n            log.set_verbosity(self.distribution.verbose)\n\n    def pseudo_tempname(self):\n        \"\"\"Return a pseudo-tempname base in the install directory.\n        This code is intentionally naive; if a malicious party can write to\n        the target directory you're already in deep doodoo.\n        \"\"\"\n        try:\n            pid = os.getpid()\n        except Exception:\n            pid = random.randint(0, sys.maxsize)\n        return os.path.join(self.install_dir, \"test-easy-install-%s\" % pid)\n\n    def warn_deprecated_options(self):\n        pass\n\n    def check_site_dir(self):\n        \"\"\"Verify that self.install_dir is .pth-capable dir, if needed\"\"\"\n\n        instdir = normalize_path(self.install_dir)\n        pth_file = os.path.join(instdir, 'easy-install.pth')\n\n        if not os.path.exists(instdir):\n            try:\n                os.makedirs(instdir)\n            except (OSError, IOError):\n                self.cant_write_to_target()\n\n        # Is it a configured, PYTHONPATH, implicit, or explicit site dir?\n        is_site_dir = instdir in self.all_site_dirs\n\n        if not is_site_dir and not self.multi_version:\n            # No?  Then directly test whether it does .pth file processing\n            is_site_dir = self.check_pth_processing()\n        else:\n            # make sure we can write to target dir\n            testfile = self.pseudo_tempname() + '.write-test'\n            test_exists = os.path.exists(testfile)\n            try:\n                if test_exists:\n                    os.unlink(testfile)\n                open(testfile, 'w').close()\n                os.unlink(testfile)\n            except (OSError, IOError):\n                self.cant_write_to_target()\n\n        if not is_site_dir and not self.multi_version:\n            # Can't install non-multi to non-site dir with easy_install\n            pythonpath = os.environ.get('PYTHONPATH', '')\n            log.warn(self.__no_default_msg, self.install_dir, pythonpath)\n\n        if is_site_dir:\n            if self.pth_file is None:\n                self.pth_file = PthDistributions(pth_file, self.all_site_dirs)\n        else:\n            self.pth_file = None\n\n        if self.multi_version and not os.path.exists(pth_file):\n            self.pth_file = None  # don't create a .pth file\n        self.install_dir = instdir\n\n    __cant_write_msg = textwrap.dedent(\"\"\"\n        can't create or remove files in install directory\n\n        The following error occurred while trying to add or remove files in the\n        installation directory:\n\n            %s\n\n        The installation directory you specified (via --install-dir, --prefix, or\n        the distutils default setting) was:\n\n            %s\n        \"\"\").lstrip()  # noqa\n\n    __not_exists_id = textwrap.dedent(\"\"\"\n        This directory does not currently exist.  Please create it and try again, or\n        choose a different installation directory (using the -d or --install-dir\n        option).\n        \"\"\").lstrip()  # noqa\n\n    __access_msg = textwrap.dedent(\"\"\"\n        Perhaps your account does not have write access to this directory?  If the\n        installation directory is a system-owned directory, you may need to sign in\n        as the administrator or \"root\" account.  If you do not have administrative\n        access to this machine, you may wish to choose a different installation\n        directory, preferably one that is listed in your PYTHONPATH environment\n        variable.\n\n        For information on other options, you may wish to consult the\n        documentation at:\n\n          https://setuptools.readthedocs.io/en/latest/easy_install.html\n\n        Please make the appropriate changes for your system and try again.\n        \"\"\").lstrip()  # noqa\n\n    def cant_write_to_target(self):\n        msg = self.__cant_write_msg % (sys.exc_info()[1], self.install_dir,)\n\n        if not os.path.exists(self.install_dir):\n            msg += '\\n' + self.__not_exists_id\n        else:\n            msg += '\\n' + self.__access_msg\n        raise DistutilsError(msg)\n\n    def check_pth_processing(self):\n        \"\"\"Empirically verify whether .pth files are supported in inst. dir\"\"\"\n        instdir = self.install_dir\n        log.info(\"Checking .pth file support in %s\", instdir)\n        pth_file = self.pseudo_tempname() + \".pth\"\n        ok_file = pth_file + '.ok'\n        ok_exists = os.path.exists(ok_file)\n        tmpl = _one_liner(\"\"\"\n            import os\n            f = open({ok_file!r}, 'w')\n            f.write('OK')\n            f.close()\n            \"\"\") + '\\n'\n        try:\n            if ok_exists:\n                os.unlink(ok_file)\n            dirname = os.path.dirname(ok_file)\n            os.makedirs(dirname, exist_ok=True)\n            f = open(pth_file, 'w')\n        except (OSError, IOError):\n            self.cant_write_to_target()\n        else:\n            try:\n                f.write(tmpl.format(**locals()))\n                f.close()\n                f = None\n                executable = sys.executable\n                if os.name == 'nt':\n                    dirname, basename = os.path.split(executable)\n                    alt = os.path.join(dirname, 'pythonw.exe')\n                    use_alt = (\n                        basename.lower() == 'python.exe' and\n                        os.path.exists(alt)\n                    )\n                    if use_alt:\n                        # use pythonw.exe to avoid opening a console window\n                        executable = alt\n\n                from distutils.spawn import spawn\n\n                spawn([executable, '-E', '-c', 'pass'], 0)\n\n                if os.path.exists(ok_file):\n                    log.info(\n                        \"TEST PASSED: %s appears to support .pth files\",\n                        instdir\n                    )\n                    return True\n            finally:\n                if f:\n                    f.close()\n                if os.path.exists(ok_file):\n                    os.unlink(ok_file)\n                if os.path.exists(pth_file):\n                    os.unlink(pth_file)\n        if not self.multi_version:\n            log.warn(\"TEST FAILED: %s does NOT support .pth files\", instdir)\n        return False\n\n    def install_egg_scripts(self, dist):\n        \"\"\"Write all the scripts for `dist`, unless scripts are excluded\"\"\"\n        if not self.exclude_scripts and dist.metadata_isdir('scripts'):\n            for script_name in dist.metadata_listdir('scripts'):\n                if dist.metadata_isdir('scripts/' + script_name):\n                    # The \"script\" is a directory, likely a Python 3\n                    # __pycache__ directory, so skip it.\n                    continue\n                self.install_script(\n                    dist, script_name,\n                    dist.get_metadata('scripts/' + script_name)\n                )\n        self.install_wrapper_scripts(dist)\n\n    def add_output(self, path):\n        if os.path.isdir(path):\n            for base, dirs, files in os.walk(path):\n                for filename in files:\n                    self.outputs.append(os.path.join(base, filename))\n        else:\n            self.outputs.append(path)\n\n    def not_editable(self, spec):\n        if self.editable:\n            raise DistutilsArgError(\n                \"Invalid argument %r: you can't use filenames or URLs \"\n                \"with --editable (except via the --find-links option).\"\n                % (spec,)\n            )\n\n    def check_editable(self, spec):\n        if not self.editable:\n            return\n\n        if os.path.exists(os.path.join(self.build_directory, spec.key)):\n            raise DistutilsArgError(\n                \"%r already exists in %s; can't do a checkout there\" %\n                (spec.key, self.build_directory)\n            )\n\n    @contextlib.contextmanager\n    def _tmpdir(self):\n        tmpdir = tempfile.mkdtemp(prefix=u\"easy_install-\")\n        try:\n            # cast to str as workaround for #709 and #710 and #712\n            yield str(tmpdir)\n        finally:\n            os.path.exists(tmpdir) and rmtree(rmtree_safe(tmpdir))\n\n    def easy_install(self, spec, deps=False):\n        with self._tmpdir() as tmpdir:\n            if not isinstance(spec, Requirement):\n                if URL_SCHEME(spec):\n                    # It's a url, download it to tmpdir and process\n                    self.not_editable(spec)\n                    dl = self.package_index.download(spec, tmpdir)\n                    return self.install_item(None, dl, tmpdir, deps, True)\n\n                elif os.path.exists(spec):\n                    # Existing file or directory, just process it directly\n                    self.not_editable(spec)\n                    return self.install_item(None, spec, tmpdir, deps, True)\n                else:\n                    spec = parse_requirement_arg(spec)\n\n            self.check_editable(spec)\n            dist = self.package_index.fetch_distribution(\n                spec, tmpdir, self.upgrade, self.editable,\n                not self.always_copy, self.local_index\n            )\n            if dist is None:\n                msg = \"Could not find suitable distribution for %r\" % spec\n                if self.always_copy:\n                    msg += \" (--always-copy skips system and development eggs)\"\n                raise DistutilsError(msg)\n            elif dist.precedence == DEVELOP_DIST:\n                # .egg-info dists don't need installing, just process deps\n                self.process_distribution(spec, dist, deps, \"Using\")\n                return dist\n            else:\n                return self.install_item(spec, dist.location, tmpdir, deps)\n\n    def install_item(self, spec, download, tmpdir, deps, install_needed=False):\n\n        # Installation is also needed if file in tmpdir or is not an egg\n        install_needed = install_needed or self.always_copy\n        install_needed = install_needed or os.path.dirname(download) == tmpdir\n        install_needed = install_needed or not download.endswith('.egg')\n        install_needed = install_needed or (\n            self.always_copy_from is not None and\n            os.path.dirname(normalize_path(download)) ==\n            normalize_path(self.always_copy_from)\n        )\n\n        if spec and not install_needed:\n            # at this point, we know it's a local .egg, we just don't know if\n            # it's already installed.\n            for dist in self.local_index[spec.project_name]:\n                if dist.location == download:\n                    break\n            else:\n                install_needed = True  # it's not in the local index\n\n        log.info(\"Processing %s\", os.path.basename(download))\n\n        if install_needed:\n            dists = self.install_eggs(spec, download, tmpdir)\n            for dist in dists:\n                self.process_distribution(spec, dist, deps)\n        else:\n            dists = [self.egg_distribution(download)]\n            self.process_distribution(spec, dists[0], deps, \"Using\")\n\n        if spec is not None:\n            for dist in dists:\n                if dist in spec:\n                    return dist\n\n    def select_scheme(self, name):\n        \"\"\"Sets the install directories by applying the install schemes.\"\"\"\n        # it's the caller's problem if they supply a bad name!\n        scheme = INSTALL_SCHEMES[name]\n        for key in SCHEME_KEYS:\n            attrname = 'install_' + key\n            if getattr(self, attrname) is None:\n                setattr(self, attrname, scheme[key])\n\n    def process_distribution(self, requirement, dist, deps=True, *info):\n        self.update_pth(dist)\n        self.package_index.add(dist)\n        if dist in self.local_index[dist.key]:\n            self.local_index.remove(dist)\n        self.local_index.add(dist)\n        self.install_egg_scripts(dist)\n        self.installed_projects[dist.key] = dist\n        log.info(self.installation_report(requirement, dist, *info))\n        if (dist.has_metadata('dependency_links.txt') and\n                not self.no_find_links):\n            self.package_index.add_find_links(\n                dist.get_metadata_lines('dependency_links.txt')\n            )\n        if not deps and not self.always_copy:\n            return\n        elif requirement is not None and dist.key != requirement.key:\n            log.warn(\"Skipping dependencies for %s\", dist)\n            return  # XXX this is not the distribution we were looking for\n        elif requirement is None or dist not in requirement:\n            # if we wound up with a different version, resolve what we've got\n            distreq = dist.as_requirement()\n            requirement = Requirement(str(distreq))\n        log.info(\"Processing dependencies for %s\", requirement)\n        try:\n            distros = WorkingSet([]).resolve(\n                [requirement], self.local_index, self.easy_install\n            )\n        except DistributionNotFound as e:\n            raise DistutilsError(str(e)) from e\n        except VersionConflict as e:\n            raise DistutilsError(e.report()) from e\n        if self.always_copy or self.always_copy_from:\n            # Force all the relevant distros to be copied or activated\n            for dist in distros:\n                if dist.key not in self.installed_projects:\n                    self.easy_install(dist.as_requirement())\n        log.info(\"Finished processing dependencies for %s\", requirement)\n\n    def should_unzip(self, dist):\n        if self.zip_ok is not None:\n            return not self.zip_ok\n        if dist.has_metadata('not-zip-safe'):\n            return True\n        if not dist.has_metadata('zip-safe'):\n            return True\n        return False\n\n    def maybe_move(self, spec, dist_filename, setup_base):\n        dst = os.path.join(self.build_directory, spec.key)\n        if os.path.exists(dst):\n            msg = (\n                \"%r already exists in %s; build directory %s will not be kept\"\n            )\n            log.warn(msg, spec.key, self.build_directory, setup_base)\n            return setup_base\n        if os.path.isdir(dist_filename):\n            setup_base = dist_filename\n        else:\n            if os.path.dirname(dist_filename) == setup_base:\n                os.unlink(dist_filename)  # get it out of the tmp dir\n            contents = os.listdir(setup_base)\n            if len(contents) == 1:\n                dist_filename = os.path.join(setup_base, contents[0])\n                if os.path.isdir(dist_filename):\n                    # if the only thing there is a directory, move it instead\n                    setup_base = dist_filename\n        ensure_directory(dst)\n        shutil.move(setup_base, dst)\n        return dst\n\n    def install_wrapper_scripts(self, dist):\n        if self.exclude_scripts:\n            return\n        for args in ScriptWriter.best().get_args(dist):\n            self.write_script(*args)\n\n    def install_script(self, dist, script_name, script_text, dev_path=None):\n        \"\"\"Generate a legacy script wrapper and install it\"\"\"\n        spec = str(dist.as_requirement())\n        is_script = is_python_script(script_text, script_name)\n\n        if is_script:\n            body = self._load_template(dev_path) % locals()\n            script_text = ScriptWriter.get_header(script_text) + body\n        self.write_script(script_name, _to_bytes(script_text), 'b')\n\n    @staticmethod\n    def _load_template(dev_path):\n        \"\"\"\n        There are a couple of template scripts in the package. This\n        function loads one of them and prepares it for use.\n        \"\"\"\n        # See https://github.com/pypa/setuptools/issues/134 for info\n        # on script file naming and downstream issues with SVR4\n        name = 'script.tmpl'\n        if dev_path:\n            name = name.replace('.tmpl', ' (dev).tmpl')\n\n        raw_bytes = resource_string('setuptools', name)\n        return raw_bytes.decode('utf-8')\n\n    def write_script(self, script_name, contents, mode=\"t\", blockers=()):\n        \"\"\"Write an executable file to the scripts directory\"\"\"\n        self.delete_blockers(  # clean up old .py/.pyw w/o a script\n            [os.path.join(self.script_dir, x) for x in blockers]\n        )\n        log.info(\"Installing %s script to %s\", script_name, self.script_dir)\n        target = os.path.join(self.script_dir, script_name)\n        self.add_output(target)\n\n        if self.dry_run:\n            return\n\n        mask = current_umask()\n        ensure_directory(target)\n        if os.path.exists(target):\n            os.unlink(target)\n        with open(target, \"w\" + mode) as f:\n            f.write(contents)\n        chmod(target, 0o777 - mask)\n\n    def install_eggs(self, spec, dist_filename, tmpdir):\n        # .egg dirs or files are already built, so just return them\n        if dist_filename.lower().endswith('.egg'):\n            return [self.install_egg(dist_filename, tmpdir)]\n        elif dist_filename.lower().endswith('.exe'):\n            return [self.install_exe(dist_filename, tmpdir)]\n        elif dist_filename.lower().endswith('.whl'):\n            return [self.install_wheel(dist_filename, tmpdir)]\n\n        # Anything else, try to extract and build\n        setup_base = tmpdir\n        if os.path.isfile(dist_filename) and not dist_filename.endswith('.py'):\n            unpack_archive(dist_filename, tmpdir, self.unpack_progress)\n        elif os.path.isdir(dist_filename):\n            setup_base = os.path.abspath(dist_filename)\n\n        if (setup_base.startswith(tmpdir)  # something we downloaded\n                and self.build_directory and spec is not None):\n            setup_base = self.maybe_move(spec, dist_filename, setup_base)\n\n        # Find the setup.py file\n        setup_script = os.path.join(setup_base, 'setup.py')\n\n        if not os.path.exists(setup_script):\n            setups = glob(os.path.join(setup_base, '*', 'setup.py'))\n            if not setups:\n                raise DistutilsError(\n                    \"Couldn't find a setup script in %s\" %\n                    os.path.abspath(dist_filename)\n                )\n            if len(setups) > 1:\n                raise DistutilsError(\n                    \"Multiple setup scripts in %s\" %\n                    os.path.abspath(dist_filename)\n                )\n            setup_script = setups[0]\n\n        # Now run it, and return the result\n        if self.editable:\n            log.info(self.report_editable(spec, setup_script))\n            return []\n        else:\n            return self.build_and_install(setup_script, setup_base)\n\n    def egg_distribution(self, egg_path):\n        if os.path.isdir(egg_path):\n            metadata = PathMetadata(egg_path, os.path.join(egg_path,\n                                                           'EGG-INFO'))\n        else:\n            metadata = EggMetadata(zipimport.zipimporter(egg_path))\n        return Distribution.from_filename(egg_path, metadata=metadata)\n\n    def install_egg(self, egg_path, tmpdir):\n        destination = os.path.join(\n            self.install_dir,\n            os.path.basename(egg_path),\n        )\n        destination = os.path.abspath(destination)\n        if not self.dry_run:\n            ensure_directory(destination)\n\n        dist = self.egg_distribution(egg_path)\n        if not samefile(egg_path, destination):\n            if os.path.isdir(destination) and not os.path.islink(destination):\n                dir_util.remove_tree(destination, dry_run=self.dry_run)\n            elif os.path.exists(destination):\n                self.execute(\n                    os.unlink,\n                    (destination,),\n                    \"Removing \" + destination,\n                )\n            try:\n                new_dist_is_zipped = False\n                if os.path.isdir(egg_path):\n                    if egg_path.startswith(tmpdir):\n                        f, m = shutil.move, \"Moving\"\n                    else:\n                        f, m = shutil.copytree, \"Copying\"\n                elif self.should_unzip(dist):\n                    self.mkpath(destination)\n                    f, m = self.unpack_and_compile, \"Extracting\"\n                else:\n                    new_dist_is_zipped = True\n                    if egg_path.startswith(tmpdir):\n                        f, m = shutil.move, \"Moving\"\n                    else:\n                        f, m = shutil.copy2, \"Copying\"\n                self.execute(\n                    f,\n                    (egg_path, destination),\n                    (m + \" %s to %s\") % (\n                        os.path.basename(egg_path),\n                        os.path.dirname(destination)\n                    ),\n                )\n                update_dist_caches(\n                    destination,\n                    fix_zipimporter_caches=new_dist_is_zipped,\n                )\n            except Exception:\n                update_dist_caches(destination, fix_zipimporter_caches=False)\n                raise\n\n        self.add_output(destination)\n        return self.egg_distribution(destination)\n\n    def install_exe(self, dist_filename, tmpdir):\n        # See if it's valid, get data\n        cfg = extract_wininst_cfg(dist_filename)\n        if cfg is None:\n            raise DistutilsError(\n                \"%s is not a valid distutils Windows .exe\" % dist_filename\n            )\n        # Create a dummy distribution object until we build the real distro\n        dist = Distribution(\n            None,\n            project_name=cfg.get('metadata', 'name'),\n            version=cfg.get('metadata', 'version'), platform=get_platform(),\n        )\n\n        # Convert the .exe to an unpacked egg\n        egg_path = os.path.join(tmpdir, dist.egg_name() + '.egg')\n        dist.location = egg_path\n        egg_tmp = egg_path + '.tmp'\n        _egg_info = os.path.join(egg_tmp, 'EGG-INFO')\n        pkg_inf = os.path.join(_egg_info, 'PKG-INFO')\n        ensure_directory(pkg_inf)  # make sure EGG-INFO dir exists\n        dist._provider = PathMetadata(egg_tmp, _egg_info)  # XXX\n        self.exe_to_egg(dist_filename, egg_tmp)\n\n        # Write EGG-INFO/PKG-INFO\n        if not os.path.exists(pkg_inf):\n            f = open(pkg_inf, 'w')\n            f.write('Metadata-Version: 1.0\\n')\n            for k, v in cfg.items('metadata'):\n                if k != 'target_version':\n                    f.write('%s: %s\\n' % (k.replace('_', '-').title(), v))\n            f.close()\n        script_dir = os.path.join(_egg_info, 'scripts')\n        # delete entry-point scripts to avoid duping\n        self.delete_blockers([\n            os.path.join(script_dir, args[0])\n            for args in ScriptWriter.get_args(dist)\n        ])\n        # Build .egg file from tmpdir\n        bdist_egg.make_zipfile(\n            egg_path, egg_tmp, verbose=self.verbose, dry_run=self.dry_run,\n        )\n        # install the .egg\n        return self.install_egg(egg_path, tmpdir)\n\n    def exe_to_egg(self, dist_filename, egg_tmp):\n        \"\"\"Extract a bdist_wininst to the directories an egg would use\"\"\"\n        # Check for .pth file and set up prefix translations\n        prefixes = get_exe_prefixes(dist_filename)\n        to_compile = []\n        native_libs = []\n        top_level = {}\n\n        def process(src, dst):\n            s = src.lower()\n            for old, new in prefixes:\n                if s.startswith(old):\n                    src = new + src[len(old):]\n                    parts = src.split('/')\n                    dst = os.path.join(egg_tmp, *parts)\n                    dl = dst.lower()\n                    if dl.endswith('.pyd') or dl.endswith('.dll'):\n                        parts[-1] = bdist_egg.strip_module(parts[-1])\n                        top_level[os.path.splitext(parts[0])[0]] = 1\n                        native_libs.append(src)\n                    elif dl.endswith('.py') and old != 'SCRIPTS/':\n                        top_level[os.path.splitext(parts[0])[0]] = 1\n                        to_compile.append(dst)\n                    return dst\n            if not src.endswith('.pth'):\n                log.warn(\"WARNING: can't process %s\", src)\n            return None\n\n        # extract, tracking .pyd/.dll->native_libs and .py -> to_compile\n        unpack_archive(dist_filename, egg_tmp, process)\n        stubs = []\n        for res in native_libs:\n            if res.lower().endswith('.pyd'):  # create stubs for .pyd's\n                parts = res.split('/')\n                resource = parts[-1]\n                parts[-1] = bdist_egg.strip_module(parts[-1]) + '.py'\n                pyfile = os.path.join(egg_tmp, *parts)\n                to_compile.append(pyfile)\n                stubs.append(pyfile)\n                bdist_egg.write_stub(resource, pyfile)\n        self.byte_compile(to_compile)  # compile .py's\n        bdist_egg.write_safety_flag(\n            os.path.join(egg_tmp, 'EGG-INFO'),\n            bdist_egg.analyze_egg(egg_tmp, stubs))  # write zip-safety flag\n\n        for name in 'top_level', 'native_libs':\n            if locals()[name]:\n                txt = os.path.join(egg_tmp, 'EGG-INFO', name + '.txt')\n                if not os.path.exists(txt):\n                    f = open(txt, 'w')\n                    f.write('\\n'.join(locals()[name]) + '\\n')\n                    f.close()\n\n    def install_wheel(self, wheel_path, tmpdir):\n        wheel = Wheel(wheel_path)\n        assert wheel.is_compatible()\n        destination = os.path.join(self.install_dir, wheel.egg_name())\n        destination = os.path.abspath(destination)\n        if not self.dry_run:\n            ensure_directory(destination)\n        if os.path.isdir(destination) and not os.path.islink(destination):\n            dir_util.remove_tree(destination, dry_run=self.dry_run)\n        elif os.path.exists(destination):\n            self.execute(\n                os.unlink,\n                (destination,),\n                \"Removing \" + destination,\n            )\n        try:\n            self.execute(\n                wheel.install_as_egg,\n                (destination,),\n                (\"Installing %s to %s\") % (\n                    os.path.basename(wheel_path),\n                    os.path.dirname(destination)\n                ),\n            )\n        finally:\n            update_dist_caches(destination, fix_zipimporter_caches=False)\n        self.add_output(destination)\n        return self.egg_distribution(destination)\n\n    __mv_warning = textwrap.dedent(\"\"\"\n        Because this distribution was installed --multi-version, before you can\n        import modules from this package in an application, you will need to\n        'import pkg_resources' and then use a 'require()' call similar to one of\n        these examples, in order to select the desired version:\n\n            pkg_resources.require(\"%(name)s\")  # latest installed version\n            pkg_resources.require(\"%(name)s==%(version)s\")  # this exact version\n            pkg_resources.require(\"%(name)s>=%(version)s\")  # this version or higher\n        \"\"\").lstrip()  # noqa\n\n    __id_warning = textwrap.dedent(\"\"\"\n        Note also that the installation directory must be on sys.path at runtime for\n        this to work.  (e.g. by being the application's script directory, by being on\n        PYTHONPATH, or by being added to sys.path by your code.)\n        \"\"\")  # noqa\n\n    def installation_report(self, req, dist, what=\"Installed\"):\n        \"\"\"Helpful installation message for display to package users\"\"\"\n        msg = \"\\n%(what)s %(eggloc)s%(extras)s\"\n        if self.multi_version and not self.no_report:\n            msg += '\\n' + self.__mv_warning\n            if self.install_dir not in map(normalize_path, sys.path):\n                msg += '\\n' + self.__id_warning\n\n        eggloc = dist.location\n        name = dist.project_name\n        version = dist.version\n        extras = ''  # TODO: self.report_extras(req, dist)\n        return msg % locals()\n\n    __editable_msg = textwrap.dedent(\"\"\"\n        Extracted editable version of %(spec)s to %(dirname)s\n\n        If it uses setuptools in its setup script, you can activate it in\n        \"development\" mode by going to that directory and running::\n\n            %(python)s setup.py develop\n\n        See the setuptools documentation for the \"develop\" command for more info.\n        \"\"\").lstrip()  # noqa\n\n    def report_editable(self, spec, setup_script):\n        dirname = os.path.dirname(setup_script)\n        python = sys.executable\n        return '\\n' + self.__editable_msg % locals()\n\n    def run_setup(self, setup_script, setup_base, args):\n        sys.modules.setdefault('distutils.command.bdist_egg', bdist_egg)\n        sys.modules.setdefault('distutils.command.egg_info', egg_info)\n\n        args = list(args)\n        if self.verbose > 2:\n            v = 'v' * (self.verbose - 1)\n            args.insert(0, '-' + v)\n        elif self.verbose < 2:\n            args.insert(0, '-q')\n        if self.dry_run:\n            args.insert(0, '-n')\n        log.info(\n            \"Running %s %s\", setup_script[len(setup_base) + 1:], ' '.join(args)\n        )\n        try:\n            run_setup(setup_script, args)\n        except SystemExit as v:\n            raise DistutilsError(\n                \"Setup script exited with %s\" % (v.args[0],)\n            ) from v\n\n    def build_and_install(self, setup_script, setup_base):\n        args = ['bdist_egg', '--dist-dir']\n\n        dist_dir = tempfile.mkdtemp(\n            prefix='egg-dist-tmp-', dir=os.path.dirname(setup_script)\n        )\n        try:\n            self._set_fetcher_options(os.path.dirname(setup_script))\n            args.append(dist_dir)\n\n            self.run_setup(setup_script, setup_base, args)\n            all_eggs = Environment([dist_dir])\n            eggs = []\n            for key in all_eggs:\n                for dist in all_eggs[key]:\n                    eggs.append(self.install_egg(dist.location, setup_base))\n            if not eggs and not self.dry_run:\n                log.warn(\"No eggs found in %s (setup script problem?)\",\n                         dist_dir)\n            return eggs\n        finally:\n            rmtree(dist_dir)\n            log.set_verbosity(self.verbose)  # restore our log verbosity\n\n    def _set_fetcher_options(self, base):\n        \"\"\"\n        When easy_install is about to run bdist_egg on a source dist, that\n        source dist might have 'setup_requires' directives, requiring\n        additional fetching. Ensure the fetcher options given to easy_install\n        are available to that command as well.\n        \"\"\"\n        # find the fetch options from easy_install and write them out\n        # to the setup.cfg file.\n        ei_opts = self.distribution.get_option_dict('easy_install').copy()\n        fetch_directives = (\n            'find_links', 'site_dirs', 'index_url', 'optimize', 'allow_hosts',\n        )\n        fetch_options = {}\n        for key, val in ei_opts.items():\n            if key not in fetch_directives:\n                continue\n            fetch_options[key.replace('_', '-')] = val[1]\n        # create a settings dictionary suitable for `edit_config`\n        settings = dict(easy_install=fetch_options)\n        cfg_filename = os.path.join(base, 'setup.cfg')\n        setopt.edit_config(cfg_filename, settings)\n\n    def update_pth(self, dist):\n        if self.pth_file is None:\n            return\n\n        for d in self.pth_file[dist.key]:  # drop old entries\n            if self.multi_version or d.location != dist.location:\n                log.info(\"Removing %s from easy-install.pth file\", d)\n                self.pth_file.remove(d)\n                if d.location in self.shadow_path:\n                    self.shadow_path.remove(d.location)\n\n        if not self.multi_version:\n            if dist.location in self.pth_file.paths:\n                log.info(\n                    \"%s is already the active version in easy-install.pth\",\n                    dist,\n                )\n            else:\n                log.info(\"Adding %s to easy-install.pth file\", dist)\n                self.pth_file.add(dist)  # add new entry\n                if dist.location not in self.shadow_path:\n                    self.shadow_path.append(dist.location)\n\n        if not self.dry_run:\n\n            self.pth_file.save()\n\n            if dist.key == 'setuptools':\n                # Ensure that setuptools itself never becomes unavailable!\n                # XXX should this check for latest version?\n                filename = os.path.join(self.install_dir, 'setuptools.pth')\n                if os.path.islink(filename):\n                    os.unlink(filename)\n                f = open(filename, 'wt')\n                f.write(self.pth_file.make_relative(dist.location) + '\\n')\n                f.close()\n\n    def unpack_progress(self, src, dst):\n        # Progress filter for unpacking\n        log.debug(\"Unpacking %s to %s\", src, dst)\n        return dst  # only unpack-and-compile skips files for dry run\n\n    def unpack_and_compile(self, egg_path, destination):\n        to_compile = []\n        to_chmod = []\n\n        def pf(src, dst):\n            if dst.endswith('.py') and not src.startswith('EGG-INFO/'):\n                to_compile.append(dst)\n            elif dst.endswith('.dll') or dst.endswith('.so'):\n                to_chmod.append(dst)\n            self.unpack_progress(src, dst)\n            return not self.dry_run and dst or None\n\n        unpack_archive(egg_path, destination, pf)\n        self.byte_compile(to_compile)\n        if not self.dry_run:\n            for f in to_chmod:\n                mode = ((os.stat(f)[stat.ST_MODE]) | 0o555) & 0o7755\n                chmod(f, mode)\n\n    def byte_compile(self, to_compile):\n        if sys.dont_write_bytecode:\n            return\n\n        from distutils.util import byte_compile\n\n        try:\n            # try to make the byte compile messages quieter\n            log.set_verbosity(self.verbose - 1)\n\n            byte_compile(to_compile, optimize=0, force=1, dry_run=self.dry_run)\n            if self.optimize:\n                byte_compile(\n                    to_compile, optimize=self.optimize, force=1,\n                    dry_run=self.dry_run,\n                )\n        finally:\n            log.set_verbosity(self.verbose)  # restore original verbosity\n\n    __no_default_msg = textwrap.dedent(\"\"\"\n        bad install directory or PYTHONPATH\n\n        You are attempting to install a package to a directory that is not\n        on PYTHONPATH and which Python does not read \".pth\" files from.  The\n        installation directory you specified (via --install-dir, --prefix, or\n        the distutils default setting) was:\n\n            %s\n\n        and your PYTHONPATH environment variable currently contains:\n\n            %r\n\n        Here are some of your options for correcting the problem:\n\n        * You can choose a different installation directory, i.e., one that is\n          on PYTHONPATH or supports .pth files\n\n        * You can add the installation directory to the PYTHONPATH environment\n          variable.  (It must then also be on PYTHONPATH whenever you run\n          Python and want to use the package(s) you are installing.)\n\n        * You can set up the installation directory to support \".pth\" files by\n          using one of the approaches described here:\n\n          https://setuptools.readthedocs.io/en/latest/easy_install.html#custom-installation-locations\n\n\n        Please make the appropriate changes for your system and try again.\n        \"\"\").strip()\n\n    def create_home_path(self):\n        \"\"\"Create directories under ~.\"\"\"\n        if not self.user:\n            return\n        home = convert_path(os.path.expanduser(\"~\"))\n        for name, path in six.iteritems(self.config_vars):\n            if path.startswith(home) and not os.path.isdir(path):\n                self.debug_print(\"os.makedirs('%s', 0o700)\" % path)\n                os.makedirs(path, 0o700)\n\n    INSTALL_SCHEMES = dict(\n        posix=dict(\n            install_dir='$base/lib/python$py_version_short/site-packages',\n            script_dir='$base/bin',\n        ),\n    )\n\n    DEFAULT_SCHEME = dict(\n        install_dir='$base/Lib/site-packages',\n        script_dir='$base/Scripts',\n    )\n\n    def _expand(self, *attrs):\n        config_vars = self.get_finalized_command('install').config_vars\n\n        if self.prefix:\n            # Set default install_dir/scripts from --prefix\n            config_vars = config_vars.copy()\n            config_vars['base'] = self.prefix\n            scheme = self.INSTALL_SCHEMES.get(os.name, self.DEFAULT_SCHEME)\n            for attr, val in scheme.items():\n                if getattr(self, attr, None) is None:\n                    setattr(self, attr, val)\n\n        from distutils.util import subst_vars\n\n        for attr in attrs:\n            val = getattr(self, attr)\n            if val is not None:\n                val = subst_vars(val, config_vars)\n                if os.name == 'posix':\n                    val = os.path.expanduser(val)\n                setattr(self, attr, val)\n\n\ndef _pythonpath():\n    items = os.environ.get('PYTHONPATH', '').split(os.pathsep)\n    return filter(None, items)\n\n\ndef get_site_dirs():\n    \"\"\"\n    Return a list of 'site' dirs\n    \"\"\"\n\n    sitedirs = []\n\n    # start with PYTHONPATH\n    sitedirs.extend(_pythonpath())\n\n    prefixes = [sys.prefix]\n    if sys.exec_prefix != sys.prefix:\n        prefixes.append(sys.exec_prefix)\n    for prefix in prefixes:\n        if prefix:\n            if sys.platform in ('os2emx', 'riscos'):\n                sitedirs.append(os.path.join(prefix, \"Lib\", \"site-packages\"))\n            elif os.sep == '/':\n                sitedirs.extend([\n                    os.path.join(\n                        prefix,\n                        \"lib\",\n                        \"python{}.{}\".format(*sys.version_info),\n                        \"site-packages\",\n                    ),\n                    os.path.join(prefix, \"lib\", \"site-python\"),\n                ])\n            else:\n                sitedirs.extend([\n                    prefix,\n                    os.path.join(prefix, \"lib\", \"site-packages\"),\n                ])\n            if sys.platform == 'darwin':\n                # for framework builds *only* we add the standard Apple\n                # locations. Currently only per-user, but /Library and\n                # /Network/Library could be added too\n                if 'Python.framework' in prefix:\n                    home = os.environ.get('HOME')\n                    if home:\n                        home_sp = os.path.join(\n                            home,\n                            'Library',\n                            'Python',\n                            '{}.{}'.format(*sys.version_info),\n                            'site-packages',\n                        )\n                        sitedirs.append(home_sp)\n    lib_paths = get_path('purelib'), get_path('platlib')\n    for site_lib in lib_paths:\n        if site_lib not in sitedirs:\n            sitedirs.append(site_lib)\n\n    if site.ENABLE_USER_SITE:\n        sitedirs.append(site.USER_SITE)\n\n    try:\n        sitedirs.extend(site.getsitepackages())\n    except AttributeError:\n        pass\n\n    sitedirs = list(map(normalize_path, sitedirs))\n\n    return sitedirs\n\n\ndef expand_paths(inputs):\n    \"\"\"Yield sys.path directories that might contain \"old-style\" packages\"\"\"\n\n    seen = {}\n\n    for dirname in inputs:\n        dirname = normalize_path(dirname)\n        if dirname in seen:\n            continue\n\n        seen[dirname] = 1\n        if not os.path.isdir(dirname):\n            continue\n\n        files = os.listdir(dirname)\n        yield dirname, files\n\n        for name in files:\n            if not name.endswith('.pth'):\n                # We only care about the .pth files\n                continue\n            if name in ('easy-install.pth', 'setuptools.pth'):\n                # Ignore .pth files that we control\n                continue\n\n            # Read the .pth file\n            f = open(os.path.join(dirname, name))\n            lines = list(yield_lines(f))\n            f.close()\n\n            # Yield existing non-dupe, non-import directory lines from it\n            for line in lines:\n                if not line.startswith(\"import\"):\n                    line = normalize_path(line.rstrip())\n                    if line not in seen:\n                        seen[line] = 1\n                        if not os.path.isdir(line):\n                            continue\n                        yield line, os.listdir(line)\n\n\ndef extract_wininst_cfg(dist_filename):\n    \"\"\"Extract configuration data from a bdist_wininst .exe\n\n    Returns a configparser.RawConfigParser, or None\n    \"\"\"\n    f = open(dist_filename, 'rb')\n    try:\n        endrec = zipfile._EndRecData(f)\n        if endrec is None:\n            return None\n\n        prepended = (endrec[9] - endrec[5]) - endrec[6]\n        if prepended < 12:  # no wininst data here\n            return None\n        f.seek(prepended - 12)\n\n        tag, cfglen, bmlen = struct.unpack(\"<iii\", f.read(12))\n        if tag not in (0x1234567A, 0x1234567B):\n            return None  # not a valid tag\n\n        f.seek(prepended - (12 + cfglen))\n        init = {'version': '', 'target_version': ''}\n        cfg = configparser.RawConfigParser(init)\n        try:\n            part = f.read(cfglen)\n            # Read up to the first null byte.\n            config = part.split(b'\\0', 1)[0]\n            # Now the config is in bytes, but for RawConfigParser, it should\n            #  be text, so decode it.\n            config = config.decode(sys.getfilesystemencoding())\n            cfg.readfp(six.StringIO(config))\n        except configparser.Error:\n            return None\n        if not cfg.has_section('metadata') or not cfg.has_section('Setup'):\n            return None\n        return cfg\n\n    finally:\n        f.close()\n\n\ndef get_exe_prefixes(exe_filename):\n    \"\"\"Get exe->egg path translations for a given .exe file\"\"\"\n\n    prefixes = [\n        ('PURELIB/', ''),\n        ('PLATLIB/pywin32_system32', ''),\n        ('PLATLIB/', ''),\n        ('SCRIPTS/', 'EGG-INFO/scripts/'),\n        ('DATA/lib/site-packages', ''),\n    ]\n    z = zipfile.ZipFile(exe_filename)\n    try:\n        for info in z.infolist():\n            name = info.filename\n            parts = name.split('/')\n            if len(parts) == 3 and parts[2] == 'PKG-INFO':\n                if parts[1].endswith('.egg-info'):\n                    prefixes.insert(0, ('/'.join(parts[:2]), 'EGG-INFO/'))\n                    break\n            if len(parts) != 2 or not name.endswith('.pth'):\n                continue\n            if name.endswith('-nspkg.pth'):\n                continue\n            if parts[0].upper() in ('PURELIB', 'PLATLIB'):\n                contents = z.read(name)\n                if not six.PY2:\n                    contents = contents.decode()\n                for pth in yield_lines(contents):\n                    pth = pth.strip().replace('\\\\', '/')\n                    if not pth.startswith('import'):\n                        prefixes.append((('%s/%s/' % (parts[0], pth)), ''))\n    finally:\n        z.close()\n    prefixes = [(x.lower(), y) for x, y in prefixes]\n    prefixes.sort()\n    prefixes.reverse()\n    return prefixes\n\n\nclass PthDistributions(Environment):\n    \"\"\"A .pth file with Distribution paths in it\"\"\"\n\n    dirty = False\n\n    def __init__(self, filename, sitedirs=()):\n        self.filename = filename\n        self.sitedirs = list(map(normalize_path, sitedirs))\n        self.basedir = normalize_path(os.path.dirname(self.filename))\n        self._load()\n        Environment.__init__(self, [], None, None)\n        for path in yield_lines(self.paths):\n            list(map(self.add, find_distributions(path, True)))\n\n    def _load(self):\n        self.paths = []\n        saw_import = False\n        seen = dict.fromkeys(self.sitedirs)\n        if os.path.isfile(self.filename):\n            f = open(self.filename, 'rt')\n            for line in f:\n                if line.startswith('import'):\n                    saw_import = True\n                    continue\n                path = line.rstrip()\n                self.paths.append(path)\n                if not path.strip() or path.strip().startswith('#'):\n                    continue\n                # skip non-existent paths, in case somebody deleted a package\n                # manually, and duplicate paths as well\n                path = self.paths[-1] = normalize_path(\n                    os.path.join(self.basedir, path)\n                )\n                if not os.path.exists(path) or path in seen:\n                    self.paths.pop()  # skip it\n                    self.dirty = True  # we cleaned up, so we're dirty now :)\n                    continue\n                seen[path] = 1\n            f.close()\n\n        if self.paths and not saw_import:\n            self.dirty = True  # ensure anything we touch has import wrappers\n        while self.paths and not self.paths[-1].strip():\n            self.paths.pop()\n\n    def save(self):\n        \"\"\"Write changed .pth file back to disk\"\"\"\n        if not self.dirty:\n            return\n\n        rel_paths = list(map(self.make_relative, self.paths))\n        if rel_paths:\n            log.debug(\"Saving %s\", self.filename)\n            lines = self._wrap_lines(rel_paths)\n            data = '\\n'.join(lines) + '\\n'\n\n            if os.path.islink(self.filename):\n                os.unlink(self.filename)\n            with open(self.filename, 'wt') as f:\n                f.write(data)\n\n        elif os.path.exists(self.filename):\n            log.debug(\"Deleting empty %s\", self.filename)\n            os.unlink(self.filename)\n\n        self.dirty = False\n\n    @staticmethod\n    def _wrap_lines(lines):\n        return lines\n\n    def add(self, dist):\n        \"\"\"Add `dist` to the distribution map\"\"\"\n        new_path = (\n            dist.location not in self.paths and (\n                dist.location not in self.sitedirs or\n                # account for '.' being in PYTHONPATH\n                dist.location == os.getcwd()\n            )\n        )\n        if new_path:\n            self.paths.append(dist.location)\n            self.dirty = True\n        Environment.add(self, dist)\n\n    def remove(self, dist):\n        \"\"\"Remove `dist` from the distribution map\"\"\"\n        while dist.location in self.paths:\n            self.paths.remove(dist.location)\n            self.dirty = True\n        Environment.remove(self, dist)\n\n    def make_relative(self, path):\n        npath, last = os.path.split(normalize_path(path))\n        baselen = len(self.basedir)\n        parts = [last]\n        sep = os.altsep == '/' and '/' or os.sep\n        while len(npath) >= baselen:\n            if npath == self.basedir:\n                parts.append(os.curdir)\n                parts.reverse()\n                return sep.join(parts)\n            npath, last = os.path.split(npath)\n            parts.append(last)\n        else:\n            return path\n\n\nclass RewritePthDistributions(PthDistributions):\n    @classmethod\n    def _wrap_lines(cls, lines):\n        yield cls.prelude\n        for line in lines:\n            yield line\n        yield cls.postlude\n\n    prelude = _one_liner(\"\"\"\n        import sys\n        sys.__plen = len(sys.path)\n        \"\"\")\n    postlude = _one_liner(\"\"\"\n        import sys\n        new = sys.path[sys.__plen:]\n        del sys.path[sys.__plen:]\n        p = getattr(sys, '__egginsert', 0)\n        sys.path[p:p] = new\n        sys.__egginsert = p + len(new)\n        \"\"\")\n\n\nif os.environ.get('SETUPTOOLS_SYS_PATH_TECHNIQUE', 'raw') == 'rewrite':\n    PthDistributions = RewritePthDistributions\n\n\ndef _first_line_re():\n    \"\"\"\n    Return a regular expression based on first_line_re suitable for matching\n    strings.\n    \"\"\"\n    if isinstance(first_line_re.pattern, str):\n        return first_line_re\n\n    # first_line_re in Python >=3.1.4 and >=3.2.1 is a bytes pattern.\n    return re.compile(first_line_re.pattern.decode())\n\n\ndef auto_chmod(func, arg, exc):\n    if func in [os.unlink, os.remove] and os.name == 'nt':\n        chmod(arg, stat.S_IWRITE)\n        return func(arg)\n    et, ev, _ = sys.exc_info()\n    six.reraise(et, (ev[0], ev[1] + (\" %s %s\" % (func, arg))))\n\n\ndef update_dist_caches(dist_path, fix_zipimporter_caches):\n    \"\"\"\n    Fix any globally cached `dist_path` related data\n\n    `dist_path` should be a path of a newly installed egg distribution (zipped\n    or unzipped).\n\n    sys.path_importer_cache contains finder objects that have been cached when\n    importing data from the original distribution. Any such finders need to be\n    cleared since the replacement distribution might be packaged differently,\n    e.g. a zipped egg distribution might get replaced with an unzipped egg\n    folder or vice versa. Having the old finders cached may then cause Python\n    to attempt loading modules from the replacement distribution using an\n    incorrect loader.\n\n    zipimport.zipimporter objects are Python loaders charged with importing\n    data packaged inside zip archives. If stale loaders referencing the\n    original distribution, are left behind, they can fail to load modules from\n    the replacement distribution. E.g. if an old zipimport.zipimporter instance\n    is used to load data from a new zipped egg archive, it may cause the\n    operation to attempt to locate the requested data in the wrong location -\n    one indicated by the original distribution's zip archive directory\n    information. Such an operation may then fail outright, e.g. report having\n    read a 'bad local file header', or even worse, it may fail silently &\n    return invalid data.\n\n    zipimport._zip_directory_cache contains cached zip archive directory\n    information for all existing zipimport.zipimporter instances and all such\n    instances connected to the same archive share the same cached directory\n    information.\n\n    If asked, and the underlying Python implementation allows it, we can fix\n    all existing zipimport.zipimporter instances instead of having to track\n    them down and remove them one by one, by updating their shared cached zip\n    archive directory information. This, of course, assumes that the\n    replacement distribution is packaged as a zipped egg.\n\n    If not asked to fix existing zipimport.zipimporter instances, we still do\n    our best to clear any remaining zipimport.zipimporter related cached data\n    that might somehow later get used when attempting to load data from the new\n    distribution and thus cause such load operations to fail. Note that when\n    tracking down such remaining stale data, we can not catch every conceivable\n    usage from here, and we clear only those that we know of and have found to\n    cause problems if left alive. Any remaining caches should be updated by\n    whomever is in charge of maintaining them, i.e. they should be ready to\n    handle us replacing their zip archives with new distributions at runtime.\n\n    \"\"\"\n    # There are several other known sources of stale zipimport.zipimporter\n    # instances that we do not clear here, but might if ever given a reason to\n    # do so:\n    # * Global setuptools pkg_resources.working_set (a.k.a. 'master working\n    # set') may contain distributions which may in turn contain their\n    #   zipimport.zipimporter loaders.\n    # * Several zipimport.zipimporter loaders held by local variables further\n    #   up the function call stack when running the setuptools installation.\n    # * Already loaded modules may have their __loader__ attribute set to the\n    #   exact loader instance used when importing them. Python 3.4 docs state\n    #   that this information is intended mostly for introspection and so is\n    #   not expected to cause us problems.\n    normalized_path = normalize_path(dist_path)\n    _uncache(normalized_path, sys.path_importer_cache)\n    if fix_zipimporter_caches:\n        _replace_zip_directory_cache_data(normalized_path)\n    else:\n        # Here, even though we do not want to fix existing and now stale\n        # zipimporter cache information, we still want to remove it. Related to\n        # Python's zip archive directory information cache, we clear each of\n        # its stale entries in two phases:\n        #   1. Clear the entry so attempting to access zip archive information\n        #      via any existing stale zipimport.zipimporter instances fails.\n        #   2. Remove the entry from the cache so any newly constructed\n        #      zipimport.zipimporter instances do not end up using old stale\n        #      zip archive directory information.\n        # This whole stale data removal step does not seem strictly necessary,\n        # but has been left in because it was done before we started replacing\n        # the zip archive directory information cache content if possible, and\n        # there are no relevant unit tests that we can depend on to tell us if\n        # this is really needed.\n        _remove_and_clear_zip_directory_cache_data(normalized_path)\n\n\ndef _collect_zipimporter_cache_entries(normalized_path, cache):\n    \"\"\"\n    Return zipimporter cache entry keys related to a given normalized path.\n\n    Alternative path spellings (e.g. those using different character case or\n    those using alternative path separators) related to the same path are\n    included. Any sub-path entries are included as well, i.e. those\n    corresponding to zip archives embedded in other zip archives.\n\n    \"\"\"\n    result = []\n    prefix_len = len(normalized_path)\n    for p in cache:\n        np = normalize_path(p)\n        if (np.startswith(normalized_path) and\n                np[prefix_len:prefix_len + 1] in (os.sep, '')):\n            result.append(p)\n    return result\n\n\ndef _update_zipimporter_cache(normalized_path, cache, updater=None):\n    \"\"\"\n    Update zipimporter cache data for a given normalized path.\n\n    Any sub-path entries are processed as well, i.e. those corresponding to zip\n    archives embedded in other zip archives.\n\n    Given updater is a callable taking a cache entry key and the original entry\n    (after already removing the entry from the cache), and expected to update\n    the entry and possibly return a new one to be inserted in its place.\n    Returning None indicates that the entry should not be replaced with a new\n    one. If no updater is given, the cache entries are simply removed without\n    any additional processing, the same as if the updater simply returned None.\n\n    \"\"\"\n    for p in _collect_zipimporter_cache_entries(normalized_path, cache):\n        # N.B. pypy's custom zipimport._zip_directory_cache implementation does\n        # not support the complete dict interface:\n        # * Does not support item assignment, thus not allowing this function\n        #    to be used only for removing existing cache entries.\n        #  * Does not support the dict.pop() method, forcing us to use the\n        #    get/del patterns instead. For more detailed information see the\n        #    following links:\n        #      https://github.com/pypa/setuptools/issues/202#issuecomment-202913420\n        #      http://bit.ly/2h9itJX\n        old_entry = cache[p]\n        del cache[p]\n        new_entry = updater and updater(p, old_entry)\n        if new_entry is not None:\n            cache[p] = new_entry\n\n\ndef _uncache(normalized_path, cache):\n    _update_zipimporter_cache(normalized_path, cache)\n\n\ndef _remove_and_clear_zip_directory_cache_data(normalized_path):\n    def clear_and_remove_cached_zip_archive_directory_data(path, old_entry):\n        old_entry.clear()\n\n    _update_zipimporter_cache(\n        normalized_path, zipimport._zip_directory_cache,\n        updater=clear_and_remove_cached_zip_archive_directory_data)\n\n\n# PyPy Python implementation does not allow directly writing to the\n# zipimport._zip_directory_cache and so prevents us from attempting to correct\n# its content. The best we can do there is clear the problematic cache content\n# and have PyPy repopulate it as needed. The downside is that if there are any\n# stale zipimport.zipimporter instances laying around, attempting to use them\n# will fail due to not having its zip archive directory information available\n# instead of being automatically corrected to use the new correct zip archive\n# directory information.\nif '__pypy__' in sys.builtin_module_names:\n    _replace_zip_directory_cache_data = \\\n        _remove_and_clear_zip_directory_cache_data\nelse:\n\n    def _replace_zip_directory_cache_data(normalized_path):\n        def replace_cached_zip_archive_directory_data(path, old_entry):\n            # N.B. In theory, we could load the zip directory information just\n            # once for all updated path spellings, and then copy it locally and\n            # update its contained path strings to contain the correct\n            # spelling, but that seems like a way too invasive move (this cache\n            # structure is not officially documented anywhere and could in\n            # theory change with new Python releases) for no significant\n            # benefit.\n            old_entry.clear()\n            zipimport.zipimporter(path)\n            old_entry.update(zipimport._zip_directory_cache[path])\n            return old_entry\n\n        _update_zipimporter_cache(\n            normalized_path, zipimport._zip_directory_cache,\n            updater=replace_cached_zip_archive_directory_data)\n\n\ndef is_python(text, filename='<string>'):\n    \"Is this string a valid Python script?\"\n    try:\n        compile(text, filename, 'exec')\n    except (SyntaxError, TypeError):\n        return False\n    else:\n        return True\n\n\ndef is_sh(executable):\n    \"\"\"Determine if the specified executable is a .sh (contains a #! line)\"\"\"\n    try:\n        with io.open(executable, encoding='latin-1') as fp:\n            magic = fp.read(2)\n    except (OSError, IOError):\n        return executable\n    return magic == '#!'\n\n\ndef nt_quote_arg(arg):\n    \"\"\"Quote a command line argument according to Windows parsing rules\"\"\"\n    return subprocess.list2cmdline([arg])\n\n\ndef is_python_script(script_text, filename):\n    \"\"\"Is this text, as a whole, a Python script? (as opposed to shell/bat/etc.\n    \"\"\"\n    if filename.endswith('.py') or filename.endswith('.pyw'):\n        return True  # extension says it's Python\n    if is_python(script_text, filename):\n        return True  # it's syntactically valid Python\n    if script_text.startswith('#!'):\n        # It begins with a '#!' line, so check if 'python' is in it somewhere\n        return 'python' in script_text.splitlines()[0].lower()\n\n    return False  # Not any Python I can recognize\n\n\ntry:\n    from os import chmod as _chmod\nexcept ImportError:\n    # Jython compatibility\n    def _chmod(*args):\n        pass\n\n\ndef chmod(path, mode):\n    log.debug(\"changing mode of %s to %o\", path, mode)\n    try:\n        _chmod(path, mode)\n    except os.error as e:\n        log.debug(\"chmod failed: %s\", e)\n\n\nclass CommandSpec(list):\n    \"\"\"\n    A command spec for a #! header, specified as a list of arguments akin to\n    those passed to Popen.\n    \"\"\"\n\n    options = []\n    split_args = dict()\n\n    @classmethod\n    def best(cls):\n        \"\"\"\n        Choose the best CommandSpec class based on environmental conditions.\n        \"\"\"\n        return cls\n\n    @classmethod\n    def _sys_executable(cls):\n        _default = os.path.normpath(sys.executable)\n        return os.environ.get('__PYVENV_LAUNCHER__', _default)\n\n    @classmethod\n    def from_param(cls, param):\n        \"\"\"\n        Construct a CommandSpec from a parameter to build_scripts, which may\n        be None.\n        \"\"\"\n        if isinstance(param, cls):\n            return param\n        if isinstance(param, list):\n            return cls(param)\n        if param is None:\n            return cls.from_environment()\n        # otherwise, assume it's a string.\n        return cls.from_string(param)\n\n    @classmethod\n    def from_environment(cls):\n        return cls([cls._sys_executable()])\n\n    @classmethod\n    def from_string(cls, string):\n        \"\"\"\n        Construct a command spec from a simple string representing a command\n        line parseable by shlex.split.\n        \"\"\"\n        items = shlex.split(string, **cls.split_args)\n        return cls(items)\n\n    def install_options(self, script_text):\n        self.options = shlex.split(self._extract_options(script_text))\n        cmdline = subprocess.list2cmdline(self)\n        if not isascii(cmdline):\n            self.options[:0] = ['-x']\n\n    @staticmethod\n    def _extract_options(orig_script):\n        \"\"\"\n        Extract any options from the first line of the script.\n        \"\"\"\n        first = (orig_script + '\\n').splitlines()[0]\n        match = _first_line_re().match(first)\n        options = match.group(1) or '' if match else ''\n        return options.strip()\n\n    def as_header(self):\n        return self._render(self + list(self.options))\n\n    @staticmethod\n    def _strip_quotes(item):\n        _QUOTES = '\"\\''\n        for q in _QUOTES:\n            if item.startswith(q) and item.endswith(q):\n                return item[1:-1]\n        return item\n\n    @staticmethod\n    def _render(items):\n        cmdline = subprocess.list2cmdline(\n            CommandSpec._strip_quotes(item.strip()) for item in items)\n        return '#!' + cmdline + '\\n'\n\n\n# For pbr compat; will be removed in a future version.\nsys_executable = CommandSpec._sys_executable()\n\n\nclass WindowsCommandSpec(CommandSpec):\n    split_args = dict(posix=False)\n\n\nclass ScriptWriter:\n    \"\"\"\n    Encapsulates behavior around writing entry point scripts for console and\n    gui apps.\n    \"\"\"\n\n    template = textwrap.dedent(r\"\"\"\n        # EASY-INSTALL-ENTRY-SCRIPT: %(spec)r,%(group)r,%(name)r\n        import re\n        import sys\n\n        # for compatibility with easy_install; see #2198\n        __requires__ = %(spec)r\n\n        try:\n            from importlib.metadata import distribution\n        except ImportError:\n            try:\n                from importlib_metadata import distribution\n            except ImportError:\n                from pkg_resources import load_entry_point\n\n\n        def importlib_load_entry_point(spec, group, name):\n            dist_name, _, _ = spec.partition('==')\n            matches = (\n                entry_point\n                for entry_point in distribution(dist_name).entry_points\n                if entry_point.group == group and entry_point.name == name\n            )\n            return next(matches).load()\n\n\n        globals().setdefault('load_entry_point', importlib_load_entry_point)\n\n\n        if __name__ == '__main__':\n            sys.argv[0] = re.sub(r'(-script\\.pyw?|\\.exe)?$', '', sys.argv[0])\n            sys.exit(load_entry_point(%(spec)r, %(group)r, %(name)r)())\n        \"\"\").lstrip()\n\n    command_spec_class = CommandSpec\n\n    @classmethod\n    def get_script_args(cls, dist, executable=None, wininst=False):\n        # for backward compatibility\n        warnings.warn(\"Use get_args\", EasyInstallDeprecationWarning)\n        writer = (WindowsScriptWriter if wininst else ScriptWriter).best()\n        header = cls.get_script_header(\"\", executable, wininst)\n        return writer.get_args(dist, header)\n\n    @classmethod\n    def get_script_header(cls, script_text, executable=None, wininst=False):\n        # for backward compatibility\n        warnings.warn(\n            \"Use get_header\", EasyInstallDeprecationWarning, stacklevel=2)\n        if wininst:\n            executable = \"python.exe\"\n        return cls.get_header(script_text, executable)\n\n    @classmethod\n    def get_args(cls, dist, header=None):\n        \"\"\"\n        Yield write_script() argument tuples for a distribution's\n        console_scripts and gui_scripts entry points.\n        \"\"\"\n        if header is None:\n            header = cls.get_header()\n        spec = str(dist.as_requirement())\n        for type_ in 'console', 'gui':\n            group = type_ + '_scripts'\n            for name, ep in dist.get_entry_map(group).items():\n                cls._ensure_safe_name(name)\n                script_text = cls.template % locals()\n                args = cls._get_script_args(type_, name, header, script_text)\n                for res in args:\n                    yield res\n\n    @staticmethod\n    def _ensure_safe_name(name):\n        \"\"\"\n        Prevent paths in *_scripts entry point names.\n        \"\"\"\n        has_path_sep = re.search(r'[\\\\/]', name)\n        if has_path_sep:\n            raise ValueError(\"Path separators not allowed in script names\")\n\n    @classmethod\n    def get_writer(cls, force_windows):\n        # for backward compatibility\n        warnings.warn(\"Use best\", EasyInstallDeprecationWarning)\n        return WindowsScriptWriter.best() if force_windows else cls.best()\n\n    @classmethod\n    def best(cls):\n        \"\"\"\n        Select the best ScriptWriter for this environment.\n        \"\"\"\n        if sys.platform == 'win32' or (os.name == 'java' and os._name == 'nt'):\n            return WindowsScriptWriter.best()\n        else:\n            return cls\n\n    @classmethod\n    def _get_script_args(cls, type_, name, header, script_text):\n        # Simply write the stub with no extension.\n        yield (name, header + script_text)\n\n    @classmethod\n    def get_header(cls, script_text=\"\", executable=None):\n        \"\"\"Create a #! line, getting options (if any) from script_text\"\"\"\n        cmd = cls.command_spec_class.best().from_param(executable)\n        cmd.install_options(script_text)\n        return cmd.as_header()\n\n\nclass WindowsScriptWriter(ScriptWriter):\n    command_spec_class = WindowsCommandSpec\n\n    @classmethod\n    def get_writer(cls):\n        # for backward compatibility\n        warnings.warn(\"Use best\", EasyInstallDeprecationWarning)\n        return cls.best()\n\n    @classmethod\n    def best(cls):\n        \"\"\"\n        Select the best ScriptWriter suitable for Windows\n        \"\"\"\n        writer_lookup = dict(\n            executable=WindowsExecutableLauncherWriter,\n            natural=cls,\n        )\n        # for compatibility, use the executable launcher by default\n        launcher = os.environ.get('SETUPTOOLS_LAUNCHER', 'executable')\n        return writer_lookup[launcher]\n\n    @classmethod\n    def _get_script_args(cls, type_, name, header, script_text):\n        \"For Windows, add a .py extension\"\n        ext = dict(console='.pya', gui='.pyw')[type_]\n        if ext not in os.environ['PATHEXT'].lower().split(';'):\n            msg = (\n                \"{ext} not listed in PATHEXT; scripts will not be \"\n                \"recognized as executables.\"\n            ).format(**locals())\n            warnings.warn(msg, UserWarning)\n        old = ['.pya', '.py', '-script.py', '.pyc', '.pyo', '.pyw', '.exe']\n        old.remove(ext)\n        header = cls._adjust_header(type_, header)\n        blockers = [name + x for x in old]\n        yield name + ext, header + script_text, 't', blockers\n\n    @classmethod\n    def _adjust_header(cls, type_, orig_header):\n        \"\"\"\n        Make sure 'pythonw' is used for gui and and 'python' is used for\n        console (regardless of what sys.executable is).\n        \"\"\"\n        pattern = 'pythonw.exe'\n        repl = 'python.exe'\n        if type_ == 'gui':\n            pattern, repl = repl, pattern\n        pattern_ob = re.compile(re.escape(pattern), re.IGNORECASE)\n        new_header = pattern_ob.sub(string=orig_header, repl=repl)\n        return new_header if cls._use_header(new_header) else orig_header\n\n    @staticmethod\n    def _use_header(new_header):\n        \"\"\"\n        Should _adjust_header use the replaced header?\n\n        On non-windows systems, always use. On\n        Windows systems, only use the replaced header if it resolves\n        to an executable on the system.\n        \"\"\"\n        clean_header = new_header[2:-1].strip('\"')\n        return sys.platform != 'win32' or find_executable(clean_header)\n\n\nclass WindowsExecutableLauncherWriter(WindowsScriptWriter):\n    @classmethod\n    def _get_script_args(cls, type_, name, header, script_text):\n        \"\"\"\n        For Windows, add a .py extension and an .exe launcher\n        \"\"\"\n        if type_ == 'gui':\n            launcher_type = 'gui'\n            ext = '-script.pyw'\n            old = ['.pyw']\n        else:\n            launcher_type = 'cli'\n            ext = '-script.py'\n            old = ['.py', '.pyc', '.pyo']\n        hdr = cls._adjust_header(type_, header)\n        blockers = [name + x for x in old]\n        yield (name + ext, hdr + script_text, 't', blockers)\n        yield (\n            name + '.exe', get_win_launcher(launcher_type),\n            'b'  # write in binary mode\n        )\n        if not is_64bit():\n            # install a manifest for the launcher to prevent Windows\n            # from detecting it as an installer (which it will for\n            #  launchers like easy_install.exe). Consider only\n            #  adding a manifest for launchers detected as installers.\n            #  See Distribute #143 for details.\n            m_name = name + '.exe.manifest'\n            yield (m_name, load_launcher_manifest(name), 't')\n\n\n# for backward-compatibility\nget_script_args = ScriptWriter.get_script_args\nget_script_header = ScriptWriter.get_script_header\n\n\ndef get_win_launcher(type):\n    \"\"\"\n    Load the Windows launcher (executable) suitable for launching a script.\n\n    `type` should be either 'cli' or 'gui'\n\n    Returns the executable as a byte string.\n    \"\"\"\n    launcher_fn = '%s.exe' % type\n    if is_64bit():\n        launcher_fn = launcher_fn.replace(\".\", \"-64.\")\n    else:\n        launcher_fn = launcher_fn.replace(\".\", \"-32.\")\n    return resource_string('setuptools', launcher_fn)\n\n\ndef load_launcher_manifest(name):\n    manifest = pkg_resources.resource_string(__name__, 'launcher manifest.xml')\n    if six.PY2:\n        return manifest % vars()\n    else:\n        return manifest.decode('utf-8') % vars()\n\n\ndef rmtree(path, ignore_errors=False, onerror=auto_chmod):\n    return shutil.rmtree(path, ignore_errors, onerror)\n\n\ndef current_umask():\n    tmp = os.umask(0o022)\n    os.umask(tmp)\n    return tmp\n\n\ndef bootstrap():\n    # This function is called when setuptools*.egg is run using /bin/sh\n    import setuptools\n\n    argv0 = os.path.dirname(setuptools.__path__[0])\n    sys.argv[0] = argv0\n    sys.argv.append(argv0)\n    main()\n\n\ndef main(argv=None, **kw):\n    from setuptools import setup\n    from setuptools.dist import Distribution\n\n    class DistributionWithoutHelpCommands(Distribution):\n        common_usage = \"\"\n\n        def _show_help(self, *args, **kw):\n            with _patch_usage():\n                Distribution._show_help(self, *args, **kw)\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    with _patch_usage():\n        setup(\n            script_args=['-q', 'easy_install', '-v'] + argv,\n            script_name=sys.argv[0] or 'easy_install',\n            distclass=DistributionWithoutHelpCommands,\n            **kw\n        )\n\n\n@contextlib.contextmanager\ndef _patch_usage():\n    import distutils.core\n    USAGE = textwrap.dedent(\"\"\"\n        usage: %(script)s [options] requirement_or_url ...\n           or: %(script)s --help\n        \"\"\").lstrip()\n\n    def gen_usage(script_name):\n        return USAGE % dict(\n            script=os.path.basename(script_name),\n        )\n\n    saved = distutils.core.gen_usage\n    distutils.core.gen_usage = gen_usage\n    try:\n        yield\n    finally:\n        distutils.core.gen_usage = saved\n\n\nclass EasyInstallDeprecationWarning(SetuptoolsDeprecationWarning):\n    \"\"\"\n    Warning for EasyInstall deprecations, bypassing suppression.\n    \"\"\"\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/command/easy_install.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/command/easy_install.py	(date 1602088701621)
@@ -38,18 +38,15 @@
 import subprocess
 import shlex
 import io
+import configparser
 
 
 from sysconfig import get_config_vars, get_path
 
 from setuptools import SetuptoolsDeprecationWarning
 
-from setuptools.extern import six
-from setuptools.extern.six.moves import configparser, map
-
 from setuptools import Command
 from setuptools.sandbox import run_setup
-from setuptools.py27compat import rmtree_safe
 from setuptools.command import setopt
 from setuptools.archive_util import unpack_archive
 from setuptools.package_index import (
@@ -65,8 +62,6 @@
 )
 import pkg_resources
 
-__metaclass__ = type
-
 # Turn on PEP440Warnings
 warnings.filterwarnings("default", category=pkg_resources.PEP440Warning)
 
@@ -96,28 +91,16 @@
     return norm_p1 == norm_p2
 
 
-if six.PY2:
-
-    def _to_bytes(s):
-        return s
-
-    def isascii(s):
-        try:
-            six.text_type(s, 'ascii')
-            return True
-        except UnicodeError:
-            return False
-else:
-
-    def _to_bytes(s):
-        return s.encode('utf8')
+def _to_bytes(s):
+    return s.encode('utf8')
 
-    def isascii(s):
-        try:
-            s.encode('ascii')
-            return True
-        except UnicodeError:
-            return False
+
+def isascii(s):
+    try:
+        s.encode('ascii')
+        return True
+    except UnicodeError:
+        return False
 
 
 def _one_liner(text):
@@ -341,7 +324,7 @@
         self.local_index = Environment(self.shadow_path + sys.path)
 
         if self.find_links is not None:
-            if isinstance(self.find_links, six.string_types):
+            if isinstance(self.find_links, str):
                 self.find_links = self.find_links.split()
         else:
             self.find_links = []
@@ -650,7 +633,7 @@
             # cast to str as workaround for #709 and #710 and #712
             yield str(tmpdir)
         finally:
-            os.path.exists(tmpdir) and rmtree(rmtree_safe(tmpdir))
+            os.path.exists(tmpdir) and rmtree(tmpdir)
 
     def easy_install(self, spec, deps=False):
         with self._tmpdir() as tmpdir:
@@ -1318,7 +1301,7 @@
         if not self.user:
             return
         home = convert_path(os.path.expanduser("~"))
-        for name, path in six.iteritems(self.config_vars):
+        for name, path in self.config_vars.items():
             if path.startswith(home) and not os.path.isdir(path):
                 self.debug_print("os.makedirs('%s', 0o700)" % path)
                 os.makedirs(path, 0o700)
@@ -1499,7 +1482,7 @@
             # Now the config is in bytes, but for RawConfigParser, it should
             #  be text, so decode it.
             config = config.decode(sys.getfilesystemencoding())
-            cfg.readfp(six.StringIO(config))
+            cfg.readfp(io.StringIO(config))
         except configparser.Error:
             return None
         if not cfg.has_section('metadata') or not cfg.has_section('Setup'):
@@ -1534,9 +1517,7 @@
             if name.endswith('-nspkg.pth'):
                 continue
             if parts[0].upper() in ('PURELIB', 'PLATLIB'):
-                contents = z.read(name)
-                if not six.PY2:
-                    contents = contents.decode()
+                contents = z.read(name).decode()
                 for pth in yield_lines(contents):
                     pth = pth.strip().replace('\\', '/')
                     if not pth.startswith('import'):
@@ -1700,7 +1681,8 @@
         chmod(arg, stat.S_IWRITE)
         return func(arg)
     et, ev, _ = sys.exc_info()
-    six.reraise(et, (ev[0], ev[1] + (" %s %s" % (func, arg))))
+    # TODO: This code doesn't make sense. What is it trying to do?
+    raise (ev[0], ev[1] + (" %s %s" % (func, arg)))
 
 
 def update_dist_caches(dist_path, fix_zipimporter_caches):
@@ -2263,10 +2245,7 @@
 
 def load_launcher_manifest(name):
     manifest = pkg_resources.resource_string(__name__, 'launcher manifest.xml')
-    if six.PY2:
-        return manifest % vars()
-    else:
-        return manifest.decode('utf-8') % vars()
+    return manifest.decode('utf-8') % vars()
 
 
 def rmtree(path, ignore_errors=False, onerror=auto_chmod):
Index: env/lib/python3.8/site-packages/pip/_internal/utils/virtualenv.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from __future__ import absolute_import\n\nimport logging\nimport os\nimport re\nimport site\nimport sys\n\nfrom pip._internal.utils.typing import MYPY_CHECK_RUNNING\n\nif MYPY_CHECK_RUNNING:\n    from typing import List, Optional\n\nlogger = logging.getLogger(__name__)\n_INCLUDE_SYSTEM_SITE_PACKAGES_REGEX = re.compile(\n    r\"include-system-site-packages\\s*=\\s*(?P<value>true|false)\"\n)\n\n\ndef _running_under_venv():\n    # type: () -> bool\n    \"\"\"Checks if sys.base_prefix and sys.prefix match.\n\n    This handles PEP 405 compliant virtual environments.\n    \"\"\"\n    return sys.prefix != getattr(sys, \"base_prefix\", sys.prefix)\n\n\ndef _running_under_regular_virtualenv():\n    # type: () -> bool\n    \"\"\"Checks if sys.real_prefix is set.\n\n    This handles virtual environments created with pypa's virtualenv.\n    \"\"\"\n    # pypa/virtualenv case\n    return hasattr(sys, 'real_prefix')\n\n\ndef running_under_virtualenv():\n    # type: () -> bool\n    \"\"\"Return True if we're running inside a virtualenv, False otherwise.\n    \"\"\"\n    return _running_under_venv() or _running_under_regular_virtualenv()\n\n\ndef _get_pyvenv_cfg_lines():\n    # type: () -> Optional[List[str]]\n    \"\"\"Reads {sys.prefix}/pyvenv.cfg and returns its contents as list of lines\n\n    Returns None, if it could not read/access the file.\n    \"\"\"\n    pyvenv_cfg_file = os.path.join(sys.prefix, 'pyvenv.cfg')\n    try:\n        with open(pyvenv_cfg_file) as f:\n            return f.read().splitlines()  # avoids trailing newlines\n    except IOError:\n        return None\n\n\ndef _no_global_under_venv():\n    # type: () -> bool\n    \"\"\"Check `{sys.prefix}/pyvenv.cfg` for system site-packages inclusion\n\n    PEP 405 specifies that when system site-packages are not supposed to be\n    visible from a virtual environment, `pyvenv.cfg` must contain the following\n    line:\n\n        include-system-site-packages = false\n\n    Additionally, log a warning if accessing the file fails.\n    \"\"\"\n    cfg_lines = _get_pyvenv_cfg_lines()\n    if cfg_lines is None:\n        # We're not in a \"sane\" venv, so assume there is no system\n        # site-packages access (since that's PEP 405's default state).\n        logger.warning(\n            \"Could not access 'pyvenv.cfg' despite a virtual environment \"\n            \"being active. Assuming global site-packages is not accessible \"\n            \"in this environment.\"\n        )\n        return True\n\n    for line in cfg_lines:\n        match = _INCLUDE_SYSTEM_SITE_PACKAGES_REGEX.match(line)\n        if match is not None and match.group('value') == 'false':\n            return True\n    return False\n\n\ndef _no_global_under_regular_virtualenv():\n    # type: () -> bool\n    \"\"\"Check if \"no-global-site-packages.txt\" exists beside site.py\n\n    This mirrors logic in pypa/virtualenv for determining whether system\n    site-packages are visible in the virtual environment.\n    \"\"\"\n    site_mod_dir = os.path.dirname(os.path.abspath(site.__file__))\n    no_global_site_packages_file = os.path.join(\n        site_mod_dir, 'no-global-site-packages.txt',\n    )\n    return os.path.exists(no_global_site_packages_file)\n\n\ndef virtualenv_no_global():\n    # type: () -> bool\n    \"\"\"Returns a boolean, whether running in venv with no system site-packages.\n    \"\"\"\n    # PEP 405 compliance needs to be checked first since virtualenv >=20 would\n    # return True for both checks, but is only able to use the PEP 405 config.\n    if _running_under_venv():\n        return _no_global_under_venv()\n\n    if _running_under_regular_virtualenv():\n        return _no_global_under_regular_virtualenv()\n\n    return False\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pip/_internal/utils/virtualenv.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pip/_internal/utils/virtualenv.py	(date 1602088700449)
@@ -1,5 +1,6 @@
 from __future__ import absolute_import
 
+import io
 import logging
 import os
 import re
@@ -51,7 +52,9 @@
     """
     pyvenv_cfg_file = os.path.join(sys.prefix, 'pyvenv.cfg')
     try:
-        with open(pyvenv_cfg_file) as f:
+        # Although PEP 405 does not specify, the built-in venv module always
+        # writes with UTF-8. (pypa/pip#8717)
+        with io.open(pyvenv_cfg_file, encoding='utf-8') as f:
             return f.read().splitlines()  # avoids trailing newlines
     except IOError:
         return None
Index: env/lib/python3.8/site-packages/setuptools/command/bdist_egg.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"setuptools.command.bdist_egg\n\nBuild .egg distributions\"\"\"\n\nfrom distutils.errors import DistutilsSetupError\nfrom distutils.dir_util import remove_tree, mkpath\nfrom distutils import log\nfrom types import CodeType\nimport sys\nimport os\nimport re\nimport textwrap\nimport marshal\nimport warnings\n\nfrom setuptools.extern import six\n\nfrom pkg_resources import get_build_platform, Distribution, ensure_directory\nfrom pkg_resources import EntryPoint\nfrom setuptools.extension import Library\nfrom setuptools import Command, SetuptoolsDeprecationWarning\n\ntry:\n    # Python 2.7 or >=3.2\n    from sysconfig import get_path, get_python_version\n\n    def _get_purelib():\n        return get_path(\"purelib\")\nexcept ImportError:\n    from distutils.sysconfig import get_python_lib, get_python_version\n\n    def _get_purelib():\n        return get_python_lib(False)\n\n\ndef strip_module(filename):\n    if '.' in filename:\n        filename = os.path.splitext(filename)[0]\n    if filename.endswith('module'):\n        filename = filename[:-6]\n    return filename\n\n\ndef sorted_walk(dir):\n    \"\"\"Do os.walk in a reproducible way,\n    independent of indeterministic filesystem readdir order\n    \"\"\"\n    for base, dirs, files in os.walk(dir):\n        dirs.sort()\n        files.sort()\n        yield base, dirs, files\n\n\ndef write_stub(resource, pyfile):\n    _stub_template = textwrap.dedent(\"\"\"\n        def __bootstrap__():\n            global __bootstrap__, __loader__, __file__\n            import sys, pkg_resources\n            from importlib.machinery import ExtensionFileLoader\n            __file__ = pkg_resources.resource_filename(__name__, %r)\n            __loader__ = None; del __bootstrap__, __loader__\n            ExtensionFileLoader(__name__,__file__).load_module()\n        __bootstrap__()\n        \"\"\").lstrip()\n    with open(pyfile, 'w') as f:\n        f.write(_stub_template % resource)\n\n\nclass bdist_egg(Command):\n    description = \"create an \\\"egg\\\" distribution\"\n\n    user_options = [\n        ('bdist-dir=', 'b',\n         \"temporary directory for creating the distribution\"),\n        ('plat-name=', 'p', \"platform name to embed in generated filenames \"\n                            \"(default: %s)\" % get_build_platform()),\n        ('exclude-source-files', None,\n         \"remove all .py files from the generated egg\"),\n        ('keep-temp', 'k',\n         \"keep the pseudo-installation tree around after \" +\n         \"creating the distribution archive\"),\n        ('dist-dir=', 'd',\n         \"directory to put final built distributions in\"),\n        ('skip-build', None,\n         \"skip rebuilding everything (for testing/debugging)\"),\n    ]\n\n    boolean_options = [\n        'keep-temp', 'skip-build', 'exclude-source-files'\n    ]\n\n    def initialize_options(self):\n        self.bdist_dir = None\n        self.plat_name = None\n        self.keep_temp = 0\n        self.dist_dir = None\n        self.skip_build = 0\n        self.egg_output = None\n        self.exclude_source_files = None\n\n    def finalize_options(self):\n        ei_cmd = self.ei_cmd = self.get_finalized_command(\"egg_info\")\n        self.egg_info = ei_cmd.egg_info\n\n        if self.bdist_dir is None:\n            bdist_base = self.get_finalized_command('bdist').bdist_base\n            self.bdist_dir = os.path.join(bdist_base, 'egg')\n\n        if self.plat_name is None:\n            self.plat_name = get_build_platform()\n\n        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))\n\n        if self.egg_output is None:\n\n            # Compute filename of the output egg\n            basename = Distribution(\n                None, None, ei_cmd.egg_name, ei_cmd.egg_version,\n                get_python_version(),\n                self.distribution.has_ext_modules() and self.plat_name\n            ).egg_name()\n\n            self.egg_output = os.path.join(self.dist_dir, basename + '.egg')\n\n    def do_install_data(self):\n        # Hack for packages that install data to install's --install-lib\n        self.get_finalized_command('install').install_lib = self.bdist_dir\n\n        site_packages = os.path.normcase(os.path.realpath(_get_purelib()))\n        old, self.distribution.data_files = self.distribution.data_files, []\n\n        for item in old:\n            if isinstance(item, tuple) and len(item) == 2:\n                if os.path.isabs(item[0]):\n                    realpath = os.path.realpath(item[0])\n                    normalized = os.path.normcase(realpath)\n                    if normalized == site_packages or normalized.startswith(\n                        site_packages + os.sep\n                    ):\n                        item = realpath[len(site_packages) + 1:], item[1]\n                        # XXX else: raise ???\n            self.distribution.data_files.append(item)\n\n        try:\n            log.info(\"installing package data to %s\", self.bdist_dir)\n            self.call_command('install_data', force=0, root=None)\n        finally:\n            self.distribution.data_files = old\n\n    def get_outputs(self):\n        return [self.egg_output]\n\n    def call_command(self, cmdname, **kw):\n        \"\"\"Invoke reinitialized command `cmdname` with keyword args\"\"\"\n        for dirname in INSTALL_DIRECTORY_ATTRS:\n            kw.setdefault(dirname, self.bdist_dir)\n        kw.setdefault('skip_build', self.skip_build)\n        kw.setdefault('dry_run', self.dry_run)\n        cmd = self.reinitialize_command(cmdname, **kw)\n        self.run_command(cmdname)\n        return cmd\n\n    def run(self):\n        # Generate metadata first\n        self.run_command(\"egg_info\")\n        # We run install_lib before install_data, because some data hacks\n        # pull their data path from the install_lib command.\n        log.info(\"installing library code to %s\", self.bdist_dir)\n        instcmd = self.get_finalized_command('install')\n        old_root = instcmd.root\n        instcmd.root = None\n        if self.distribution.has_c_libraries() and not self.skip_build:\n            self.run_command('build_clib')\n        cmd = self.call_command('install_lib', warn_dir=0)\n        instcmd.root = old_root\n\n        all_outputs, ext_outputs = self.get_ext_outputs()\n        self.stubs = []\n        to_compile = []\n        for (p, ext_name) in enumerate(ext_outputs):\n            filename, ext = os.path.splitext(ext_name)\n            pyfile = os.path.join(self.bdist_dir, strip_module(filename) +\n                                  '.py')\n            self.stubs.append(pyfile)\n            log.info(\"creating stub loader for %s\", ext_name)\n            if not self.dry_run:\n                write_stub(os.path.basename(ext_name), pyfile)\n            to_compile.append(pyfile)\n            ext_outputs[p] = ext_name.replace(os.sep, '/')\n\n        if to_compile:\n            cmd.byte_compile(to_compile)\n        if self.distribution.data_files:\n            self.do_install_data()\n\n        # Make the EGG-INFO directory\n        archive_root = self.bdist_dir\n        egg_info = os.path.join(archive_root, 'EGG-INFO')\n        self.mkpath(egg_info)\n        if self.distribution.scripts:\n            script_dir = os.path.join(egg_info, 'scripts')\n            log.info(\"installing scripts to %s\", script_dir)\n            self.call_command('install_scripts', install_dir=script_dir,\n                              no_ep=1)\n\n        self.copy_metadata_to(egg_info)\n        native_libs = os.path.join(egg_info, \"native_libs.txt\")\n        if all_outputs:\n            log.info(\"writing %s\", native_libs)\n            if not self.dry_run:\n                ensure_directory(native_libs)\n                libs_file = open(native_libs, 'wt')\n                libs_file.write('\\n'.join(all_outputs))\n                libs_file.write('\\n')\n                libs_file.close()\n        elif os.path.isfile(native_libs):\n            log.info(\"removing %s\", native_libs)\n            if not self.dry_run:\n                os.unlink(native_libs)\n\n        write_safety_flag(\n            os.path.join(archive_root, 'EGG-INFO'), self.zip_safe()\n        )\n\n        if os.path.exists(os.path.join(self.egg_info, 'depends.txt')):\n            log.warn(\n                \"WARNING: 'depends.txt' will not be used by setuptools 0.6!\\n\"\n                \"Use the install_requires/extras_require setup() args instead.\"\n            )\n\n        if self.exclude_source_files:\n            self.zap_pyfiles()\n\n        # Make the archive\n        make_zipfile(self.egg_output, archive_root, verbose=self.verbose,\n                     dry_run=self.dry_run, mode=self.gen_header())\n        if not self.keep_temp:\n            remove_tree(self.bdist_dir, dry_run=self.dry_run)\n\n        # Add to 'Distribution.dist_files' so that the \"upload\" command works\n        getattr(self.distribution, 'dist_files', []).append(\n            ('bdist_egg', get_python_version(), self.egg_output))\n\n    def zap_pyfiles(self):\n        log.info(\"Removing .py files from temporary directory\")\n        for base, dirs, files in walk_egg(self.bdist_dir):\n            for name in files:\n                path = os.path.join(base, name)\n\n                if name.endswith('.py'):\n                    log.debug(\"Deleting %s\", path)\n                    os.unlink(path)\n\n                if base.endswith('__pycache__'):\n                    path_old = path\n\n                    pattern = r'(?P<name>.+)\\.(?P<magic>[^.]+)\\.pyc'\n                    m = re.match(pattern, name)\n                    path_new = os.path.join(\n                        base, os.pardir, m.group('name') + '.pyc')\n                    log.info(\n                        \"Renaming file from [%s] to [%s]\"\n                        % (path_old, path_new))\n                    try:\n                        os.remove(path_new)\n                    except OSError:\n                        pass\n                    os.rename(path_old, path_new)\n\n    def zip_safe(self):\n        safe = getattr(self.distribution, 'zip_safe', None)\n        if safe is not None:\n            return safe\n        log.warn(\"zip_safe flag not set; analyzing archive contents...\")\n        return analyze_egg(self.bdist_dir, self.stubs)\n\n    def gen_header(self):\n        epm = EntryPoint.parse_map(self.distribution.entry_points or '')\n        ep = epm.get('setuptools.installation', {}).get('eggsecutable')\n        if ep is None:\n            return 'w'  # not an eggsecutable, do it the usual way.\n\n        warnings.warn(\n            \"Eggsecutables are deprecated and will be removed in a future \"\n            \"version.\",\n            SetuptoolsDeprecationWarning\n        )\n\n        if not ep.attrs or ep.extras:\n            raise DistutilsSetupError(\n                \"eggsecutable entry point (%r) cannot have 'extras' \"\n                \"or refer to a module\" % (ep,)\n            )\n\n        pyver = '{}.{}'.format(*sys.version_info)\n        pkg = ep.module_name\n        full = '.'.join(ep.attrs)\n        base = ep.attrs[0]\n        basename = os.path.basename(self.egg_output)\n\n        header = (\n            \"#!/bin/sh\\n\"\n            'if [ `basename $0` = \"%(basename)s\" ]\\n'\n            'then exec python%(pyver)s -c \"'\n            \"import sys, os; sys.path.insert(0, os.path.abspath('$0')); \"\n            \"from %(pkg)s import %(base)s; sys.exit(%(full)s())\"\n            '\" \"$@\"\\n'\n            'else\\n'\n            '  echo $0 is not the correct name for this egg file.\\n'\n            '  echo Please rename it back to %(basename)s and try again.\\n'\n            '  exec false\\n'\n            'fi\\n'\n        ) % locals()\n\n        if not self.dry_run:\n            mkpath(os.path.dirname(self.egg_output), dry_run=self.dry_run)\n            f = open(self.egg_output, 'w')\n            f.write(header)\n            f.close()\n        return 'a'\n\n    def copy_metadata_to(self, target_dir):\n        \"Copy metadata (egg info) to the target_dir\"\n        # normalize the path (so that a forward-slash in egg_info will\n        # match using startswith below)\n        norm_egg_info = os.path.normpath(self.egg_info)\n        prefix = os.path.join(norm_egg_info, '')\n        for path in self.ei_cmd.filelist.files:\n            if path.startswith(prefix):\n                target = os.path.join(target_dir, path[len(prefix):])\n                ensure_directory(target)\n                self.copy_file(path, target)\n\n    def get_ext_outputs(self):\n        \"\"\"Get a list of relative paths to C extensions in the output distro\"\"\"\n\n        all_outputs = []\n        ext_outputs = []\n\n        paths = {self.bdist_dir: ''}\n        for base, dirs, files in sorted_walk(self.bdist_dir):\n            for filename in files:\n                if os.path.splitext(filename)[1].lower() in NATIVE_EXTENSIONS:\n                    all_outputs.append(paths[base] + filename)\n            for filename in dirs:\n                paths[os.path.join(base, filename)] = (paths[base] +\n                                                       filename + '/')\n\n        if self.distribution.has_ext_modules():\n            build_cmd = self.get_finalized_command('build_ext')\n            for ext in build_cmd.extensions:\n                if isinstance(ext, Library):\n                    continue\n                fullname = build_cmd.get_ext_fullname(ext.name)\n                filename = build_cmd.get_ext_filename(fullname)\n                if not os.path.basename(filename).startswith('dl-'):\n                    if os.path.exists(os.path.join(self.bdist_dir, filename)):\n                        ext_outputs.append(filename)\n\n        return all_outputs, ext_outputs\n\n\nNATIVE_EXTENSIONS = dict.fromkeys('.dll .so .dylib .pyd'.split())\n\n\ndef walk_egg(egg_dir):\n    \"\"\"Walk an unpacked egg's contents, skipping the metadata directory\"\"\"\n    walker = sorted_walk(egg_dir)\n    base, dirs, files = next(walker)\n    if 'EGG-INFO' in dirs:\n        dirs.remove('EGG-INFO')\n    yield base, dirs, files\n    for bdf in walker:\n        yield bdf\n\n\ndef analyze_egg(egg_dir, stubs):\n    # check for existing flag in EGG-INFO\n    for flag, fn in safety_flags.items():\n        if os.path.exists(os.path.join(egg_dir, 'EGG-INFO', fn)):\n            return flag\n    if not can_scan():\n        return False\n    safe = True\n    for base, dirs, files in walk_egg(egg_dir):\n        for name in files:\n            if name.endswith('.py') or name.endswith('.pyw'):\n                continue\n            elif name.endswith('.pyc') or name.endswith('.pyo'):\n                # always scan, even if we already know we're not safe\n                safe = scan_module(egg_dir, base, name, stubs) and safe\n    return safe\n\n\ndef write_safety_flag(egg_dir, safe):\n    # Write or remove zip safety flag file(s)\n    for flag, fn in safety_flags.items():\n        fn = os.path.join(egg_dir, fn)\n        if os.path.exists(fn):\n            if safe is None or bool(safe) != flag:\n                os.unlink(fn)\n        elif safe is not None and bool(safe) == flag:\n            f = open(fn, 'wt')\n            f.write('\\n')\n            f.close()\n\n\nsafety_flags = {\n    True: 'zip-safe',\n    False: 'not-zip-safe',\n}\n\n\ndef scan_module(egg_dir, base, name, stubs):\n    \"\"\"Check whether module possibly uses unsafe-for-zipfile stuff\"\"\"\n\n    filename = os.path.join(base, name)\n    if filename[:-1] in stubs:\n        return True  # Extension module\n    pkg = base[len(egg_dir) + 1:].replace(os.sep, '.')\n    module = pkg + (pkg and '.' or '') + os.path.splitext(name)[0]\n    if six.PY2:\n        skip = 8  # skip magic & date\n    elif sys.version_info < (3, 7):\n        skip = 12  # skip magic & date & file size\n    else:\n        skip = 16  # skip magic & reserved? & date & file size\n    f = open(filename, 'rb')\n    f.read(skip)\n    code = marshal.load(f)\n    f.close()\n    safe = True\n    symbols = dict.fromkeys(iter_symbols(code))\n    for bad in ['__file__', '__path__']:\n        if bad in symbols:\n            log.warn(\"%s: module references %s\", module, bad)\n            safe = False\n    if 'inspect' in symbols:\n        for bad in [\n            'getsource', 'getabsfile', 'getsourcefile', 'getfile'\n            'getsourcelines', 'findsource', 'getcomments', 'getframeinfo',\n            'getinnerframes', 'getouterframes', 'stack', 'trace'\n        ]:\n            if bad in symbols:\n                log.warn(\"%s: module MAY be using inspect.%s\", module, bad)\n                safe = False\n    return safe\n\n\ndef iter_symbols(code):\n    \"\"\"Yield names and strings used by `code` and its nested code objects\"\"\"\n    for name in code.co_names:\n        yield name\n    for const in code.co_consts:\n        if isinstance(const, six.string_types):\n            yield const\n        elif isinstance(const, CodeType):\n            for name in iter_symbols(const):\n                yield name\n\n\ndef can_scan():\n    if not sys.platform.startswith('java') and sys.platform != 'cli':\n        # CPython, PyPy, etc.\n        return True\n    log.warn(\"Unable to analyze compiled code on this platform.\")\n    log.warn(\"Please ask the author to include a 'zip_safe'\"\n             \" setting (either True or False) in the package's setup.py\")\n\n\n# Attribute names of options for commands that might need to be convinced to\n# install to the egg build directory\n\nINSTALL_DIRECTORY_ATTRS = [\n    'install_lib', 'install_dir', 'install_data', 'install_base'\n]\n\n\ndef make_zipfile(zip_filename, base_dir, verbose=0, dry_run=0, compress=True,\n                 mode='w'):\n    \"\"\"Create a zip file from all the files under 'base_dir'.  The output\n    zip file will be named 'base_dir' + \".zip\".  Uses either the \"zipfile\"\n    Python module (if available) or the InfoZIP \"zip\" utility (if installed\n    and found on the default search path).  If neither tool is available,\n    raises DistutilsExecError.  Returns the name of the output zip file.\n    \"\"\"\n    import zipfile\n\n    mkpath(os.path.dirname(zip_filename), dry_run=dry_run)\n    log.info(\"creating '%s' and adding '%s' to it\", zip_filename, base_dir)\n\n    def visit(z, dirname, names):\n        for name in names:\n            path = os.path.normpath(os.path.join(dirname, name))\n            if os.path.isfile(path):\n                p = path[len(base_dir) + 1:]\n                if not dry_run:\n                    z.write(path, p)\n                log.debug(\"adding '%s'\", p)\n\n    compression = zipfile.ZIP_DEFLATED if compress else zipfile.ZIP_STORED\n    if not dry_run:\n        z = zipfile.ZipFile(zip_filename, mode, compression=compression)\n        for dirname, dirs, files in sorted_walk(base_dir):\n            visit(z, dirname, files)\n        z.close()\n    else:\n        for dirname, dirs, files in sorted_walk(base_dir):\n            visit(None, dirname, files)\n    return zip_filename\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/command/bdist_egg.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/command/bdist_egg.py	(date 1602088701621)
@@ -13,24 +13,16 @@
 import marshal
 import warnings
 
-from setuptools.extern import six
-
 from pkg_resources import get_build_platform, Distribution, ensure_directory
 from pkg_resources import EntryPoint
 from setuptools.extension import Library
 from setuptools import Command, SetuptoolsDeprecationWarning
 
-try:
-    # Python 2.7 or >=3.2
-    from sysconfig import get_path, get_python_version
+from sysconfig import get_path, get_python_version
 
-    def _get_purelib():
-        return get_path("purelib")
-except ImportError:
-    from distutils.sysconfig import get_python_lib, get_python_version
-
-    def _get_purelib():
-        return get_python_lib(False)
+
+def _get_purelib():
+    return get_path("purelib")
 
 
 def strip_module(filename):
@@ -55,11 +47,12 @@
     _stub_template = textwrap.dedent("""
         def __bootstrap__():
             global __bootstrap__, __loader__, __file__
-            import sys, pkg_resources
-            from importlib.machinery import ExtensionFileLoader
+            import sys, pkg_resources, importlib.util
             __file__ = pkg_resources.resource_filename(__name__, %r)
             __loader__ = None; del __bootstrap__, __loader__
-            ExtensionFileLoader(__name__,__file__).load_module()
+            spec = importlib.util.spec_from_file_location(__name__,__file__)
+            mod = importlib.util.module_from_spec(spec)
+            spec.loader.exec_module(mod)
         __bootstrap__()
         """).lstrip()
     with open(pyfile, 'w') as f:
@@ -419,9 +412,7 @@
         return True  # Extension module
     pkg = base[len(egg_dir) + 1:].replace(os.sep, '.')
     module = pkg + (pkg and '.' or '') + os.path.splitext(name)[0]
-    if six.PY2:
-        skip = 8  # skip magic & date
-    elif sys.version_info < (3, 7):
+    if sys.version_info < (3, 7):
         skip = 12  # skip magic & date & file size
     else:
         skip = 16  # skip magic & reserved? & date & file size
@@ -452,7 +443,7 @@
     for name in code.co_names:
         yield name
     for const in code.co_consts:
-        if isinstance(const, six.string_types):
+        if isinstance(const, str):
             yield const
         elif isinstance(const, CodeType):
             for name in iter_symbols(const):
Index: env/lib/python3.8/site-packages/setuptools/command/sdist.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from distutils import log\nimport distutils.command.sdist as orig\nimport os\nimport sys\nimport io\nimport contextlib\n\nfrom setuptools.extern import six, ordered_set\n\nfrom .py36compat import sdist_add_defaults\n\nimport pkg_resources\n\n_default_revctrl = list\n\n\ndef walk_revctrl(dirname=''):\n    \"\"\"Find all files under revision control\"\"\"\n    for ep in pkg_resources.iter_entry_points('setuptools.file_finders'):\n        for item in ep.load()(dirname):\n            yield item\n\n\nclass sdist(sdist_add_defaults, orig.sdist):\n    \"\"\"Smart sdist that finds anything supported by revision control\"\"\"\n\n    user_options = [\n        ('formats=', None,\n         \"formats for source distribution (comma-separated list)\"),\n        ('keep-temp', 'k',\n         \"keep the distribution tree around after creating \" +\n         \"archive file(s)\"),\n        ('dist-dir=', 'd',\n         \"directory to put the source distribution archive(s) in \"\n         \"[default: dist]\"),\n    ]\n\n    negative_opt = {}\n\n    README_EXTENSIONS = ['', '.rst', '.txt', '.md']\n    READMES = tuple('README{0}'.format(ext) for ext in README_EXTENSIONS)\n\n    def run(self):\n        self.run_command('egg_info')\n        ei_cmd = self.get_finalized_command('egg_info')\n        self.filelist = ei_cmd.filelist\n        self.filelist.append(os.path.join(ei_cmd.egg_info, 'SOURCES.txt'))\n        self.check_readme()\n\n        # Run sub commands\n        for cmd_name in self.get_sub_commands():\n            self.run_command(cmd_name)\n\n        self.make_distribution()\n\n        dist_files = getattr(self.distribution, 'dist_files', [])\n        for file in self.archive_files:\n            data = ('sdist', '', file)\n            if data not in dist_files:\n                dist_files.append(data)\n\n    def initialize_options(self):\n        orig.sdist.initialize_options(self)\n\n        self._default_to_gztar()\n\n    def _default_to_gztar(self):\n        # only needed on Python prior to 3.6.\n        if sys.version_info >= (3, 6, 0, 'beta', 1):\n            return\n        self.formats = ['gztar']\n\n    def make_distribution(self):\n        \"\"\"\n        Workaround for #516\n        \"\"\"\n        with self._remove_os_link():\n            orig.sdist.make_distribution(self)\n\n    @staticmethod\n    @contextlib.contextmanager\n    def _remove_os_link():\n        \"\"\"\n        In a context, remove and restore os.link if it exists\n        \"\"\"\n\n        class NoValue:\n            pass\n\n        orig_val = getattr(os, 'link', NoValue)\n        try:\n            del os.link\n        except Exception:\n            pass\n        try:\n            yield\n        finally:\n            if orig_val is not NoValue:\n                setattr(os, 'link', orig_val)\n\n    def __read_template_hack(self):\n        # This grody hack closes the template file (MANIFEST.in) if an\n        #  exception occurs during read_template.\n        # Doing so prevents an error when easy_install attempts to delete the\n        #  file.\n        try:\n            orig.sdist.read_template(self)\n        except Exception:\n            _, _, tb = sys.exc_info()\n            tb.tb_next.tb_frame.f_locals['template'].close()\n            raise\n\n    # Beginning with Python 2.7.2, 3.1.4, and 3.2.1, this leaky file handle\n    #  has been fixed, so only override the method if we're using an earlier\n    #  Python.\n    has_leaky_handle = (\n        sys.version_info < (2, 7, 2)\n        or (3, 0) <= sys.version_info < (3, 1, 4)\n        or (3, 2) <= sys.version_info < (3, 2, 1)\n    )\n    if has_leaky_handle:\n        read_template = __read_template_hack\n\n    def _add_defaults_optional(self):\n        if six.PY2:\n            sdist_add_defaults._add_defaults_optional(self)\n        else:\n            super()._add_defaults_optional()\n        if os.path.isfile('pyproject.toml'):\n            self.filelist.append('pyproject.toml')\n\n    def _add_defaults_python(self):\n        \"\"\"getting python files\"\"\"\n        if self.distribution.has_pure_modules():\n            build_py = self.get_finalized_command('build_py')\n            self.filelist.extend(build_py.get_source_files())\n            self._add_data_files(self._safe_data_files(build_py))\n\n    def _safe_data_files(self, build_py):\n        \"\"\"\n        Extracting data_files from build_py is known to cause\n        infinite recursion errors when `include_package_data`\n        is enabled, so suppress it in that case.\n        \"\"\"\n        if self.distribution.include_package_data:\n            return ()\n        return build_py.data_files\n\n    def _add_data_files(self, data_files):\n        \"\"\"\n        Add data files as found in build_py.data_files.\n        \"\"\"\n        self.filelist.extend(\n            os.path.join(src_dir, name)\n            for _, src_dir, _, filenames in data_files\n            for name in filenames\n        )\n\n    def _add_defaults_data_files(self):\n        try:\n            if six.PY2:\n                sdist_add_defaults._add_defaults_data_files(self)\n            else:\n                super()._add_defaults_data_files()\n        except TypeError:\n            log.warn(\"data_files contains unexpected objects\")\n\n    def check_readme(self):\n        for f in self.READMES:\n            if os.path.exists(f):\n                return\n        else:\n            self.warn(\n                \"standard file not found: should have one of \" +\n                ', '.join(self.READMES)\n            )\n\n    def make_release_tree(self, base_dir, files):\n        orig.sdist.make_release_tree(self, base_dir, files)\n\n        # Save any egg_info command line options used to create this sdist\n        dest = os.path.join(base_dir, 'setup.cfg')\n        if hasattr(os, 'link') and os.path.exists(dest):\n            # unlink and re-copy, since it might be hard-linked, and\n            # we don't want to change the source version\n            os.unlink(dest)\n            self.copy_file('setup.cfg', dest)\n\n        self.get_finalized_command('egg_info').save_version_info(dest)\n\n    def _manifest_is_not_generated(self):\n        # check for special comment used in 2.7.1 and higher\n        if not os.path.isfile(self.manifest):\n            return False\n\n        with io.open(self.manifest, 'rb') as fp:\n            first_line = fp.readline()\n        return (first_line !=\n                '# file GENERATED by distutils, do NOT edit\\n'.encode())\n\n    def read_manifest(self):\n        \"\"\"Read the manifest file (named by 'self.manifest') and use it to\n        fill in 'self.filelist', the list of files to include in the source\n        distribution.\n        \"\"\"\n        log.info(\"reading manifest file '%s'\", self.manifest)\n        manifest = open(self.manifest, 'rb')\n        for line in manifest:\n            # The manifest must contain UTF-8. See #303.\n            if not six.PY2:\n                try:\n                    line = line.decode('UTF-8')\n                except UnicodeDecodeError:\n                    log.warn(\"%r not UTF-8 decodable -- skipping\" % line)\n                    continue\n            # ignore comments and blank lines\n            line = line.strip()\n            if line.startswith('#') or not line:\n                continue\n            self.filelist.append(line)\n        manifest.close()\n\n    def check_license(self):\n        \"\"\"Checks if license_file' or 'license_files' is configured and adds any\n        valid paths to 'self.filelist'.\n        \"\"\"\n\n        files = ordered_set.OrderedSet()\n\n        opts = self.distribution.get_option_dict('metadata')\n\n        # ignore the source of the value\n        _, license_file = opts.get('license_file', (None, None))\n\n        if license_file is None:\n            log.debug(\"'license_file' option was not specified\")\n        else:\n            files.add(license_file)\n\n        try:\n            files.update(self.distribution.metadata.license_files)\n        except TypeError:\n            log.warn(\"warning: 'license_files' option is malformed\")\n\n        for f in files:\n            if not os.path.exists(f):\n                log.warn(\n                    \"warning: Failed to find the configured license file '%s'\",\n                    f)\n                files.remove(f)\n\n        self.filelist.extend(files)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/command/sdist.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/command/sdist.py	(date 1602088701625)
@@ -5,7 +5,7 @@
 import io
 import contextlib
 
-from setuptools.extern import six, ordered_set
+from setuptools.extern import ordered_set
 
 from .py36compat import sdist_add_defaults
 
@@ -98,34 +98,8 @@
             if orig_val is not NoValue:
                 setattr(os, 'link', orig_val)
 
-    def __read_template_hack(self):
-        # This grody hack closes the template file (MANIFEST.in) if an
-        #  exception occurs during read_template.
-        # Doing so prevents an error when easy_install attempts to delete the
-        #  file.
-        try:
-            orig.sdist.read_template(self)
-        except Exception:
-            _, _, tb = sys.exc_info()
-            tb.tb_next.tb_frame.f_locals['template'].close()
-            raise
-
-    # Beginning with Python 2.7.2, 3.1.4, and 3.2.1, this leaky file handle
-    #  has been fixed, so only override the method if we're using an earlier
-    #  Python.
-    has_leaky_handle = (
-        sys.version_info < (2, 7, 2)
-        or (3, 0) <= sys.version_info < (3, 1, 4)
-        or (3, 2) <= sys.version_info < (3, 2, 1)
-    )
-    if has_leaky_handle:
-        read_template = __read_template_hack
-
     def _add_defaults_optional(self):
-        if six.PY2:
-            sdist_add_defaults._add_defaults_optional(self)
-        else:
-            super()._add_defaults_optional()
+        super()._add_defaults_optional()
         if os.path.isfile('pyproject.toml'):
             self.filelist.append('pyproject.toml')
 
@@ -158,10 +132,7 @@
 
     def _add_defaults_data_files(self):
         try:
-            if six.PY2:
-                sdist_add_defaults._add_defaults_data_files(self)
-            else:
-                super()._add_defaults_data_files()
+            super()._add_defaults_data_files()
         except TypeError:
             log.warn("data_files contains unexpected objects")
 
@@ -207,12 +178,11 @@
         manifest = open(self.manifest, 'rb')
         for line in manifest:
             # The manifest must contain UTF-8. See #303.
-            if not six.PY2:
-                try:
-                    line = line.decode('UTF-8')
-                except UnicodeDecodeError:
-                    log.warn("%r not UTF-8 decodable -- skipping" % line)
-                    continue
+            try:
+                line = line.decode('UTF-8')
+            except UnicodeDecodeError:
+                log.warn("%r not UTF-8 decodable -- skipping" % line)
+                continue
             # ignore comments and blank lines
             line = line.strip()
             if line.startswith('#') or not line:
Index: env/lib/python3.8/site-packages/setuptools/command/upload_docs.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># -*- coding: utf-8 -*-\n\"\"\"upload_docs\n\nImplements a Distutils 'upload_docs' subcommand (upload documentation to\nPyPI's pythonhosted.org).\n\"\"\"\n\nfrom base64 import standard_b64encode\nfrom distutils import log\nfrom distutils.errors import DistutilsOptionError\nimport os\nimport socket\nimport zipfile\nimport tempfile\nimport shutil\nimport itertools\nimport functools\n\nfrom setuptools.extern import six\nfrom setuptools.extern.six.moves import http_client, urllib\n\nfrom pkg_resources import iter_entry_points\nfrom .upload import upload\n\n\ndef _encode(s):\n    errors = 'strict' if six.PY2 else 'surrogateescape'\n    return s.encode('utf-8', errors)\n\n\nclass upload_docs(upload):\n    # override the default repository as upload_docs isn't\n    # supported by Warehouse (and won't be).\n    DEFAULT_REPOSITORY = 'https://pypi.python.org/pypi/'\n\n    description = 'Upload documentation to PyPI'\n\n    user_options = [\n        ('repository=', 'r',\n         \"url of repository [default: %s]\" % upload.DEFAULT_REPOSITORY),\n        ('show-response', None,\n         'display full response text from server'),\n        ('upload-dir=', None, 'directory to upload'),\n    ]\n    boolean_options = upload.boolean_options\n\n    def has_sphinx(self):\n        if self.upload_dir is None:\n            for ep in iter_entry_points('distutils.commands', 'build_sphinx'):\n                return True\n\n    sub_commands = [('build_sphinx', has_sphinx)]\n\n    def initialize_options(self):\n        upload.initialize_options(self)\n        self.upload_dir = None\n        self.target_dir = None\n\n    def finalize_options(self):\n        upload.finalize_options(self)\n        if self.upload_dir is None:\n            if self.has_sphinx():\n                build_sphinx = self.get_finalized_command('build_sphinx')\n                self.target_dir = build_sphinx.builder_target_dir\n            else:\n                build = self.get_finalized_command('build')\n                self.target_dir = os.path.join(build.build_base, 'docs')\n        else:\n            self.ensure_dirname('upload_dir')\n            self.target_dir = self.upload_dir\n        if 'pypi.python.org' in self.repository:\n            log.warn(\"Upload_docs command is deprecated. Use RTD instead.\")\n        self.announce('Using upload directory %s' % self.target_dir)\n\n    def create_zipfile(self, filename):\n        zip_file = zipfile.ZipFile(filename, \"w\")\n        try:\n            self.mkpath(self.target_dir)  # just in case\n            for root, dirs, files in os.walk(self.target_dir):\n                if root == self.target_dir and not files:\n                    tmpl = \"no files found in upload directory '%s'\"\n                    raise DistutilsOptionError(tmpl % self.target_dir)\n                for name in files:\n                    full = os.path.join(root, name)\n                    relative = root[len(self.target_dir):].lstrip(os.path.sep)\n                    dest = os.path.join(relative, name)\n                    zip_file.write(full, dest)\n        finally:\n            zip_file.close()\n\n    def run(self):\n        # Run sub commands\n        for cmd_name in self.get_sub_commands():\n            self.run_command(cmd_name)\n\n        tmp_dir = tempfile.mkdtemp()\n        name = self.distribution.metadata.get_name()\n        zip_file = os.path.join(tmp_dir, \"%s.zip\" % name)\n        try:\n            self.create_zipfile(zip_file)\n            self.upload_file(zip_file)\n        finally:\n            shutil.rmtree(tmp_dir)\n\n    @staticmethod\n    def _build_part(item, sep_boundary):\n        key, values = item\n        title = '\\nContent-Disposition: form-data; name=\"%s\"' % key\n        # handle multiple entries for the same name\n        if not isinstance(values, list):\n            values = [values]\n        for value in values:\n            if isinstance(value, tuple):\n                title += '; filename=\"%s\"' % value[0]\n                value = value[1]\n            else:\n                value = _encode(value)\n            yield sep_boundary\n            yield _encode(title)\n            yield b\"\\n\\n\"\n            yield value\n            if value and value[-1:] == b'\\r':\n                yield b'\\n'  # write an extra newline (lurve Macs)\n\n    @classmethod\n    def _build_multipart(cls, data):\n        \"\"\"\n        Build up the MIME payload for the POST data\n        \"\"\"\n        boundary = '--------------GHSKFJDLGDS7543FJKLFHRE75642756743254'\n        sep_boundary = b'\\n--' + boundary.encode('ascii')\n        end_boundary = sep_boundary + b'--'\n        end_items = end_boundary, b\"\\n\",\n        builder = functools.partial(\n            cls._build_part,\n            sep_boundary=sep_boundary,\n        )\n        part_groups = map(builder, data.items())\n        parts = itertools.chain.from_iterable(part_groups)\n        body_items = itertools.chain(parts, end_items)\n        content_type = 'multipart/form-data; boundary=%s' % boundary\n        return b''.join(body_items), content_type\n\n    def upload_file(self, filename):\n        with open(filename, 'rb') as f:\n            content = f.read()\n        meta = self.distribution.metadata\n        data = {\n            ':action': 'doc_upload',\n            'name': meta.get_name(),\n            'content': (os.path.basename(filename), content),\n        }\n        # set up the authentication\n        credentials = _encode(self.username + ':' + self.password)\n        credentials = standard_b64encode(credentials)\n        if not six.PY2:\n            credentials = credentials.decode('ascii')\n        auth = \"Basic \" + credentials\n\n        body, ct = self._build_multipart(data)\n\n        msg = \"Submitting documentation to %s\" % (self.repository)\n        self.announce(msg, log.INFO)\n\n        # build the Request\n        # We can't use urllib2 since we need to send the Basic\n        # auth right with the first request\n        schema, netloc, url, params, query, fragments = \\\n            urllib.parse.urlparse(self.repository)\n        assert not params and not query and not fragments\n        if schema == 'http':\n            conn = http_client.HTTPConnection(netloc)\n        elif schema == 'https':\n            conn = http_client.HTTPSConnection(netloc)\n        else:\n            raise AssertionError(\"unsupported schema \" + schema)\n\n        data = ''\n        try:\n            conn.connect()\n            conn.putrequest(\"POST\", url)\n            content_type = ct\n            conn.putheader('Content-type', content_type)\n            conn.putheader('Content-length', str(len(body)))\n            conn.putheader('Authorization', auth)\n            conn.endheaders()\n            conn.send(body)\n        except socket.error as e:\n            self.announce(str(e), log.ERROR)\n            return\n\n        r = conn.getresponse()\n        if r.status == 200:\n            msg = 'Server response (%s): %s' % (r.status, r.reason)\n            self.announce(msg, log.INFO)\n        elif r.status == 301:\n            location = r.getheader('Location')\n            if location is None:\n                location = 'https://pythonhosted.org/%s/' % meta.get_name()\n            msg = 'Upload successful. Visit %s' % location\n            self.announce(msg, log.INFO)\n        else:\n            msg = 'Upload failed (%s): %s' % (r.status, r.reason)\n            self.announce(msg, log.ERROR)\n        if self.show_response:\n            print('-' * 75, r.read(), '-' * 75)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/command/upload_docs.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/command/upload_docs.py	(date 1602088701625)
@@ -15,17 +15,15 @@
 import shutil
 import itertools
 import functools
-
-from setuptools.extern import six
-from setuptools.extern.six.moves import http_client, urllib
+import http.client
+import urllib.parse
 
 from pkg_resources import iter_entry_points
 from .upload import upload
 
 
 def _encode(s):
-    errors = 'strict' if six.PY2 else 'surrogateescape'
-    return s.encode('utf-8', errors)
+    return s.encode('utf-8', 'surrogateescape')
 
 
 class upload_docs(upload):
@@ -152,9 +150,7 @@
         }
         # set up the authentication
         credentials = _encode(self.username + ':' + self.password)
-        credentials = standard_b64encode(credentials)
-        if not six.PY2:
-            credentials = credentials.decode('ascii')
+        credentials = standard_b64encode(credentials).decode('ascii')
         auth = "Basic " + credentials
 
         body, ct = self._build_multipart(data)
@@ -169,9 +165,9 @@
             urllib.parse.urlparse(self.repository)
         assert not params and not query and not fragments
         if schema == 'http':
-            conn = http_client.HTTPConnection(netloc)
+            conn = http.client.HTTPConnection(netloc)
         elif schema == 'https':
-            conn = http_client.HTTPSConnection(netloc)
+            conn = http.client.HTTPSConnection(netloc)
         else:
             raise AssertionError("unsupported schema " + schema)
 
Index: env/lib/python3.8/site-packages/setuptools/command/setopt.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from distutils.util import convert_path\nfrom distutils import log\nfrom distutils.errors import DistutilsOptionError\nimport distutils\nimport os\n\nfrom setuptools.extern.six.moves import configparser\n\nfrom setuptools import Command\n\n__all__ = ['config_file', 'edit_config', 'option_base', 'setopt']\n\n\ndef config_file(kind=\"local\"):\n    \"\"\"Get the filename of the distutils, local, global, or per-user config\n\n    `kind` must be one of \"local\", \"global\", or \"user\"\n    \"\"\"\n    if kind == 'local':\n        return 'setup.cfg'\n    if kind == 'global':\n        return os.path.join(\n            os.path.dirname(distutils.__file__), 'distutils.cfg'\n        )\n    if kind == 'user':\n        dot = os.name == 'posix' and '.' or ''\n        return os.path.expanduser(convert_path(\"~/%spydistutils.cfg\" % dot))\n    raise ValueError(\n        \"config_file() type must be 'local', 'global', or 'user'\", kind\n    )\n\n\ndef edit_config(filename, settings, dry_run=False):\n    \"\"\"Edit a configuration file to include `settings`\n\n    `settings` is a dictionary of dictionaries or ``None`` values, keyed by\n    command/section name.  A ``None`` value means to delete the entire section,\n    while a dictionary lists settings to be changed or deleted in that section.\n    A setting of ``None`` means to delete that setting.\n    \"\"\"\n    log.debug(\"Reading configuration from %s\", filename)\n    opts = configparser.RawConfigParser()\n    opts.read([filename])\n    for section, options in settings.items():\n        if options is None:\n            log.info(\"Deleting section [%s] from %s\", section, filename)\n            opts.remove_section(section)\n        else:\n            if not opts.has_section(section):\n                log.debug(\"Adding new section [%s] to %s\", section, filename)\n                opts.add_section(section)\n            for option, value in options.items():\n                if value is None:\n                    log.debug(\n                        \"Deleting %s.%s from %s\",\n                        section, option, filename\n                    )\n                    opts.remove_option(section, option)\n                    if not opts.options(section):\n                        log.info(\"Deleting empty [%s] section from %s\",\n                                 section, filename)\n                        opts.remove_section(section)\n                else:\n                    log.debug(\n                        \"Setting %s.%s to %r in %s\",\n                        section, option, value, filename\n                    )\n                    opts.set(section, option, value)\n\n    log.info(\"Writing %s\", filename)\n    if not dry_run:\n        with open(filename, 'w') as f:\n            opts.write(f)\n\n\nclass option_base(Command):\n    \"\"\"Abstract base class for commands that mess with config files\"\"\"\n\n    user_options = [\n        ('global-config', 'g',\n         \"save options to the site-wide distutils.cfg file\"),\n        ('user-config', 'u',\n         \"save options to the current user's pydistutils.cfg file\"),\n        ('filename=', 'f',\n         \"configuration file to use (default=setup.cfg)\"),\n    ]\n\n    boolean_options = [\n        'global-config', 'user-config',\n    ]\n\n    def initialize_options(self):\n        self.global_config = None\n        self.user_config = None\n        self.filename = None\n\n    def finalize_options(self):\n        filenames = []\n        if self.global_config:\n            filenames.append(config_file('global'))\n        if self.user_config:\n            filenames.append(config_file('user'))\n        if self.filename is not None:\n            filenames.append(self.filename)\n        if not filenames:\n            filenames.append(config_file('local'))\n        if len(filenames) > 1:\n            raise DistutilsOptionError(\n                \"Must specify only one configuration file option\",\n                filenames\n            )\n        self.filename, = filenames\n\n\nclass setopt(option_base):\n    \"\"\"Save command-line options to a file\"\"\"\n\n    description = \"set an option in setup.cfg or another config file\"\n\n    user_options = [\n        ('command=', 'c', 'command to set an option for'),\n        ('option=', 'o', 'option to set'),\n        ('set-value=', 's', 'value of the option'),\n        ('remove', 'r', 'remove (unset) the value'),\n    ] + option_base.user_options\n\n    boolean_options = option_base.boolean_options + ['remove']\n\n    def initialize_options(self):\n        option_base.initialize_options(self)\n        self.command = None\n        self.option = None\n        self.set_value = None\n        self.remove = None\n\n    def finalize_options(self):\n        option_base.finalize_options(self)\n        if self.command is None or self.option is None:\n            raise DistutilsOptionError(\"Must specify --command *and* --option\")\n        if self.set_value is None and not self.remove:\n            raise DistutilsOptionError(\"Must specify --set-value or --remove\")\n\n    def run(self):\n        edit_config(\n            self.filename, {\n                self.command: {self.option.replace('-', '_'): self.set_value}\n            },\n            self.dry_run\n        )\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/command/setopt.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/command/setopt.py	(date 1602088701625)
@@ -3,8 +3,7 @@
 from distutils.errors import DistutilsOptionError
 import distutils
 import os
-
-from setuptools.extern.six.moves import configparser
+import configparser
 
 from setuptools import Command
 
Index: ursina_tictactoe.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from ursina import *\n\n\napp = Ursina()\n\ncamera.orthographic = True\ncamera.fov = 4\ncamera.position = (1, 1)\nText.default_resolution *= 2\n\nplayer = Entity(name='o', color=color.azure)\ncursor = Tooltip(player.name, color=player.color,\n                 origin=(0, 0), scale=4, enabled=True)\ncursor.background.color = color.clear\nbg = Entity(parent=scene, model='quad', texture='shore',\n            scale=(16, 8), z=10, color=color.light_gray)\nmouse.visible = False\n\n# create a matrix to store the buttons in. makes it easier to check for victory\nboard = [[None for x in range(3)] for y in range(3)]\n\nfor y in range(3):\n    for x in range(3):\n        b = Button(parent=scene, position=(x, y))\n        board[x][y] = b\n\n        def on_click(b=b):\n            b.text = player.name\n            b.color = player.color\n            b.collision = False\n            check_for_victory()\n\n            if player.name == 'o':\n                player.name = 'x'\n                player.color = color.orange\n            else:\n                player.name = 'o'\n                player.color = color.azure\n\n            cursor.text = player.name\n            cursor.color = player.color\n\n        b.on_click = on_click\n\n\ndef check_for_victory():\n    name = player.name\n\n    won = (\n        # across the bottom\n        (board[0][0].text == name and board[1][0].text == name and board[2][0].text == name) or\n        # across the middle\n        (board[0][1].text == name and board[1][1].text == name and board[2][1].text == name) or\n        # across the top\n        (board[0][2].text == name and board[1][2].text == name and board[2][2].text == name) or\n        # down the left side\n        (board[0][0].text == name and board[0][1].text == name and board[0][2].text == name) or\n        # down the middle\n        (board[1][0].text == name and board[1][1].text == name and board[1][2].text == name) or\n        # down the right side\n        (board[2][0].text == name and board[2][1].text == name and board[2][2].text == name) or\n        # diagonal /\n        (board[0][0].text == name and board[1][1].text == name and board[2][2].text == name) or\n        (board[0][2].text == name and board[1][1].text == name and board[2][0].text == name))   # diagonal \\\n\n    if won:\n        print('winner is:', name)\n        destroy(cursor)\n        mouse.visible = True\n        Panel(z=1, scale=10, model='quad')\n        t = Text(f'player\\n{name}\\nwon!', scale=3,\n                 origin=(0, 0), background=True)\n        t.create_background(padding=(.5, .25), radius=Text.size/2)\n        t.background.color = player.color.tint(-.2)\n\n\napp.run()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- ursina_tictactoe.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ ursina_tictactoe.py	(date 1602089186984)
@@ -9,11 +9,15 @@
 Text.default_resolution *= 2
 
 player = Entity(name='o', color=color.azure)
+
 cursor = Tooltip(player.name, color=player.color,
                  origin=(0, 0), scale=4, enabled=True)
+
 cursor.background.color = color.clear
+
 bg = Entity(parent=scene, model='quad', texture='shore',
             scale=(16, 8), z=10, color=color.light_gray)
+
 mouse.visible = False
 
 # create a matrix to store the buttons in. makes it easier to check for victory
Index: env/lib/python3.8/site-packages/pip/_internal/utils/misc.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># The following comment should be removed at some point in the future.\n# mypy: strict-optional=False\n# mypy: disallow-untyped-defs=False\n\nfrom __future__ import absolute_import\n\nimport contextlib\nimport errno\nimport getpass\nimport hashlib\nimport io\nimport logging\nimport os\nimport posixpath\nimport shutil\nimport stat\nimport sys\nfrom collections import deque\nfrom itertools import tee\n\nfrom pip._vendor import pkg_resources\nfrom pip._vendor.packaging.utils import canonicalize_name\n# NOTE: retrying is not annotated in typeshed as on 2017-07-17, which is\n#       why we ignore the type on this import.\nfrom pip._vendor.retrying import retry  # type: ignore\nfrom pip._vendor.six import PY2, text_type\nfrom pip._vendor.six.moves import filter, filterfalse, input, map, zip_longest\nfrom pip._vendor.six.moves.urllib import parse as urllib_parse\nfrom pip._vendor.six.moves.urllib.parse import unquote as urllib_unquote\n\nfrom pip import __version__\nfrom pip._internal.exceptions import CommandError\nfrom pip._internal.locations import (\n    get_major_minor_version,\n    site_packages,\n    user_site,\n)\nfrom pip._internal.utils.compat import (\n    WINDOWS,\n    expanduser,\n    stdlib_pkgs,\n    str_to_display,\n)\nfrom pip._internal.utils.typing import MYPY_CHECK_RUNNING, cast\nfrom pip._internal.utils.virtualenv import (\n    running_under_virtualenv,\n    virtualenv_no_global,\n)\n\nif PY2:\n    from io import BytesIO as StringIO\nelse:\n    from io import StringIO\n\nif MYPY_CHECK_RUNNING:\n    from typing import (\n        Any, AnyStr, Callable, Container, Iterable, Iterator, List, Optional,\n        Text, Tuple, TypeVar, Union,\n    )\n    from pip._vendor.pkg_resources import Distribution\n\n    VersionInfo = Tuple[int, int, int]\n    T = TypeVar(\"T\")\n\n\n__all__ = ['rmtree', 'display_path', 'backup_dir',\n           'ask', 'splitext',\n           'format_size', 'is_installable_dir',\n           'normalize_path',\n           'renames', 'get_prog',\n           'captured_stdout', 'ensure_dir',\n           'get_installed_version', 'remove_auth_from_url']\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_pip_version():\n    # type: () -> str\n    pip_pkg_dir = os.path.join(os.path.dirname(__file__), \"..\", \"..\")\n    pip_pkg_dir = os.path.abspath(pip_pkg_dir)\n\n    return (\n        'pip {} from {} (python {})'.format(\n            __version__, pip_pkg_dir, get_major_minor_version(),\n        )\n    )\n\n\ndef normalize_version_info(py_version_info):\n    # type: (Tuple[int, ...]) -> Tuple[int, int, int]\n    \"\"\"\n    Convert a tuple of ints representing a Python version to one of length\n    three.\n\n    :param py_version_info: a tuple of ints representing a Python version,\n        or None to specify no version. The tuple can have any length.\n\n    :return: a tuple of length three if `py_version_info` is non-None.\n        Otherwise, return `py_version_info` unchanged (i.e. None).\n    \"\"\"\n    if len(py_version_info) < 3:\n        py_version_info += (3 - len(py_version_info)) * (0,)\n    elif len(py_version_info) > 3:\n        py_version_info = py_version_info[:3]\n\n    return cast('VersionInfo', py_version_info)\n\n\ndef ensure_dir(path):\n    # type: (AnyStr) -> None\n    \"\"\"os.path.makedirs without EEXIST.\"\"\"\n    try:\n        os.makedirs(path)\n    except OSError as e:\n        # Windows can raise spurious ENOTEMPTY errors. See #6426.\n        if e.errno != errno.EEXIST and e.errno != errno.ENOTEMPTY:\n            raise\n\n\ndef get_prog():\n    # type: () -> str\n    try:\n        prog = os.path.basename(sys.argv[0])\n        if prog in ('__main__.py', '-c'):\n            return \"{} -m pip\".format(sys.executable)\n        else:\n            return prog\n    except (AttributeError, TypeError, IndexError):\n        pass\n    return 'pip'\n\n\n# Retry every half second for up to 3 seconds\n@retry(stop_max_delay=3000, wait_fixed=500)\ndef rmtree(dir, ignore_errors=False):\n    # type: (Text, bool) -> None\n    shutil.rmtree(dir, ignore_errors=ignore_errors,\n                  onerror=rmtree_errorhandler)\n\n\ndef rmtree_errorhandler(func, path, exc_info):\n    \"\"\"On Windows, the files in .svn are read-only, so when rmtree() tries to\n    remove them, an exception is thrown.  We catch that here, remove the\n    read-only attribute, and hopefully continue without problems.\"\"\"\n    try:\n        has_attr_readonly = not (os.stat(path).st_mode & stat.S_IWRITE)\n    except (IOError, OSError):\n        # it's equivalent to os.path.exists\n        return\n\n    if has_attr_readonly:\n        # convert to read/write\n        os.chmod(path, stat.S_IWRITE)\n        # use the original function to repeat the operation\n        func(path)\n        return\n    else:\n        raise\n\n\ndef path_to_display(path):\n    # type: (Optional[Union[str, Text]]) -> Optional[Text]\n    \"\"\"\n    Convert a bytes (or text) path to text (unicode in Python 2) for display\n    and logging purposes.\n\n    This function should never error out. Also, this function is mainly needed\n    for Python 2 since in Python 3 str paths are already text.\n    \"\"\"\n    if path is None:\n        return None\n    if isinstance(path, text_type):\n        return path\n    # Otherwise, path is a bytes object (str in Python 2).\n    try:\n        display_path = path.decode(sys.getfilesystemencoding(), 'strict')\n    except UnicodeDecodeError:\n        # Include the full bytes to make troubleshooting easier, even though\n        # it may not be very human readable.\n        if PY2:\n            # Convert the bytes to a readable str representation using\n            # repr(), and then convert the str to unicode.\n            #   Also, we add the prefix \"b\" to the repr() return value both\n            # to make the Python 2 output look like the Python 3 output, and\n            # to signal to the user that this is a bytes representation.\n            display_path = str_to_display('b{!r}'.format(path))\n        else:\n            # Silence the \"F821 undefined name 'ascii'\" flake8 error since\n            # in Python 3 ascii() is a built-in.\n            display_path = ascii(path)  # noqa: F821\n\n    return display_path\n\n\ndef display_path(path):\n    # type: (Union[str, Text]) -> str\n    \"\"\"Gives the display value for a given path, making it relative to cwd\n    if possible.\"\"\"\n    path = os.path.normcase(os.path.abspath(path))\n    if sys.version_info[0] == 2:\n        path = path.decode(sys.getfilesystemencoding(), 'replace')\n        path = path.encode(sys.getdefaultencoding(), 'replace')\n    if path.startswith(os.getcwd() + os.path.sep):\n        path = '.' + path[len(os.getcwd()):]\n    return path\n\n\ndef backup_dir(dir, ext='.bak'):\n    # type: (str, str) -> str\n    \"\"\"Figure out the name of a directory to back up the given dir to\n    (adding .bak, .bak2, etc)\"\"\"\n    n = 1\n    extension = ext\n    while os.path.exists(dir + extension):\n        n += 1\n        extension = ext + str(n)\n    return dir + extension\n\n\ndef ask_path_exists(message, options):\n    # type: (str, Iterable[str]) -> str\n    for action in os.environ.get('PIP_EXISTS_ACTION', '').split():\n        if action in options:\n            return action\n    return ask(message, options)\n\n\ndef _check_no_input(message):\n    # type: (str) -> None\n    \"\"\"Raise an error if no input is allowed.\"\"\"\n    if os.environ.get('PIP_NO_INPUT'):\n        raise Exception(\n            'No input was expected ($PIP_NO_INPUT set); question: {}'.format(\n                message)\n        )\n\n\ndef ask(message, options):\n    # type: (str, Iterable[str]) -> str\n    \"\"\"Ask the message interactively, with the given possible responses\"\"\"\n    while 1:\n        _check_no_input(message)\n        response = input(message)\n        response = response.strip().lower()\n        if response not in options:\n            print(\n                'Your response ({!r}) was not one of the expected responses: '\n                '{}'.format(response, ', '.join(options))\n            )\n        else:\n            return response\n\n\ndef ask_input(message):\n    # type: (str) -> str\n    \"\"\"Ask for input interactively.\"\"\"\n    _check_no_input(message)\n    return input(message)\n\n\ndef ask_password(message):\n    # type: (str) -> str\n    \"\"\"Ask for a password interactively.\"\"\"\n    _check_no_input(message)\n    return getpass.getpass(message)\n\n\ndef format_size(bytes):\n    # type: (float) -> str\n    if bytes > 1000 * 1000:\n        return '{:.1f} MB'.format(bytes / 1000.0 / 1000)\n    elif bytes > 10 * 1000:\n        return '{} kB'.format(int(bytes / 1000))\n    elif bytes > 1000:\n        return '{:.1f} kB'.format(bytes / 1000.0)\n    else:\n        return '{} bytes'.format(int(bytes))\n\n\ndef tabulate(rows):\n    # type: (Iterable[Iterable[Any]]) -> Tuple[List[str], List[int]]\n    \"\"\"Return a list of formatted rows and a list of column sizes.\n\n    For example::\n\n    >>> tabulate([['foobar', 2000], [0xdeadbeef]])\n    (['foobar     2000', '3735928559'], [10, 4])\n    \"\"\"\n    rows = [tuple(map(str, row)) for row in rows]\n    sizes = [max(map(len, col)) for col in zip_longest(*rows, fillvalue='')]\n    table = [\" \".join(map(str.ljust, row, sizes)).rstrip() for row in rows]\n    return table, sizes\n\n\ndef is_installable_dir(path):\n    # type: (str) -> bool\n    \"\"\"Is path is a directory containing setup.py or pyproject.toml?\n    \"\"\"\n    if not os.path.isdir(path):\n        return False\n    setup_py = os.path.join(path, 'setup.py')\n    if os.path.isfile(setup_py):\n        return True\n    pyproject_toml = os.path.join(path, 'pyproject.toml')\n    if os.path.isfile(pyproject_toml):\n        return True\n    return False\n\n\ndef read_chunks(file, size=io.DEFAULT_BUFFER_SIZE):\n    \"\"\"Yield pieces of data from a file-like object until EOF.\"\"\"\n    while True:\n        chunk = file.read(size)\n        if not chunk:\n            break\n        yield chunk\n\n\ndef normalize_path(path, resolve_symlinks=True):\n    # type: (str, bool) -> str\n    \"\"\"\n    Convert a path to its canonical, case-normalized, absolute version.\n\n    \"\"\"\n    path = expanduser(path)\n    if resolve_symlinks:\n        path = os.path.realpath(path)\n    else:\n        path = os.path.abspath(path)\n    return os.path.normcase(path)\n\n\ndef splitext(path):\n    # type: (str) -> Tuple[str, str]\n    \"\"\"Like os.path.splitext, but take off .tar too\"\"\"\n    base, ext = posixpath.splitext(path)\n    if base.lower().endswith('.tar'):\n        ext = base[-4:] + ext\n        base = base[:-4]\n    return base, ext\n\n\ndef renames(old, new):\n    # type: (str, str) -> None\n    \"\"\"Like os.renames(), but handles renaming across devices.\"\"\"\n    # Implementation borrowed from os.renames().\n    head, tail = os.path.split(new)\n    if head and tail and not os.path.exists(head):\n        os.makedirs(head)\n\n    shutil.move(old, new)\n\n    head, tail = os.path.split(old)\n    if head and tail:\n        try:\n            os.removedirs(head)\n        except OSError:\n            pass\n\n\ndef is_local(path):\n    # type: (str) -> bool\n    \"\"\"\n    Return True if path is within sys.prefix, if we're running in a virtualenv.\n\n    If we're not in a virtualenv, all paths are considered \"local.\"\n\n    Caution: this function assumes the head of path has been normalized\n    with normalize_path.\n    \"\"\"\n    if not running_under_virtualenv():\n        return True\n    return path.startswith(normalize_path(sys.prefix))\n\n\ndef dist_is_local(dist):\n    # type: (Distribution) -> bool\n    \"\"\"\n    Return True if given Distribution object is installed locally\n    (i.e. within current virtualenv).\n\n    Always True if we're not in a virtualenv.\n\n    \"\"\"\n    return is_local(dist_location(dist))\n\n\ndef dist_in_usersite(dist):\n    # type: (Distribution) -> bool\n    \"\"\"\n    Return True if given Distribution is installed in user site.\n    \"\"\"\n    return dist_location(dist).startswith(normalize_path(user_site))\n\n\ndef dist_in_site_packages(dist):\n    # type: (Distribution) -> bool\n    \"\"\"\n    Return True if given Distribution is installed in\n    sysconfig.get_python_lib().\n    \"\"\"\n    return dist_location(dist).startswith(normalize_path(site_packages))\n\n\ndef dist_is_editable(dist):\n    # type: (Distribution) -> bool\n    \"\"\"\n    Return True if given Distribution is an editable install.\n    \"\"\"\n    for path_item in sys.path:\n        egg_link = os.path.join(path_item, dist.project_name + '.egg-link')\n        if os.path.isfile(egg_link):\n            return True\n    return False\n\n\ndef get_installed_distributions(\n        local_only=True,  # type: bool\n        skip=stdlib_pkgs,  # type: Container[str]\n        include_editables=True,  # type: bool\n        editables_only=False,  # type: bool\n        user_only=False,  # type: bool\n        paths=None  # type: Optional[List[str]]\n):\n    # type: (...) -> List[Distribution]\n    \"\"\"\n    Return a list of installed Distribution objects.\n\n    If ``local_only`` is True (default), only return installations\n    local to the current virtualenv, if in a virtualenv.\n\n    ``skip`` argument is an iterable of lower-case project names to\n    ignore; defaults to stdlib_pkgs\n\n    If ``include_editables`` is False, don't report editables.\n\n    If ``editables_only`` is True , only report editables.\n\n    If ``user_only`` is True , only report installations in the user\n    site directory.\n\n    If ``paths`` is set, only report the distributions present at the\n    specified list of locations.\n    \"\"\"\n    if paths:\n        working_set = pkg_resources.WorkingSet(paths)\n    else:\n        working_set = pkg_resources.working_set\n\n    if local_only:\n        local_test = dist_is_local\n    else:\n        def local_test(d):\n            return True\n\n    if include_editables:\n        def editable_test(d):\n            return True\n    else:\n        def editable_test(d):\n            return not dist_is_editable(d)\n\n    if editables_only:\n        def editables_only_test(d):\n            return dist_is_editable(d)\n    else:\n        def editables_only_test(d):\n            return True\n\n    if user_only:\n        user_test = dist_in_usersite\n    else:\n        def user_test(d):\n            return True\n\n    return [d for d in working_set\n            if local_test(d) and\n            d.key not in skip and\n            editable_test(d) and\n            editables_only_test(d) and\n            user_test(d)\n            ]\n\n\ndef search_distribution(req_name):\n\n    # Canonicalize the name before searching in the list of\n    # installed distributions and also while creating the package\n    # dictionary to get the Distribution object\n    req_name = canonicalize_name(req_name)\n    packages = get_installed_distributions(skip=())\n    pkg_dict = {canonicalize_name(p.key): p for p in packages}\n    return pkg_dict.get(req_name)\n\n\ndef get_distribution(req_name):\n    \"\"\"Given a requirement name, return the installed Distribution object\"\"\"\n\n    # Search the distribution by looking through the working set\n    dist = search_distribution(req_name)\n\n    # If distribution could not be found, call working_set.require\n    # to update the working set, and try to find the distribution\n    # again.\n    # This might happen for e.g. when you install a package\n    # twice, once using setup.py develop and again using setup.py install.\n    # Now when run pip uninstall twice, the package gets removed\n    # from the working set in the first uninstall, so we have to populate\n    # the working set again so that pip knows about it and the packages\n    # gets picked up and is successfully uninstalled the second time too.\n    if not dist:\n        try:\n            pkg_resources.working_set.require(req_name)\n        except pkg_resources.DistributionNotFound:\n            return None\n    return search_distribution(req_name)\n\n\ndef egg_link_path(dist):\n    # type: (Distribution) -> Optional[str]\n    \"\"\"\n    Return the path for the .egg-link file if it exists, otherwise, None.\n\n    There's 3 scenarios:\n    1) not in a virtualenv\n       try to find in site.USER_SITE, then site_packages\n    2) in a no-global virtualenv\n       try to find in site_packages\n    3) in a yes-global virtualenv\n       try to find in site_packages, then site.USER_SITE\n       (don't look in global location)\n\n    For #1 and #3, there could be odd cases, where there's an egg-link in 2\n    locations.\n\n    This method will just return the first one found.\n    \"\"\"\n    sites = []\n    if running_under_virtualenv():\n        sites.append(site_packages)\n        if not virtualenv_no_global() and user_site:\n            sites.append(user_site)\n    else:\n        if user_site:\n            sites.append(user_site)\n        sites.append(site_packages)\n\n    for site in sites:\n        egglink = os.path.join(site, dist.project_name) + '.egg-link'\n        if os.path.isfile(egglink):\n            return egglink\n    return None\n\n\ndef dist_location(dist):\n    # type: (Distribution) -> str\n    \"\"\"\n    Get the site-packages location of this distribution. Generally\n    this is dist.location, except in the case of develop-installed\n    packages, where dist.location is the source code location, and we\n    want to know where the egg-link file is.\n\n    The returned location is normalized (in particular, with symlinks removed).\n    \"\"\"\n    egg_link = egg_link_path(dist)\n    if egg_link:\n        return normalize_path(egg_link)\n    return normalize_path(dist.location)\n\n\ndef write_output(msg, *args):\n    # type: (Any, Any) -> None\n    logger.info(msg, *args)\n\n\nclass FakeFile(object):\n    \"\"\"Wrap a list of lines in an object with readline() to make\n    ConfigParser happy.\"\"\"\n    def __init__(self, lines):\n        self._gen = iter(lines)\n\n    def readline(self):\n        try:\n            return next(self._gen)\n        except StopIteration:\n            return ''\n\n    def __iter__(self):\n        return self._gen\n\n\nclass StreamWrapper(StringIO):\n\n    @classmethod\n    def from_stream(cls, orig_stream):\n        cls.orig_stream = orig_stream\n        return cls()\n\n    # compileall.compile_dir() needs stdout.encoding to print to stdout\n    @property\n    def encoding(self):\n        return self.orig_stream.encoding\n\n\n@contextlib.contextmanager\ndef captured_output(stream_name):\n    \"\"\"Return a context manager used by captured_stdout/stdin/stderr\n    that temporarily replaces the sys stream *stream_name* with a StringIO.\n\n    Taken from Lib/support/__init__.py in the CPython repo.\n    \"\"\"\n    orig_stdout = getattr(sys, stream_name)\n    setattr(sys, stream_name, StreamWrapper.from_stream(orig_stdout))\n    try:\n        yield getattr(sys, stream_name)\n    finally:\n        setattr(sys, stream_name, orig_stdout)\n\n\ndef captured_stdout():\n    \"\"\"Capture the output of sys.stdout:\n\n       with captured_stdout() as stdout:\n           print('hello')\n       self.assertEqual(stdout.getvalue(), 'hello\\n')\n\n    Taken from Lib/support/__init__.py in the CPython repo.\n    \"\"\"\n    return captured_output('stdout')\n\n\ndef captured_stderr():\n    \"\"\"\n    See captured_stdout().\n    \"\"\"\n    return captured_output('stderr')\n\n\ndef get_installed_version(dist_name, working_set=None):\n    \"\"\"Get the installed version of dist_name avoiding pkg_resources cache\"\"\"\n    # Create a requirement that we'll look for inside of setuptools.\n    req = pkg_resources.Requirement.parse(dist_name)\n\n    if working_set is None:\n        # We want to avoid having this cached, so we need to construct a new\n        # working set each time.\n        working_set = pkg_resources.WorkingSet()\n\n    # Get the installed distribution from our working set\n    dist = working_set.find(req)\n\n    # Check to see if we got an installed distribution or not, if we did\n    # we want to return it's version.\n    return dist.version if dist else None\n\n\ndef consume(iterator):\n    \"\"\"Consume an iterable at C speed.\"\"\"\n    deque(iterator, maxlen=0)\n\n\n# Simulates an enum\ndef enum(*sequential, **named):\n    enums = dict(zip(sequential, range(len(sequential))), **named)\n    reverse = {value: key for key, value in enums.items()}\n    enums['reverse_mapping'] = reverse\n    return type('Enum', (), enums)\n\n\ndef build_netloc(host, port):\n    # type: (str, Optional[int]) -> str\n    \"\"\"\n    Build a netloc from a host-port pair\n    \"\"\"\n    if port is None:\n        return host\n    if ':' in host:\n        # Only wrap host with square brackets when it is IPv6\n        host = '[{}]'.format(host)\n    return '{}:{}'.format(host, port)\n\n\ndef build_url_from_netloc(netloc, scheme='https'):\n    # type: (str, str) -> str\n    \"\"\"\n    Build a full URL from a netloc.\n    \"\"\"\n    if netloc.count(':') >= 2 and '@' not in netloc and '[' not in netloc:\n        # It must be a bare IPv6 address, so wrap it with brackets.\n        netloc = '[{}]'.format(netloc)\n    return '{}://{}'.format(scheme, netloc)\n\n\ndef parse_netloc(netloc):\n    # type: (str) -> Tuple[str, Optional[int]]\n    \"\"\"\n    Return the host-port pair from a netloc.\n    \"\"\"\n    url = build_url_from_netloc(netloc)\n    parsed = urllib_parse.urlparse(url)\n    return parsed.hostname, parsed.port\n\n\ndef split_auth_from_netloc(netloc):\n    \"\"\"\n    Parse out and remove the auth information from a netloc.\n\n    Returns: (netloc, (username, password)).\n    \"\"\"\n    if '@' not in netloc:\n        return netloc, (None, None)\n\n    # Split from the right because that's how urllib.parse.urlsplit()\n    # behaves if more than one @ is present (which can be checked using\n    # the password attribute of urlsplit()'s return value).\n    auth, netloc = netloc.rsplit('@', 1)\n    if ':' in auth:\n        # Split from the left because that's how urllib.parse.urlsplit()\n        # behaves if more than one : is present (which again can be checked\n        # using the password attribute of the return value)\n        user_pass = auth.split(':', 1)\n    else:\n        user_pass = auth, None\n\n    user_pass = tuple(\n        None if x is None else urllib_unquote(x) for x in user_pass\n    )\n\n    return netloc, user_pass\n\n\ndef redact_netloc(netloc):\n    # type: (str) -> str\n    \"\"\"\n    Replace the sensitive data in a netloc with \"****\", if it exists.\n\n    For example:\n        - \"user:pass@example.com\" returns \"user:****@example.com\"\n        - \"accesstoken@example.com\" returns \"****@example.com\"\n    \"\"\"\n    netloc, (user, password) = split_auth_from_netloc(netloc)\n    if user is None:\n        return netloc\n    if password is None:\n        user = '****'\n        password = ''\n    else:\n        user = urllib_parse.quote(user)\n        password = ':****'\n    return '{user}{password}@{netloc}'.format(user=user,\n                                              password=password,\n                                              netloc=netloc)\n\n\ndef _transform_url(url, transform_netloc):\n    \"\"\"Transform and replace netloc in a url.\n\n    transform_netloc is a function taking the netloc and returning a\n    tuple. The first element of this tuple is the new netloc. The\n    entire tuple is returned.\n\n    Returns a tuple containing the transformed url as item 0 and the\n    original tuple returned by transform_netloc as item 1.\n    \"\"\"\n    purl = urllib_parse.urlsplit(url)\n    netloc_tuple = transform_netloc(purl.netloc)\n    # stripped url\n    url_pieces = (\n        purl.scheme, netloc_tuple[0], purl.path, purl.query, purl.fragment\n    )\n    surl = urllib_parse.urlunsplit(url_pieces)\n    return surl, netloc_tuple\n\n\ndef _get_netloc(netloc):\n    return split_auth_from_netloc(netloc)\n\n\ndef _redact_netloc(netloc):\n    return (redact_netloc(netloc),)\n\n\ndef split_auth_netloc_from_url(url):\n    # type: (str) -> Tuple[str, str, Tuple[str, str]]\n    \"\"\"\n    Parse a url into separate netloc, auth, and url with no auth.\n\n    Returns: (url_without_auth, netloc, (username, password))\n    \"\"\"\n    url_without_auth, (netloc, auth) = _transform_url(url, _get_netloc)\n    return url_without_auth, netloc, auth\n\n\ndef remove_auth_from_url(url):\n    # type: (str) -> str\n    \"\"\"Return a copy of url with 'username:password@' removed.\"\"\"\n    # username/pass params are passed to subversion through flags\n    # and are not recognized in the url.\n    return _transform_url(url, _get_netloc)[0]\n\n\ndef redact_auth_from_url(url):\n    # type: (str) -> str\n    \"\"\"Replace the password in a given url with ****.\"\"\"\n    return _transform_url(url, _redact_netloc)[0]\n\n\nclass HiddenText(object):\n    def __init__(\n        self,\n        secret,    # type: str\n        redacted,  # type: str\n    ):\n        # type: (...) -> None\n        self.secret = secret\n        self.redacted = redacted\n\n    def __repr__(self):\n        # type: (...) -> str\n        return '<HiddenText {!r}>'.format(str(self))\n\n    def __str__(self):\n        # type: (...) -> str\n        return self.redacted\n\n    # This is useful for testing.\n    def __eq__(self, other):\n        # type: (Any) -> bool\n        if type(self) != type(other):\n            return False\n\n        # The string being used for redaction doesn't also have to match,\n        # just the raw, original string.\n        return (self.secret == other.secret)\n\n    # We need to provide an explicit __ne__ implementation for Python 2.\n    # TODO: remove this when we drop PY2 support.\n    def __ne__(self, other):\n        # type: (Any) -> bool\n        return not self == other\n\n\ndef hide_value(value):\n    # type: (str) -> HiddenText\n    return HiddenText(value, redacted='****')\n\n\ndef hide_url(url):\n    # type: (str) -> HiddenText\n    redacted = redact_auth_from_url(url)\n    return HiddenText(url, redacted=redacted)\n\n\ndef protect_pip_from_modification_on_windows(modifying_pip):\n    # type: (bool) -> None\n    \"\"\"Protection of pip.exe from modification on Windows\n\n    On Windows, any operation modifying pip should be run as:\n        python -m pip ...\n    \"\"\"\n    pip_names = [\n        \"pip.exe\",\n        \"pip{}.exe\".format(sys.version_info[0]),\n        \"pip{}.{}.exe\".format(*sys.version_info[:2])\n    ]\n\n    # See https://github.com/pypa/pip/issues/1299 for more discussion\n    should_show_use_python_msg = (\n        modifying_pip and\n        WINDOWS and\n        os.path.basename(sys.argv[0]) in pip_names\n    )\n\n    if should_show_use_python_msg:\n        new_command = [\n            sys.executable, \"-m\", \"pip\"\n        ] + sys.argv[1:]\n        raise CommandError(\n            'To modify pip, please run the following command:\\n{}'\n            .format(\" \".join(new_command))\n        )\n\n\ndef is_console_interactive():\n    # type: () -> bool\n    \"\"\"Is this console interactive?\n    \"\"\"\n    return sys.stdin is not None and sys.stdin.isatty()\n\n\ndef hash_file(path, blocksize=1 << 20):\n    # type: (Text, int) -> Tuple[Any, int]\n    \"\"\"Return (hash, length) for path using hashlib.sha256()\n    \"\"\"\n\n    h = hashlib.sha256()\n    length = 0\n    with open(path, 'rb') as f:\n        for block in read_chunks(f, size=blocksize):\n            length += len(block)\n            h.update(block)\n    return h, length\n\n\ndef is_wheel_installed():\n    \"\"\"\n    Return whether the wheel package is installed.\n    \"\"\"\n    try:\n        import wheel  # noqa: F401\n    except ImportError:\n        return False\n\n    return True\n\n\ndef pairwise(iterable):\n    # type: (Iterable[Any]) -> Iterator[Tuple[Any, Any]]\n    \"\"\"\n    Return paired elements.\n\n    For example:\n        s -> (s0, s1), (s2, s3), (s4, s5), ...\n    \"\"\"\n    iterable = iter(iterable)\n    return zip_longest(iterable, iterable)\n\n\ndef partition(\n    pred,  # type: Callable[[T], bool]\n    iterable,  # type: Iterable[T]\n):\n    # type: (...) -> Tuple[Iterable[T], Iterable[T]]\n    \"\"\"\n    Use a predicate to partition entries into false entries and true entries,\n    like\n\n        partition(is_odd, range(10)) --> 0 2 4 6 8   and  1 3 5 7 9\n    \"\"\"\n    t1, t2 = tee(iterable)\n    return filterfalse(pred, t1), filter(pred, t2)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pip/_internal/utils/misc.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pip/_internal/utils/misc.py	(date 1602088700445)
@@ -483,22 +483,39 @@
             ]
 
 
-def search_distribution(req_name):
+def _search_distribution(req_name):
+    # type: (str) -> Optional[Distribution]
+    """Find a distribution matching the ``req_name`` in the environment.
 
+    This searches from *all* distributions available in the environment, to
+    match the behavior of ``pkg_resources.get_distribution()``.
+    """
     # Canonicalize the name before searching in the list of
     # installed distributions and also while creating the package
     # dictionary to get the Distribution object
     req_name = canonicalize_name(req_name)
-    packages = get_installed_distributions(skip=())
+    packages = get_installed_distributions(
+        local_only=False,
+        skip=(),
+        include_editables=True,
+        editables_only=False,
+        user_only=False,
+        paths=None,
+    )
     pkg_dict = {canonicalize_name(p.key): p for p in packages}
     return pkg_dict.get(req_name)
 
 
 def get_distribution(req_name):
-    """Given a requirement name, return the installed Distribution object"""
+    # type: (str) -> Optional[Distribution]
+    """Given a requirement name, return the installed Distribution object.
+
+    This searches from *all* distributions available in the environment, to
+    match the behavior of ``pkg_resources.get_distribution()``.
+    """
 
     # Search the distribution by looking through the working set
-    dist = search_distribution(req_name)
+    dist = _search_distribution(req_name)
 
     # If distribution could not be found, call working_set.require
     # to update the working set, and try to find the distribution
@@ -514,7 +531,7 @@
             pkg_resources.working_set.require(req_name)
         except pkg_resources.DistributionNotFound:
             return None
-    return search_distribution(req_name)
+    return _search_distribution(req_name)
 
 
 def egg_link_path(dist):
Index: env/lib/python3.8/site-packages/setuptools/extern/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import sys\n\n\nclass VendorImporter:\n    \"\"\"\n    A PEP 302 meta path importer for finding optionally-vendored\n    or otherwise naturally-installed packages from root_name.\n    \"\"\"\n\n    def __init__(self, root_name, vendored_names=(), vendor_pkg=None):\n        self.root_name = root_name\n        self.vendored_names = set(vendored_names)\n        self.vendor_pkg = vendor_pkg or root_name.replace('extern', '_vendor')\n\n    @property\n    def search_path(self):\n        \"\"\"\n        Search first the vendor package then as a natural package.\n        \"\"\"\n        yield self.vendor_pkg + '.'\n        yield ''\n\n    def find_module(self, fullname, path=None):\n        \"\"\"\n        Return self when fullname starts with root_name and the\n        target module is one vendored through this importer.\n        \"\"\"\n        root, base, target = fullname.partition(self.root_name + '.')\n        if root:\n            return\n        if not any(map(target.startswith, self.vendored_names)):\n            return\n        return self\n\n    def load_module(self, fullname):\n        \"\"\"\n        Iterate over the search path to locate and load fullname.\n        \"\"\"\n        root, base, target = fullname.partition(self.root_name + '.')\n        for prefix in self.search_path:\n            try:\n                extant = prefix + target\n                __import__(extant)\n                mod = sys.modules[extant]\n                sys.modules[fullname] = mod\n                return mod\n            except ImportError:\n                pass\n        else:\n            raise ImportError(\n                \"The '{target}' package is required; \"\n                \"normally this is bundled with this package so if you get \"\n                \"this warning, consult the packager of your \"\n                \"distribution.\".format(**locals())\n            )\n\n    def install(self):\n        \"\"\"\n        Install this importer into sys.meta_path if not already present.\n        \"\"\"\n        if self not in sys.meta_path:\n            sys.meta_path.append(self)\n\n\nnames = 'six', 'packaging', 'pyparsing', 'ordered_set',\nVendorImporter(__name__, names, 'setuptools._vendor').install()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/extern/__init__.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/extern/__init__.py	(date 1602088701625)
@@ -62,5 +62,5 @@
             sys.meta_path.append(self)
 
 
-names = 'six', 'packaging', 'pyparsing', 'ordered_set',
+names = 'packaging', 'pyparsing', 'ordered_set',
 VendorImporter(__name__, names, 'setuptools._vendor').install()
Index: env/lib/python3.8/site-packages/setuptools/ssl_support.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport socket\nimport atexit\nimport re\nimport functools\n\nfrom setuptools.extern.six.moves import urllib, http_client, map, filter\n\nfrom pkg_resources import ResolutionError, ExtractionError\n\ntry:\n    import ssl\nexcept ImportError:\n    ssl = None\n\n__all__ = [\n    'VerifyingHTTPSHandler', 'find_ca_bundle', 'is_available', 'cert_paths',\n    'opener_for'\n]\n\ncert_paths = \"\"\"\n/etc/pki/tls/certs/ca-bundle.crt\n/etc/ssl/certs/ca-certificates.crt\n/usr/share/ssl/certs/ca-bundle.crt\n/usr/local/share/certs/ca-root.crt\n/etc/ssl/cert.pem\n/System/Library/OpenSSL/certs/cert.pem\n/usr/local/share/certs/ca-root-nss.crt\n/etc/ssl/ca-bundle.pem\n\"\"\".strip().split()\n\ntry:\n    HTTPSHandler = urllib.request.HTTPSHandler\n    HTTPSConnection = http_client.HTTPSConnection\nexcept AttributeError:\n    HTTPSHandler = HTTPSConnection = object\n\nis_available = ssl is not None and object not in (\n    HTTPSHandler, HTTPSConnection)\n\n\ntry:\n    from ssl import CertificateError, match_hostname\nexcept ImportError:\n    try:\n        from backports.ssl_match_hostname import CertificateError\n        from backports.ssl_match_hostname import match_hostname\n    except ImportError:\n        CertificateError = None\n        match_hostname = None\n\nif not CertificateError:\n\n    class CertificateError(ValueError):\n        pass\n\n\nif not match_hostname:\n\n    def _dnsname_match(dn, hostname, max_wildcards=1):\n        \"\"\"Matching according to RFC 6125, section 6.4.3\n\n        https://tools.ietf.org/html/rfc6125#section-6.4.3\n        \"\"\"\n        pats = []\n        if not dn:\n            return False\n\n        # Ported from python3-syntax:\n        # leftmost, *remainder = dn.split(r'.')\n        parts = dn.split(r'.')\n        leftmost = parts[0]\n        remainder = parts[1:]\n\n        wildcards = leftmost.count('*')\n        if wildcards > max_wildcards:\n            # Issue #17980: avoid denials of service by refusing more\n            # than one wildcard per fragment.  A survey of established\n            # policy among SSL implementations showed it to be a\n            # reasonable choice.\n            raise CertificateError(\n                \"too many wildcards in certificate DNS name: \" + repr(dn))\n\n        # speed up common case w/o wildcards\n        if not wildcards:\n            return dn.lower() == hostname.lower()\n\n        # RFC 6125, section 6.4.3, subitem 1.\n        # The client SHOULD NOT attempt to match a\n        # presented identifier in which the wildcard\n        # character comprises a label other than the\n        # left-most label.\n        if leftmost == '*':\n            # When '*' is a fragment by itself, it matches a non-empty dotless\n            # fragment.\n            pats.append('[^.]+')\n        elif leftmost.startswith('xn--') or hostname.startswith('xn--'):\n            # RFC 6125, section 6.4.3, subitem 3.\n            # The client SHOULD NOT attempt to match a presented identifier\n            # where the wildcard character is embedded within an A-label or\n            # U-label of an internationalized domain name.\n            pats.append(re.escape(leftmost))\n        else:\n            # Otherwise, '*' matches any dotless string, e.g. www*\n            pats.append(re.escape(leftmost).replace(r'\\*', '[^.]*'))\n\n        # add the remaining fragments, ignore any wildcards\n        for frag in remainder:\n            pats.append(re.escape(frag))\n\n        pat = re.compile(r'\\A' + r'\\.'.join(pats) + r'\\Z', re.IGNORECASE)\n        return pat.match(hostname)\n\n    def match_hostname(cert, hostname):\n        \"\"\"Verify that *cert* (in decoded format as returned by\n        SSLSocket.getpeercert()) matches the *hostname*.  RFC 2818 and RFC 6125\n        rules are followed, but IP addresses are not accepted for *hostname*.\n\n        CertificateError is raised on failure. On success, the function\n        returns nothing.\n        \"\"\"\n        if not cert:\n            raise ValueError(\"empty or no certificate\")\n        dnsnames = []\n        san = cert.get('subjectAltName', ())\n        for key, value in san:\n            if key == 'DNS':\n                if _dnsname_match(value, hostname):\n                    return\n                dnsnames.append(value)\n        if not dnsnames:\n            # The subject is only checked when there is no dNSName entry\n            # in subjectAltName\n            for sub in cert.get('subject', ()):\n                for key, value in sub:\n                    # XXX according to RFC 2818, the most specific Common Name\n                    # must be used.\n                    if key == 'commonName':\n                        if _dnsname_match(value, hostname):\n                            return\n                        dnsnames.append(value)\n        if len(dnsnames) > 1:\n            raise CertificateError(\n                \"hostname %r doesn't match either of %s\"\n                % (hostname, ', '.join(map(repr, dnsnames))))\n        elif len(dnsnames) == 1:\n            raise CertificateError(\n                \"hostname %r doesn't match %r\"\n                % (hostname, dnsnames[0]))\n        else:\n            raise CertificateError(\n                \"no appropriate commonName or \"\n                \"subjectAltName fields were found\")\n\n\nclass VerifyingHTTPSHandler(HTTPSHandler):\n    \"\"\"Simple verifying handler: no auth, subclasses, timeouts, etc.\"\"\"\n\n    def __init__(self, ca_bundle):\n        self.ca_bundle = ca_bundle\n        HTTPSHandler.__init__(self)\n\n    def https_open(self, req):\n        return self.do_open(\n            lambda host, **kw: VerifyingHTTPSConn(host, self.ca_bundle, **kw),\n            req\n        )\n\n\nclass VerifyingHTTPSConn(HTTPSConnection):\n    \"\"\"Simple verifying connection: no auth, subclasses, timeouts, etc.\"\"\"\n\n    def __init__(self, host, ca_bundle, **kw):\n        HTTPSConnection.__init__(self, host, **kw)\n        self.ca_bundle = ca_bundle\n\n    def connect(self):\n        sock = socket.create_connection(\n            (self.host, self.port), getattr(self, 'source_address', None)\n        )\n\n        # Handle the socket if a (proxy) tunnel is present\n        if hasattr(self, '_tunnel') and getattr(self, '_tunnel_host', None):\n            self.sock = sock\n            self._tunnel()\n            # http://bugs.python.org/issue7776: Python>=3.4.1 and >=2.7.7\n            # change self.host to mean the proxy server host when tunneling is\n            # being used. Adapt, since we are interested in the destination\n            # host for the match_hostname() comparison.\n            actual_host = self._tunnel_host\n        else:\n            actual_host = self.host\n\n        if hasattr(ssl, 'create_default_context'):\n            ctx = ssl.create_default_context(cafile=self.ca_bundle)\n            self.sock = ctx.wrap_socket(sock, server_hostname=actual_host)\n        else:\n            # This is for python < 2.7.9 and < 3.4?\n            self.sock = ssl.wrap_socket(\n                sock, cert_reqs=ssl.CERT_REQUIRED, ca_certs=self.ca_bundle\n            )\n        try:\n            match_hostname(self.sock.getpeercert(), actual_host)\n        except CertificateError:\n            self.sock.shutdown(socket.SHUT_RDWR)\n            self.sock.close()\n            raise\n\n\ndef opener_for(ca_bundle=None):\n    \"\"\"Get a urlopen() replacement that uses ca_bundle for verification\"\"\"\n    return urllib.request.build_opener(\n        VerifyingHTTPSHandler(ca_bundle or find_ca_bundle())\n    ).open\n\n\n# from jaraco.functools\ndef once(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if not hasattr(func, 'always_returns'):\n            func.always_returns = func(*args, **kwargs)\n        return func.always_returns\n    return wrapper\n\n\n@once\ndef get_win_certfile():\n    try:\n        import wincertstore\n    except ImportError:\n        return None\n\n    class CertFile(wincertstore.CertFile):\n        def __init__(self):\n            super(CertFile, self).__init__()\n            atexit.register(self.close)\n\n        def close(self):\n            try:\n                super(CertFile, self).close()\n            except OSError:\n                pass\n\n    _wincerts = CertFile()\n    _wincerts.addstore('CA')\n    _wincerts.addstore('ROOT')\n    return _wincerts.name\n\n\ndef find_ca_bundle():\n    \"\"\"Return an existing CA bundle path, or None\"\"\"\n    extant_cert_paths = filter(os.path.isfile, cert_paths)\n    return (\n        get_win_certfile()\n        or next(extant_cert_paths, None)\n        or _certifi_where()\n    )\n\n\ndef _certifi_where():\n    try:\n        return __import__('certifi').where()\n    except (ImportError, ResolutionError, ExtractionError):\n        pass\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/ssl_support.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/ssl_support.py	(date 1602088701601)
@@ -3,8 +3,9 @@
 import atexit
 import re
 import functools
+import urllib.request
+import http.client
 
-from setuptools.extern.six.moves import urllib, http_client, map, filter
 
 from pkg_resources import ResolutionError, ExtractionError
 
@@ -31,7 +32,7 @@
 
 try:
     HTTPSHandler = urllib.request.HTTPSHandler
-    HTTPSConnection = http_client.HTTPSConnection
+    HTTPSConnection = http.client.HTTPSConnection
 except AttributeError:
     HTTPSHandler = HTTPSConnection = object
 
Index: env/lib/python3.8/site-packages/setuptools/monkey.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\nMonkey patching of distutils.\n\"\"\"\n\nimport sys\nimport distutils.filelist\nimport platform\nimport types\nimport functools\nfrom importlib import import_module\nimport inspect\n\nfrom setuptools.extern import six\n\nimport setuptools\n\n__all__ = []\n\"\"\"\nEverything is private. Contact the project team\nif you think you need this functionality.\n\"\"\"\n\n\ndef _get_mro(cls):\n    \"\"\"\n    Returns the bases classes for cls sorted by the MRO.\n\n    Works around an issue on Jython where inspect.getmro will not return all\n    base classes if multiple classes share the same name. Instead, this\n    function will return a tuple containing the class itself, and the contents\n    of cls.__bases__. See https://github.com/pypa/setuptools/issues/1024.\n    \"\"\"\n    if platform.python_implementation() == \"Jython\":\n        return (cls,) + cls.__bases__\n    return inspect.getmro(cls)\n\n\ndef get_unpatched(item):\n    lookup = (\n        get_unpatched_class if isinstance(item, six.class_types) else\n        get_unpatched_function if isinstance(item, types.FunctionType) else\n        lambda item: None\n    )\n    return lookup(item)\n\n\ndef get_unpatched_class(cls):\n    \"\"\"Protect against re-patching the distutils if reloaded\n\n    Also ensures that no other distutils extension monkeypatched the distutils\n    first.\n    \"\"\"\n    external_bases = (\n        cls\n        for cls in _get_mro(cls)\n        if not cls.__module__.startswith('setuptools')\n    )\n    base = next(external_bases)\n    if not base.__module__.startswith('distutils'):\n        msg = \"distutils has already been patched by %r\" % cls\n        raise AssertionError(msg)\n    return base\n\n\ndef patch_all():\n    # we can't patch distutils.cmd, alas\n    distutils.core.Command = setuptools.Command\n\n    has_issue_12885 = sys.version_info <= (3, 5, 3)\n\n    if has_issue_12885:\n        # fix findall bug in distutils (http://bugs.python.org/issue12885)\n        distutils.filelist.findall = setuptools.findall\n\n    needs_warehouse = (\n        sys.version_info < (2, 7, 13)\n        or\n        (3, 4) < sys.version_info < (3, 4, 6)\n        or\n        (3, 5) < sys.version_info <= (3, 5, 3)\n    )\n\n    if needs_warehouse:\n        warehouse = 'https://upload.pypi.org/legacy/'\n        distutils.config.PyPIRCCommand.DEFAULT_REPOSITORY = warehouse\n\n    _patch_distribution_metadata()\n\n    # Install Distribution throughout the distutils\n    for module in distutils.dist, distutils.core, distutils.cmd:\n        module.Distribution = setuptools.dist.Distribution\n\n    # Install the patched Extension\n    distutils.core.Extension = setuptools.extension.Extension\n    distutils.extension.Extension = setuptools.extension.Extension\n    if 'distutils.command.build_ext' in sys.modules:\n        sys.modules['distutils.command.build_ext'].Extension = (\n            setuptools.extension.Extension\n        )\n\n    patch_for_msvc_specialized_compiler()\n\n\ndef _patch_distribution_metadata():\n    \"\"\"Patch write_pkg_file and read_pkg_file for higher metadata standards\"\"\"\n    for attr in ('write_pkg_file', 'read_pkg_file', 'get_metadata_version'):\n        new_val = getattr(setuptools.dist, attr)\n        setattr(distutils.dist.DistributionMetadata, attr, new_val)\n\n\ndef patch_func(replacement, target_mod, func_name):\n    \"\"\"\n    Patch func_name in target_mod with replacement\n\n    Important - original must be resolved by name to avoid\n    patching an already patched function.\n    \"\"\"\n    original = getattr(target_mod, func_name)\n\n    # set the 'unpatched' attribute on the replacement to\n    # point to the original.\n    vars(replacement).setdefault('unpatched', original)\n\n    # replace the function in the original module\n    setattr(target_mod, func_name, replacement)\n\n\ndef get_unpatched_function(candidate):\n    return getattr(candidate, 'unpatched')\n\n\ndef patch_for_msvc_specialized_compiler():\n    \"\"\"\n    Patch functions in distutils to use standalone Microsoft Visual C++\n    compilers.\n    \"\"\"\n    # import late to avoid circular imports on Python < 3.5\n    msvc = import_module('setuptools.msvc')\n\n    if platform.system() != 'Windows':\n        # Compilers only availables on Microsoft Windows\n        return\n\n    def patch_params(mod_name, func_name):\n        \"\"\"\n        Prepare the parameters for patch_func to patch indicated function.\n        \"\"\"\n        repl_prefix = 'msvc9_' if 'msvc9' in mod_name else 'msvc14_'\n        repl_name = repl_prefix + func_name.lstrip('_')\n        repl = getattr(msvc, repl_name)\n        mod = import_module(mod_name)\n        if not hasattr(mod, func_name):\n            raise ImportError(func_name)\n        return repl, mod, func_name\n\n    # Python 2.7 to 3.4\n    msvc9 = functools.partial(patch_params, 'distutils.msvc9compiler')\n\n    # Python 3.5+\n    msvc14 = functools.partial(patch_params, 'distutils._msvccompiler')\n\n    try:\n        # Patch distutils.msvc9compiler\n        patch_func(*msvc9('find_vcvarsall'))\n        patch_func(*msvc9('query_vcvarsall'))\n    except ImportError:\n        pass\n\n    try:\n        # Patch distutils._msvccompiler._get_vc_env\n        patch_func(*msvc14('_get_vc_env'))\n    except ImportError:\n        pass\n\n    try:\n        # Patch distutils._msvccompiler.gen_lib_options for Numpy\n        patch_func(*msvc14('gen_lib_options'))\n    except ImportError:\n        pass\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/monkey.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/monkey.py	(date 1602088701601)
@@ -10,8 +10,6 @@
 from importlib import import_module
 import inspect
 
-from setuptools.extern import six
-
 import setuptools
 
 __all__ = []
@@ -37,7 +35,7 @@
 
 def get_unpatched(item):
     lookup = (
-        get_unpatched_class if isinstance(item, six.class_types) else
+        get_unpatched_class if isinstance(item, type) else
         get_unpatched_function if isinstance(item, types.FunctionType) else
         lambda item: None
     )
@@ -138,7 +136,7 @@
     msvc = import_module('setuptools.msvc')
 
     if platform.system() != 'Windows':
-        # Compilers only availables on Microsoft Windows
+        # Compilers only available on Microsoft Windows
         return
 
     def patch_params(mod_name, func_name):
Index: env/lib/python3.8/site-packages/pip/_internal/req/req_install.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># The following comment should be removed at some point in the future.\n# mypy: strict-optional=False\n\nfrom __future__ import absolute_import\n\nimport logging\nimport os\nimport shutil\nimport sys\nimport uuid\nimport zipfile\n\nfrom pip._vendor import pkg_resources, six\nfrom pip._vendor.packaging.requirements import Requirement\nfrom pip._vendor.packaging.utils import canonicalize_name\nfrom pip._vendor.packaging.version import Version\nfrom pip._vendor.packaging.version import parse as parse_version\nfrom pip._vendor.pep517.wrappers import Pep517HookCaller\n\nfrom pip._internal.build_env import NoOpBuildEnvironment\nfrom pip._internal.exceptions import InstallationError\nfrom pip._internal.locations import get_scheme\nfrom pip._internal.models.link import Link\nfrom pip._internal.operations.build.metadata import generate_metadata\nfrom pip._internal.operations.build.metadata_legacy import \\\n    generate_metadata as generate_metadata_legacy\nfrom pip._internal.operations.install.editable_legacy import \\\n    install_editable as install_editable_legacy\nfrom pip._internal.operations.install.legacy import LegacyInstallFailure\nfrom pip._internal.operations.install.legacy import install as install_legacy\nfrom pip._internal.operations.install.wheel import install_wheel\nfrom pip._internal.pyproject import load_pyproject_toml, make_pyproject_path\nfrom pip._internal.req.req_uninstall import UninstallPathSet\nfrom pip._internal.utils.deprecation import deprecated\nfrom pip._internal.utils.direct_url_helpers import direct_url_from_link\nfrom pip._internal.utils.hashes import Hashes\nfrom pip._internal.utils.logging import indent_log\nfrom pip._internal.utils.misc import (\n    ask_path_exists,\n    backup_dir,\n    display_path,\n    dist_in_site_packages,\n    dist_in_usersite,\n    get_distribution,\n    get_installed_version,\n    hide_url,\n    redact_auth_from_url,\n)\nfrom pip._internal.utils.packaging import get_metadata\nfrom pip._internal.utils.temp_dir import TempDirectory, tempdir_kinds\nfrom pip._internal.utils.typing import MYPY_CHECK_RUNNING\nfrom pip._internal.utils.virtualenv import running_under_virtualenv\nfrom pip._internal.vcs import vcs\n\nif MYPY_CHECK_RUNNING:\n    from typing import (\n        Any, Dict, Iterable, List, Optional, Sequence, Union,\n    )\n    from pip._internal.build_env import BuildEnvironment\n    from pip._vendor.pkg_resources import Distribution\n    from pip._vendor.packaging.specifiers import SpecifierSet\n    from pip._vendor.packaging.markers import Marker\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef _get_dist(metadata_directory):\n    # type: (str) -> Distribution\n    \"\"\"Return a pkg_resources.Distribution for the provided\n    metadata directory.\n    \"\"\"\n    dist_dir = metadata_directory.rstrip(os.sep)\n\n    # Build a PathMetadata object, from path to metadata. :wink:\n    base_dir, dist_dir_name = os.path.split(dist_dir)\n    metadata = pkg_resources.PathMetadata(base_dir, dist_dir)\n\n    # Determine the correct Distribution object type.\n    if dist_dir.endswith(\".egg-info\"):\n        dist_cls = pkg_resources.Distribution\n        dist_name = os.path.splitext(dist_dir_name)[0]\n    else:\n        assert dist_dir.endswith(\".dist-info\")\n        dist_cls = pkg_resources.DistInfoDistribution\n        dist_name = os.path.splitext(dist_dir_name)[0].split(\"-\")[0]\n\n    return dist_cls(\n        base_dir,\n        project_name=dist_name,\n        metadata=metadata,\n    )\n\n\nclass InstallRequirement(object):\n    \"\"\"\n    Represents something that may be installed later on, may have information\n    about where to fetch the relevant requirement and also contains logic for\n    installing the said requirement.\n    \"\"\"\n\n    def __init__(\n        self,\n        req,  # type: Optional[Requirement]\n        comes_from,  # type: Optional[Union[str, InstallRequirement]]\n        editable=False,  # type: bool\n        link=None,  # type: Optional[Link]\n        markers=None,  # type: Optional[Marker]\n        use_pep517=None,  # type: Optional[bool]\n        isolated=False,  # type: bool\n        install_options=None,  # type: Optional[List[str]]\n        global_options=None,  # type: Optional[List[str]]\n        hash_options=None,  # type: Optional[Dict[str, List[str]]]\n        constraint=False,  # type: bool\n        extras=(),  # type: Iterable[str]\n        user_supplied=False,  # type: bool\n    ):\n        # type: (...) -> None\n        assert req is None or isinstance(req, Requirement), req\n        self.req = req\n        self.comes_from = comes_from\n        self.constraint = constraint\n        self.editable = editable\n\n        # source_dir is the local directory where the linked requirement is\n        # located, or unpacked. In case unpacking is needed, creating and\n        # populating source_dir is done by the RequirementPreparer. Note this\n        # is not necessarily the directory where pyproject.toml or setup.py is\n        # located - that one is obtained via unpacked_source_directory.\n        self.source_dir = None  # type: Optional[str]\n        if self.editable:\n            assert link\n            if link.is_file:\n                self.source_dir = os.path.normpath(\n                    os.path.abspath(link.file_path)\n                )\n\n        if link is None and req and req.url:\n            # PEP 508 URL requirement\n            link = Link(req.url)\n        self.link = self.original_link = link\n        self.original_link_is_in_wheel_cache = False\n\n        # Path to any downloaded or already-existing package.\n        self.local_file_path = None  # type: Optional[str]\n        if self.link and self.link.is_file:\n            self.local_file_path = self.link.file_path\n\n        if extras:\n            self.extras = extras\n        elif req:\n            self.extras = {\n                pkg_resources.safe_extra(extra) for extra in req.extras\n            }\n        else:\n            self.extras = set()\n        if markers is None and req:\n            markers = req.marker\n        self.markers = markers\n\n        # This holds the pkg_resources.Distribution object if this requirement\n        # is already available:\n        self.satisfied_by = None  # type: Optional[Distribution]\n        # Whether the installation process should try to uninstall an existing\n        # distribution before installing this requirement.\n        self.should_reinstall = False\n        # Temporary build location\n        self._temp_build_dir = None  # type: Optional[TempDirectory]\n        # Set to True after successful installation\n        self.install_succeeded = None  # type: Optional[bool]\n        # Supplied options\n        self.install_options = install_options if install_options else []\n        self.global_options = global_options if global_options else []\n        self.hash_options = hash_options if hash_options else {}\n        # Set to True after successful preparation of this requirement\n        self.prepared = False\n        # User supplied requirement are explicitly requested for installation\n        # by the user via CLI arguments or requirements files, as opposed to,\n        # e.g. dependencies, extras or constraints.\n        self.user_supplied = user_supplied\n\n        # Set by the legacy resolver when the requirement has been downloaded\n        # TODO: This introduces a strong coupling between the resolver and the\n        #       requirement (the coupling was previously between the resolver\n        #       and the requirement set). This should be refactored to allow\n        #       the requirement to decide for itself when it has been\n        #       successfully downloaded - but that is more tricky to get right,\n        #       se we are making the change in stages.\n        self.successfully_downloaded = False\n\n        self.isolated = isolated\n        self.build_env = NoOpBuildEnvironment()  # type: BuildEnvironment\n\n        # For PEP 517, the directory where we request the project metadata\n        # gets stored. We need this to pass to build_wheel, so the backend\n        # can ensure that the wheel matches the metadata (see the PEP for\n        # details).\n        self.metadata_directory = None  # type: Optional[str]\n\n        # The static build requirements (from pyproject.toml)\n        self.pyproject_requires = None  # type: Optional[List[str]]\n\n        # Build requirements that we will check are available\n        self.requirements_to_check = []  # type: List[str]\n\n        # The PEP 517 backend we should use to build the project\n        self.pep517_backend = None  # type: Optional[Pep517HookCaller]\n\n        # Are we using PEP 517 for this requirement?\n        # After pyproject.toml has been loaded, the only valid values are True\n        # and False. Before loading, None is valid (meaning \"use the default\").\n        # Setting an explicit value before loading pyproject.toml is supported,\n        # but after loading this flag should be treated as read only.\n        self.use_pep517 = use_pep517\n\n    def __str__(self):\n        # type: () -> str\n        if self.req:\n            s = str(self.req)\n            if self.link:\n                s += ' from {}'.format(redact_auth_from_url(self.link.url))\n        elif self.link:\n            s = redact_auth_from_url(self.link.url)\n        else:\n            s = '<InstallRequirement>'\n        if self.satisfied_by is not None:\n            s += ' in {}'.format(display_path(self.satisfied_by.location))\n        if self.comes_from:\n            if isinstance(self.comes_from, six.string_types):\n                comes_from = self.comes_from  # type: Optional[str]\n            else:\n                comes_from = self.comes_from.from_path()\n            if comes_from:\n                s += ' (from {})'.format(comes_from)\n        return s\n\n    def __repr__(self):\n        # type: () -> str\n        return '<{} object: {} editable={!r}>'.format(\n            self.__class__.__name__, str(self), self.editable)\n\n    def format_debug(self):\n        # type: () -> str\n        \"\"\"An un-tested helper for getting state, for debugging.\n        \"\"\"\n        attributes = vars(self)\n        names = sorted(attributes)\n\n        state = (\n            \"{}={!r}\".format(attr, attributes[attr]) for attr in sorted(names)\n        )\n        return '<{name} object: {{{state}}}>'.format(\n            name=self.__class__.__name__,\n            state=\", \".join(state),\n        )\n\n    # Things that are valid for all kinds of requirements?\n    @property\n    def name(self):\n        # type: () -> Optional[str]\n        if self.req is None:\n            return None\n        return six.ensure_str(pkg_resources.safe_name(self.req.name))\n\n    @property\n    def specifier(self):\n        # type: () -> SpecifierSet\n        return self.req.specifier\n\n    @property\n    def is_pinned(self):\n        # type: () -> bool\n        \"\"\"Return whether I am pinned to an exact version.\n\n        For example, some-package==1.2 is pinned; some-package>1.2 is not.\n        \"\"\"\n        specifiers = self.specifier\n        return (len(specifiers) == 1 and\n                next(iter(specifiers)).operator in {'==', '==='})\n\n    @property\n    def installed_version(self):\n        # type: () -> Optional[str]\n        return get_installed_version(self.name)\n\n    def match_markers(self, extras_requested=None):\n        # type: (Optional[Iterable[str]]) -> bool\n        if not extras_requested:\n            # Provide an extra to safely evaluate the markers\n            # without matching any extra\n            extras_requested = ('',)\n        if self.markers is not None:\n            return any(\n                self.markers.evaluate({'extra': extra})\n                for extra in extras_requested)\n        else:\n            return True\n\n    @property\n    def has_hash_options(self):\n        # type: () -> bool\n        \"\"\"Return whether any known-good hashes are specified as options.\n\n        These activate --require-hashes mode; hashes specified as part of a\n        URL do not.\n\n        \"\"\"\n        return bool(self.hash_options)\n\n    def hashes(self, trust_internet=True):\n        # type: (bool) -> Hashes\n        \"\"\"Return a hash-comparer that considers my option- and URL-based\n        hashes to be known-good.\n\n        Hashes in URLs--ones embedded in the requirements file, not ones\n        downloaded from an index server--are almost peers with ones from\n        flags. They satisfy --require-hashes (whether it was implicitly or\n        explicitly activated) but do not activate it. md5 and sha224 are not\n        allowed in flags, which should nudge people toward good algos. We\n        always OR all hashes together, even ones from URLs.\n\n        :param trust_internet: Whether to trust URL-based (#md5=...) hashes\n            downloaded from the internet, as by populate_link()\n\n        \"\"\"\n        good_hashes = self.hash_options.copy()\n        link = self.link if trust_internet else self.original_link\n        if link and link.hash:\n            good_hashes.setdefault(link.hash_name, []).append(link.hash)\n        return Hashes(good_hashes)\n\n    def from_path(self):\n        # type: () -> Optional[str]\n        \"\"\"Format a nice indicator to show where this \"comes from\"\n        \"\"\"\n        if self.req is None:\n            return None\n        s = str(self.req)\n        if self.comes_from:\n            if isinstance(self.comes_from, six.string_types):\n                comes_from = self.comes_from\n            else:\n                comes_from = self.comes_from.from_path()\n            if comes_from:\n                s += '->' + comes_from\n        return s\n\n    def ensure_build_location(self, build_dir, autodelete, parallel_builds):\n        # type: (str, bool, bool) -> str\n        assert build_dir is not None\n        if self._temp_build_dir is not None:\n            assert self._temp_build_dir.path\n            return self._temp_build_dir.path\n        if self.req is None:\n            # Some systems have /tmp as a symlink which confuses custom\n            # builds (such as numpy). Thus, we ensure that the real path\n            # is returned.\n            self._temp_build_dir = TempDirectory(\n                kind=tempdir_kinds.REQ_BUILD, globally_managed=True\n            )\n\n            return self._temp_build_dir.path\n\n        # When parallel builds are enabled, add a UUID to the build directory\n        # name so multiple builds do not interfere with each other.\n        dir_name = canonicalize_name(self.name)\n        if parallel_builds:\n            dir_name = \"{}_{}\".format(dir_name, uuid.uuid4().hex)\n\n        # FIXME: Is there a better place to create the build_dir? (hg and bzr\n        # need this)\n        if not os.path.exists(build_dir):\n            logger.debug('Creating directory %s', build_dir)\n            os.makedirs(build_dir)\n        actual_build_dir = os.path.join(build_dir, dir_name)\n        # `None` indicates that we respect the globally-configured deletion\n        # settings, which is what we actually want when auto-deleting.\n        delete_arg = None if autodelete else False\n        return TempDirectory(\n            path=actual_build_dir,\n            delete=delete_arg,\n            kind=tempdir_kinds.REQ_BUILD,\n            globally_managed=True,\n        ).path\n\n    def _set_requirement(self):\n        # type: () -> None\n        \"\"\"Set requirement after generating metadata.\n        \"\"\"\n        assert self.req is None\n        assert self.metadata is not None\n        assert self.source_dir is not None\n\n        # Construct a Requirement object from the generated metadata\n        if isinstance(parse_version(self.metadata[\"Version\"]), Version):\n            op = \"==\"\n        else:\n            op = \"===\"\n\n        self.req = Requirement(\n            \"\".join([\n                self.metadata[\"Name\"],\n                op,\n                self.metadata[\"Version\"],\n            ])\n        )\n\n    def warn_on_mismatching_name(self):\n        # type: () -> None\n        metadata_name = canonicalize_name(self.metadata[\"Name\"])\n        if canonicalize_name(self.req.name) == metadata_name:\n            # Everything is fine.\n            return\n\n        # If we're here, there's a mismatch. Log a warning about it.\n        logger.warning(\n            'Generating metadata for package %s '\n            'produced metadata for project name %s. Fix your '\n            '#egg=%s fragments.',\n            self.name, metadata_name, self.name\n        )\n        self.req = Requirement(metadata_name)\n\n    def check_if_exists(self, use_user_site):\n        # type: (bool) -> None\n        \"\"\"Find an installed distribution that satisfies or conflicts\n        with this requirement, and set self.satisfied_by or\n        self.should_reinstall appropriately.\n        \"\"\"\n        if self.req is None:\n            return\n        existing_dist = get_distribution(self.req.name)\n        if not existing_dist:\n            return\n\n        existing_version = existing_dist.parsed_version\n        if not self.req.specifier.contains(existing_version, prereleases=True):\n            self.satisfied_by = None\n            if use_user_site:\n                if dist_in_usersite(existing_dist):\n                    self.should_reinstall = True\n                elif (running_under_virtualenv() and\n                        dist_in_site_packages(existing_dist)):\n                    raise InstallationError(\n                        \"Will not install to the user site because it will \"\n                        \"lack sys.path precedence to {} in {}\".format(\n                            existing_dist.project_name, existing_dist.location)\n                    )\n            else:\n                self.should_reinstall = True\n        else:\n            if self.editable:\n                self.should_reinstall = True\n                # when installing editables, nothing pre-existing should ever\n                # satisfy\n                self.satisfied_by = None\n            else:\n                self.satisfied_by = existing_dist\n\n    # Things valid for wheels\n    @property\n    def is_wheel(self):\n        # type: () -> bool\n        if not self.link:\n            return False\n        return self.link.is_wheel\n\n    # Things valid for sdists\n    @property\n    def unpacked_source_directory(self):\n        # type: () -> str\n        return os.path.join(\n            self.source_dir,\n            self.link and self.link.subdirectory_fragment or '')\n\n    @property\n    def setup_py_path(self):\n        # type: () -> str\n        assert self.source_dir, \"No source dir for {}\".format(self)\n        setup_py = os.path.join(self.unpacked_source_directory, 'setup.py')\n\n        # Python2 __file__ should not be unicode\n        if six.PY2 and isinstance(setup_py, six.text_type):\n            setup_py = setup_py.encode(sys.getfilesystemencoding())\n\n        return setup_py\n\n    @property\n    def pyproject_toml_path(self):\n        # type: () -> str\n        assert self.source_dir, \"No source dir for {}\".format(self)\n        return make_pyproject_path(self.unpacked_source_directory)\n\n    def load_pyproject_toml(self):\n        # type: () -> None\n        \"\"\"Load the pyproject.toml file.\n\n        After calling this routine, all of the attributes related to PEP 517\n        processing for this requirement have been set. In particular, the\n        use_pep517 attribute can be used to determine whether we should\n        follow the PEP 517 or legacy (setup.py) code path.\n        \"\"\"\n        pyproject_toml_data = load_pyproject_toml(\n            self.use_pep517,\n            self.pyproject_toml_path,\n            self.setup_py_path,\n            str(self)\n        )\n\n        if pyproject_toml_data is None:\n            self.use_pep517 = False\n            return\n\n        self.use_pep517 = True\n        requires, backend, check, backend_path = pyproject_toml_data\n        self.requirements_to_check = check\n        self.pyproject_requires = requires\n        self.pep517_backend = Pep517HookCaller(\n            self.unpacked_source_directory, backend, backend_path=backend_path,\n        )\n\n    def _generate_metadata(self):\n        # type: () -> str\n        \"\"\"Invokes metadata generator functions, with the required arguments.\n        \"\"\"\n        if not self.use_pep517:\n            assert self.unpacked_source_directory\n\n            return generate_metadata_legacy(\n                build_env=self.build_env,\n                setup_py_path=self.setup_py_path,\n                source_dir=self.unpacked_source_directory,\n                isolated=self.isolated,\n                details=self.name or \"from {}\".format(self.link)\n            )\n\n        assert self.pep517_backend is not None\n\n        return generate_metadata(\n            build_env=self.build_env,\n            backend=self.pep517_backend,\n        )\n\n    def prepare_metadata(self):\n        # type: () -> None\n        \"\"\"Ensure that project metadata is available.\n\n        Under PEP 517, call the backend hook to prepare the metadata.\n        Under legacy processing, call setup.py egg-info.\n        \"\"\"\n        assert self.source_dir\n\n        with indent_log():\n            self.metadata_directory = self._generate_metadata()\n\n        # Act on the newly generated metadata, based on the name and version.\n        if not self.name:\n            self._set_requirement()\n        else:\n            self.warn_on_mismatching_name()\n\n        self.assert_source_matches_version()\n\n    @property\n    def metadata(self):\n        # type: () -> Any\n        if not hasattr(self, '_metadata'):\n            self._metadata = get_metadata(self.get_dist())\n\n        return self._metadata\n\n    def get_dist(self):\n        # type: () -> Distribution\n        return _get_dist(self.metadata_directory)\n\n    def assert_source_matches_version(self):\n        # type: () -> None\n        assert self.source_dir\n        version = self.metadata['version']\n        if self.req.specifier and version not in self.req.specifier:\n            logger.warning(\n                'Requested %s, but installing version %s',\n                self,\n                version,\n            )\n        else:\n            logger.debug(\n                'Source in %s has version %s, which satisfies requirement %s',\n                display_path(self.source_dir),\n                version,\n                self,\n            )\n\n    # For both source distributions and editables\n    def ensure_has_source_dir(\n        self,\n        parent_dir,\n        autodelete=False,\n        parallel_builds=False,\n    ):\n        # type: (str, bool, bool) -> None\n        \"\"\"Ensure that a source_dir is set.\n\n        This will create a temporary build dir if the name of the requirement\n        isn't known yet.\n\n        :param parent_dir: The ideal pip parent_dir for the source_dir.\n            Generally src_dir for editables and build_dir for sdists.\n        :return: self.source_dir\n        \"\"\"\n        if self.source_dir is None:\n            self.source_dir = self.ensure_build_location(\n                parent_dir,\n                autodelete=autodelete,\n                parallel_builds=parallel_builds,\n            )\n\n    # For editable installations\n    def update_editable(self, obtain=True):\n        # type: (bool) -> None\n        if not self.link:\n            logger.debug(\n                \"Cannot update repository at %s; repository location is \"\n                \"unknown\",\n                self.source_dir,\n            )\n            return\n        assert self.editable\n        assert self.source_dir\n        if self.link.scheme == 'file':\n            # Static paths don't get updated\n            return\n        assert '+' in self.link.url, \\\n            \"bad url: {self.link.url!r}\".format(**locals())\n        vc_type, url = self.link.url.split('+', 1)\n        vcs_backend = vcs.get_backend(vc_type)\n        if vcs_backend:\n            if not self.link.is_vcs:\n                reason = (\n                    \"This form of VCS requirement is being deprecated: {}.\"\n                ).format(\n                    self.link.url\n                )\n                replacement = None\n                if self.link.url.startswith(\"git+git@\"):\n                    replacement = (\n                        \"git+https://git@example.com/..., \"\n                        \"git+ssh://git@example.com/..., \"\n                        \"or the insecure git+git://git@example.com/...\"\n                    )\n                deprecated(reason, replacement, gone_in=\"21.0\", issue=7554)\n            hidden_url = hide_url(self.link.url)\n            if obtain:\n                vcs_backend.obtain(self.source_dir, url=hidden_url)\n            else:\n                vcs_backend.export(self.source_dir, url=hidden_url)\n        else:\n            assert 0, (\n                'Unexpected version control type (in {}): {}'.format(\n                    self.link, vc_type))\n\n    # Top-level Actions\n    def uninstall(self, auto_confirm=False, verbose=False):\n        # type: (bool, bool) -> Optional[UninstallPathSet]\n        \"\"\"\n        Uninstall the distribution currently satisfying this requirement.\n\n        Prompts before removing or modifying files unless\n        ``auto_confirm`` is True.\n\n        Refuses to delete or modify files outside of ``sys.prefix`` -\n        thus uninstallation within a virtual environment can only\n        modify that virtual environment, even if the virtualenv is\n        linked to global site-packages.\n\n        \"\"\"\n        assert self.req\n        dist = get_distribution(self.req.name)\n        if not dist:\n            logger.warning(\"Skipping %s as it is not installed.\", self.name)\n            return None\n        logger.info('Found existing installation: %s', dist)\n\n        uninstalled_pathset = UninstallPathSet.from_dist(dist)\n        uninstalled_pathset.remove(auto_confirm, verbose)\n        return uninstalled_pathset\n\n    def _get_archive_name(self, path, parentdir, rootdir):\n        # type: (str, str, str) -> str\n\n        def _clean_zip_name(name, prefix):\n            # type: (str, str) -> str\n            assert name.startswith(prefix + os.path.sep), (\n                \"name {name!r} doesn't start with prefix {prefix!r}\"\n                .format(**locals())\n            )\n            name = name[len(prefix) + 1:]\n            name = name.replace(os.path.sep, '/')\n            return name\n\n        path = os.path.join(parentdir, path)\n        name = _clean_zip_name(path, rootdir)\n        return self.name + '/' + name\n\n    def archive(self, build_dir):\n        # type: (str) -> None\n        \"\"\"Saves archive to provided build_dir.\n\n        Used for saving downloaded VCS requirements as part of `pip download`.\n        \"\"\"\n        assert self.source_dir\n\n        create_archive = True\n        archive_name = '{}-{}.zip'.format(self.name, self.metadata[\"version\"])\n        archive_path = os.path.join(build_dir, archive_name)\n\n        if os.path.exists(archive_path):\n            response = ask_path_exists(\n                'The file {} exists. (i)gnore, (w)ipe, '\n                '(b)ackup, (a)bort '.format(\n                    display_path(archive_path)),\n                ('i', 'w', 'b', 'a'))\n            if response == 'i':\n                create_archive = False\n            elif response == 'w':\n                logger.warning('Deleting %s', display_path(archive_path))\n                os.remove(archive_path)\n            elif response == 'b':\n                dest_file = backup_dir(archive_path)\n                logger.warning(\n                    'Backing up %s to %s',\n                    display_path(archive_path),\n                    display_path(dest_file),\n                )\n                shutil.move(archive_path, dest_file)\n            elif response == 'a':\n                sys.exit(-1)\n\n        if not create_archive:\n            return\n\n        zip_output = zipfile.ZipFile(\n            archive_path, 'w', zipfile.ZIP_DEFLATED, allowZip64=True,\n        )\n        with zip_output:\n            dir = os.path.normcase(\n                os.path.abspath(self.unpacked_source_directory)\n            )\n            for dirpath, dirnames, filenames in os.walk(dir):\n                for dirname in dirnames:\n                    dir_arcname = self._get_archive_name(\n                        dirname, parentdir=dirpath, rootdir=dir,\n                    )\n                    zipdir = zipfile.ZipInfo(dir_arcname + '/')\n                    zipdir.external_attr = 0x1ED << 16  # 0o755\n                    zip_output.writestr(zipdir, '')\n                for filename in filenames:\n                    file_arcname = self._get_archive_name(\n                        filename, parentdir=dirpath, rootdir=dir,\n                    )\n                    filename = os.path.join(dirpath, filename)\n                    zip_output.write(filename, file_arcname)\n\n        logger.info('Saved %s', display_path(archive_path))\n\n    def install(\n        self,\n        install_options,  # type: List[str]\n        global_options=None,  # type: Optional[Sequence[str]]\n        root=None,  # type: Optional[str]\n        home=None,  # type: Optional[str]\n        prefix=None,  # type: Optional[str]\n        warn_script_location=True,  # type: bool\n        use_user_site=False,  # type: bool\n        pycompile=True  # type: bool\n    ):\n        # type: (...) -> None\n        scheme = get_scheme(\n            self.name,\n            user=use_user_site,\n            home=home,\n            root=root,\n            isolated=self.isolated,\n            prefix=prefix,\n        )\n\n        global_options = global_options if global_options is not None else []\n        if self.editable:\n            install_editable_legacy(\n                install_options,\n                global_options,\n                prefix=prefix,\n                home=home,\n                use_user_site=use_user_site,\n                name=self.name,\n                setup_py_path=self.setup_py_path,\n                isolated=self.isolated,\n                build_env=self.build_env,\n                unpacked_source_directory=self.unpacked_source_directory,\n            )\n            self.install_succeeded = True\n            return\n\n        if self.is_wheel:\n            assert self.local_file_path\n            direct_url = None\n            if self.original_link:\n                direct_url = direct_url_from_link(\n                    self.original_link,\n                    self.source_dir,\n                    self.original_link_is_in_wheel_cache,\n                )\n            install_wheel(\n                self.name,\n                self.local_file_path,\n                scheme=scheme,\n                req_description=str(self.req),\n                pycompile=pycompile,\n                warn_script_location=warn_script_location,\n                direct_url=direct_url,\n                requested=self.user_supplied,\n            )\n            self.install_succeeded = True\n            return\n\n        # TODO: Why don't we do this for editable installs?\n\n        # Extend the list of global and install options passed on to\n        # the setup.py call with the ones from the requirements file.\n        # Options specified in requirements file override those\n        # specified on the command line, since the last option given\n        # to setup.py is the one that is used.\n        global_options = list(global_options) + self.global_options\n        install_options = list(install_options) + self.install_options\n\n        try:\n            success = install_legacy(\n                install_options=install_options,\n                global_options=global_options,\n                root=root,\n                home=home,\n                prefix=prefix,\n                use_user_site=use_user_site,\n                pycompile=pycompile,\n                scheme=scheme,\n                setup_py_path=self.setup_py_path,\n                isolated=self.isolated,\n                req_name=self.name,\n                build_env=self.build_env,\n                unpacked_source_directory=self.unpacked_source_directory,\n                req_description=str(self.req),\n            )\n        except LegacyInstallFailure as exc:\n            self.install_succeeded = False\n            six.reraise(*exc.parent)\n        except Exception:\n            self.install_succeeded = True\n            raise\n\n        self.install_succeeded = success\n\n\ndef check_invalid_constraint_type(req):\n    # type: (InstallRequirement) -> str\n\n    # Check for unsupported forms\n    problem = \"\"\n    if not req.name:\n        problem = \"Unnamed requirements are not allowed as constraints\"\n    elif req.link:\n        problem = \"Links are not allowed as constraints\"\n    elif req.extras:\n        problem = \"Constraints cannot have extras\"\n\n    if problem:\n        deprecated(\n            reason=(\n                \"Constraints are only allowed to take the form of a package \"\n                \"name and a version specifier. Other forms were originally \"\n                \"permitted as an accident of the implementation, but were \"\n                \"undocumented. The new implementation of the resolver no \"\n                \"longer supports these forms.\"\n            ),\n            replacement=(\n                \"replacing the constraint with a requirement.\"\n            ),\n            # No plan yet for when the new resolver becomes default\n            gone_in=None,\n            issue=8210\n        )\n\n    return problem\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/pip/_internal/req/req_install.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/pip/_internal/req/req_install.py	(date 1602088700401)
@@ -121,6 +121,7 @@
         self.comes_from = comes_from
         self.constraint = constraint
         self.editable = editable
+        self.legacy_install_reason = None  # type: Optional[int]
 
         # source_dir is the local directory where the linked requirement is
         # located, or unpacked. In case unpacking is needed, creating and
@@ -859,6 +860,18 @@
 
         self.install_succeeded = success
 
+        if success and self.legacy_install_reason == 8368:
+            deprecated(
+                reason=(
+                    "{} was installed using the legacy 'setup.py install' "
+                    "method, because a wheel could not be built for it.".
+                    format(self.name)
+                ),
+                replacement="to fix the wheel build issue reported above",
+                gone_in="21.0",
+                issue=8368,
+            )
+
 
 def check_invalid_constraint_type(req):
     # type: (InstallRequirement) -> str
Index: env/lib/python3.8/site-packages/setuptools/lib2to3_ex.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\nCustomized Mixin2to3 support:\n\n - adds support for converting doctests\n\n\nThis module raises an ImportError on Python 2.\n\"\"\"\n\nimport warnings\nfrom distutils.util import Mixin2to3 as _Mixin2to3\nfrom distutils import log\nfrom lib2to3.refactor import RefactoringTool, get_fixers_from_package\n\nimport setuptools\nfrom ._deprecation_warning import SetuptoolsDeprecationWarning\n\n\nclass DistutilsRefactoringTool(RefactoringTool):\n    def log_error(self, msg, *args, **kw):\n        log.error(msg, *args)\n\n    def log_message(self, msg, *args):\n        log.info(msg, *args)\n\n    def log_debug(self, msg, *args):\n        log.debug(msg, *args)\n\n\nclass Mixin2to3(_Mixin2to3):\n    def run_2to3(self, files, doctests=False):\n        # See of the distribution option has been set, otherwise check the\n        # setuptools default.\n        if self.distribution.use_2to3 is not True:\n            return\n        if not files:\n            return\n\n        warnings.warn(\n            \"2to3 support is deprecated. If the project still \"\n            \"requires Python 2 support, please migrate to \"\n            \"a single-codebase solution or employ an \"\n            \"independent conversion process.\",\n            SetuptoolsDeprecationWarning)\n        log.info(\"Fixing \" + \" \".join(files))\n        self.__build_fixer_names()\n        self.__exclude_fixers()\n        if doctests:\n            if setuptools.run_2to3_on_doctests:\n                r = DistutilsRefactoringTool(self.fixer_names)\n                r.refactor(files, write=True, doctests_only=True)\n        else:\n            _Mixin2to3.run_2to3(self, files)\n\n    def __build_fixer_names(self):\n        if self.fixer_names:\n            return\n        self.fixer_names = []\n        for p in setuptools.lib2to3_fixer_packages:\n            self.fixer_names.extend(get_fixers_from_package(p))\n        if self.distribution.use_2to3_fixers is not None:\n            for p in self.distribution.use_2to3_fixers:\n                self.fixer_names.extend(get_fixers_from_package(p))\n\n    def __exclude_fixers(self):\n        excluded_fixers = getattr(self, 'exclude_fixers', [])\n        if self.distribution.use_2to3_exclude_fixers is not None:\n            excluded_fixers.extend(self.distribution.use_2to3_exclude_fixers)\n        for fixer_name in excluded_fixers:\n            if fixer_name in self.fixer_names:\n                self.fixer_names.remove(fixer_name)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/lib2to3_ex.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/lib2to3_ex.py	(date 1602088701601)
@@ -2,9 +2,6 @@
 Customized Mixin2to3 support:
 
  - adds support for converting doctests
-
-
-This module raises an ImportError on Python 2.
 """
 
 import warnings
Index: env/lib/python3.8/site-packages/setuptools/namespaces.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nfrom distutils import log\nimport itertools\n\nfrom setuptools.extern.six.moves import map\n\n\nflatten = itertools.chain.from_iterable\n\n\nclass Installer:\n\n    nspkg_ext = '-nspkg.pth'\n\n    def install_namespaces(self):\n        nsp = self._get_all_ns_packages()\n        if not nsp:\n            return\n        filename, ext = os.path.splitext(self._get_target())\n        filename += self.nspkg_ext\n        self.outputs.append(filename)\n        log.info(\"Installing %s\", filename)\n        lines = map(self._gen_nspkg_line, nsp)\n\n        if self.dry_run:\n            # always generate the lines, even in dry run\n            list(lines)\n            return\n\n        with open(filename, 'wt') as f:\n            f.writelines(lines)\n\n    def uninstall_namespaces(self):\n        filename, ext = os.path.splitext(self._get_target())\n        filename += self.nspkg_ext\n        if not os.path.exists(filename):\n            return\n        log.info(\"Removing %s\", filename)\n        os.remove(filename)\n\n    def _get_target(self):\n        return self.target\n\n    _nspkg_tmpl = (\n        \"import sys, types, os\",\n        \"has_mfs = sys.version_info > (3, 5)\",\n        \"p = os.path.join(%(root)s, *%(pth)r)\",\n        \"importlib = has_mfs and __import__('importlib.util')\",\n        \"has_mfs and __import__('importlib.machinery')\",\n        (\n            \"m = has_mfs and \"\n            \"sys.modules.setdefault(%(pkg)r, \"\n            \"importlib.util.module_from_spec(\"\n            \"importlib.machinery.PathFinder.find_spec(%(pkg)r, \"\n            \"[os.path.dirname(p)])))\"\n        ),\n        (\n            \"m = m or \"\n            \"sys.modules.setdefault(%(pkg)r, types.ModuleType(%(pkg)r))\"\n        ),\n        \"mp = (m or []) and m.__dict__.setdefault('__path__',[])\",\n        \"(p not in mp) and mp.append(p)\",\n    )\n    \"lines for the namespace installer\"\n\n    _nspkg_tmpl_multi = (\n        'm and setattr(sys.modules[%(parent)r], %(child)r, m)',\n    )\n    \"additional line(s) when a parent package is indicated\"\n\n    def _get_root(self):\n        return \"sys._getframe(1).f_locals['sitedir']\"\n\n    def _gen_nspkg_line(self, pkg):\n        # ensure pkg is not a unicode string under Python 2.7\n        pkg = str(pkg)\n        pth = tuple(pkg.split('.'))\n        root = self._get_root()\n        tmpl_lines = self._nspkg_tmpl\n        parent, sep, child = pkg.rpartition('.')\n        if parent:\n            tmpl_lines += self._nspkg_tmpl_multi\n        return ';'.join(tmpl_lines) % locals() + '\\n'\n\n    def _get_all_ns_packages(self):\n        \"\"\"Return sorted list of all package namespaces\"\"\"\n        pkgs = self.distribution.namespace_packages or []\n        return sorted(flatten(map(self._pkg_names, pkgs)))\n\n    @staticmethod\n    def _pkg_names(pkg):\n        \"\"\"\n        Given a namespace package, yield the components of that\n        package.\n\n        >>> names = Installer._pkg_names('a.b.c')\n        >>> set(names) == set(['a', 'a.b', 'a.b.c'])\n        True\n        \"\"\"\n        parts = pkg.split('.')\n        while parts:\n            yield '.'.join(parts)\n            parts.pop()\n\n\nclass DevelopInstaller(Installer):\n    def _get_root(self):\n        return repr(str(self.egg_path))\n\n    def _get_target(self):\n        return self.egg_link\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- env/lib/python3.8/site-packages/setuptools/namespaces.py	(revision f534df4cef52bf2e71b93948326a6c50784c503b)
+++ env/lib/python3.8/site-packages/setuptools/namespaces.py	(date 1602088701601)
@@ -2,8 +2,6 @@
 from distutils import log
 import itertools
 
-from setuptools.extern.six.moves import map
-
 
 flatten = itertools.chain.from_iterable
 
@@ -72,8 +70,6 @@
         return "sys._getframe(1).f_locals['sitedir']"
 
     def _gen_nspkg_line(self, pkg):
-        # ensure pkg is not a unicode string under Python 2.7
-        pkg = str(pkg)
         pth = tuple(pkg.split('.'))
         root = self._get_root()
         tmpl_lines = self._nspkg_tmpl
